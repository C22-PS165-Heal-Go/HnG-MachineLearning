{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db3773cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from model import PMF, DRRAveStateRepresentation, Actor, Critic\n",
    "\n",
    "from utils.prioritized_replay_buffer import NaivePrioritizedReplayMemory, Transition\n",
    "from utils.history_buffer import HistoryBuffer\n",
    "from utils.general import export_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c50c81e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRRTrainer(object):\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 actor_function,\n",
    "                 critic_function,\n",
    "                 state_rep_function,\n",
    "                 reward_function,\n",
    "                 users,\n",
    "                 items,\n",
    "                 train_data,\n",
    "                 test_data,\n",
    "                 user_embeddings,\n",
    "                 item_embeddings):\n",
    "        \n",
    "        ## importing reward function\n",
    "        self.reward_function = reward_function\n",
    "        ## importing training and testing data\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        ## importing users and items\n",
    "        self.users = users\n",
    "        self.items = items\n",
    "        ## importing user and item embeddings\n",
    "        self.user_embeddings = user_embeddings\n",
    "        self.item_embeddings = item_embeddings\n",
    "        ## declaring index identifier for dataset\n",
    "        ## u for user, i for item, r for reward/rating\n",
    "        self.u = 0\n",
    "        self.i = 1\n",
    "        self.r = 2\n",
    "        \n",
    "        ## dimensions\n",
    "        ## self.item_embeddings already hold the weights array\n",
    "        ## this should be 100\n",
    "        self.item_features = self.item_embeddings.shape[1]\n",
    "        self.user_features = self.user_embeddings.shape[1]\n",
    "        \n",
    "        ## number of user and items\n",
    "        self.n_items = self.item_embeddings.shape[0]\n",
    "        self.n_users = self.user_embeddings.shape[0]\n",
    "        \n",
    "        ## the shape of state space, action space\n",
    "        ## this should be 300\n",
    "        self.state_shape = 3 * self.item_features\n",
    "        ## this should be 100\n",
    "        self.action_shape = self.item_features\n",
    "        \n",
    "        self.critic_output_shape = 1\n",
    "        self.config = config\n",
    "        ## Data dimensions Extracted\n",
    "        \n",
    "        ## instantiate a drravestaterepresentation\n",
    "        self.state_rep_net = state_rep_function(self.config.history_buffer_size,\n",
    "                                                self.item_features,\n",
    "                                                self.user_features)\n",
    "        \n",
    "        ## instantiate actor and target actor networks\n",
    "        self.actor_net = actor_function(self.state_shape, self.action_shape)                                \n",
    "        self.target_actor_net = actor_function(self.state_shape, self.action_shape)\n",
    "        \n",
    "        ## instantiate critic and target critics networks\n",
    "        self.critic_net = critic_function(self.action_shape,\n",
    "                                          self.state_shape,\n",
    "                                          self.critic_output_shape)\n",
    "        \n",
    "        self.target_critic_net = critic_function(self.action_shape,\n",
    "                                                 self.state_shape,\n",
    "                                                 self.critic_output_shape)\n",
    "        \n",
    "        ## data flow for building the model\n",
    "        flow_item = tf.convert_to_tensor(np.random.rand(5, 100), dtype='float32')\n",
    "        flow_state = tf.convert_to_tensor(np.random.rand(1, 300), dtype='float32')\n",
    "        flow_action = tf.convert_to_tensor(np.random.rand(1, 100), dtype='float32')\n",
    "        \n",
    "        ## flowing the data into the model to build the model\n",
    "        self.state_rep_net(user_embeddings[0], flow_item)\n",
    "        self.actor_net(flow_state)\n",
    "        self.target_actor_net(flow_state)\n",
    "        self.critic_net(flow_state, flow_action)\n",
    "        self.target_critic_net(flow_state, flow_action)\n",
    "        print(\"Actor-Critic model has successfully instantiated\")\n",
    "        \n",
    "        self.state_rep_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=self.config.lr_state_rep)\n",
    "        \n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=self.config.lr_actor)\n",
    "        \n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=self.config.lr_critic)\n",
    "        \n",
    "        print(\"DRR Instantiazed\")\n",
    "        \n",
    "    def learn(self):\n",
    "        # Initialize buffers\n",
    "        print(\"NPRM and History Buffer Initialized\")\n",
    "        replay_buffer = NaivePrioritizedReplayMemory(self.config.replay_buffer_size,\n",
    "                                                     prob_alpha=self.config.prob_alpha)\n",
    "\n",
    "        history_buffer = HistoryBuffer(self.config.history_buffer_size)\n",
    "        \n",
    "        # Initialize trackers\n",
    "        # initialize timesteps and epoch\n",
    "        timesteps = 0\n",
    "        epoch = 0\n",
    "        ## this variable is for episode\n",
    "        eps_slope = abs(self.config.eps_start - self.config.eps)/self.config.eps_steps\n",
    "        eps = self.config.eps_start\n",
    "        ## this variable is to hold the losses along the time\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        ## this variable is to hold the episodic rewards\n",
    "        epi_rewards = []\n",
    "        epi_avg_rewards = []\n",
    "        \n",
    "        e_arr = []\n",
    "        \n",
    "        ## this variable holds the user index\n",
    "        ## got from the dictionary\n",
    "        user_idxs = np.array(list(self.users.values()))\n",
    "        np.random.shuffle(user_idxs)\n",
    "        \n",
    "        ## loop all the users based on indexes\n",
    "        for idx, e in enumerate(user_idxs):\n",
    "            ## starting the episodes\n",
    "            \n",
    "            ## the loops stop when timesteps-learning_start\n",
    "            ## is bigger than the max timesteps\n",
    "            if timesteps - self.config.learning_start > self.config.max_timesteps_train:\n",
    "                break\n",
    "            \n",
    "            ## extracting positive user reviews\n",
    "            ## e variable is an element right now\n",
    "            user_reviews = self.train_data[self.train_data[:, self.u] == e]\n",
    "            pos_user_reviews = user_reviews[user_reviews[:, self.r] > 0]\n",
    "            \n",
    "            ## check if the user ratings doesn't have enough positive review\n",
    "            ## in this case history_buffer_size is 4\n",
    "            ## get the shape object and 0 denote the row index\n",
    "            if pos_user_reviews.shape[0] < self.config.history_buffer_size:\n",
    "                continue\n",
    "                \n",
    "            candidate_items = tf.identity(tf.stop_gradient(self.item_embeddings))\n",
    "            \n",
    "            ## extracting user embedding tensors\n",
    "            user_emb = self.user_embeddings[e]\n",
    "            \n",
    "            ## fill history buffer with positive item embeddings\n",
    "            ## and remove item embeddings from candidate item sets\n",
    "            ignored_items = []\n",
    "            \n",
    "            ## history_buffer_size has size of n items\n",
    "            ## in this case 5\n",
    "            for i in range(self.config.history_buffer_size):\n",
    "                emb = candidate_items[tf.cast(pos_user_reviews[i, self.i], dtype='int32')]\n",
    "                history_buffer.push(tf.identity(tf.stop_gradient(emb)))\n",
    "                \n",
    "            ## initialize rewards list\n",
    "            rewards = []\n",
    "            \n",
    "            ## starting item index\n",
    "            t = 0\n",
    "            \n",
    "            ## declaring the needed variable\n",
    "            state = None\n",
    "            action = None\n",
    "            reward = None\n",
    "            next_state = None\n",
    "            \n",
    "            while t < self.config.episode_length:\n",
    "                ## observing the current state\n",
    "                ## choose action according to actor network explorations\n",
    "                \n",
    "                ## inference calls start here\n",
    "                \n",
    "                if eps > self.config.eps:\n",
    "                    eps -= eps_slope\n",
    "                else:\n",
    "                    eps = self.config.eps\n",
    "                \n",
    "                ## state is the result of DRRAve model inference\n",
    "                ## history_buffer.to_list get the list of previous items\n",
    "                ## state representaton has the size (300, )\n",
    "                state = self.state_rep_net(user_emb, tf.stack(history_buffer.to_list()))\n",
    "                if np.random.uniform(0, 1) < eps:\n",
    "                    action = tf.convert_to_tensor(np.random.rand(1, self.action_shape), dtype='float32') \n",
    "                else:\n",
    "                    action = self.actor_net(tf.stop_gradient(tf.reshape(state, [1, state.shape[0]])), training=False)\n",
    "                    \n",
    "                ranking_scores = candidate_items @ tf.reshape(action, (action.shape[1], 1))\n",
    "                ## calculating ranking scores accross items, discard ignored items\n",
    "                \n",
    "                if len(ignored_items) > 0:\n",
    "                    rec_items = tf.stack(ignored_items)\n",
    "                else:\n",
    "                    rec_items = []\n",
    "                \n",
    "                ## setting value of negative infinite\n",
    "#                 ranking_scores[rec_items] = -sys.maxsize -1\n",
    "                \n",
    "                ## get the recommended items\n",
    "                ## first get the maximum value index\n",
    "                ## then get the items by index from candidate items\n",
    "                ranking_scores = tf.reshape(ranking_scores, (ranking_scores.shape[0],))\n",
    "                rec_item_idx = tf.math.argmax(ranking_scores)\n",
    "                rec_item_emb = candidate_items[rec_item_idx]\n",
    "                \n",
    "                ## get the item reward\n",
    "                if tf.cast(rec_item_idx, 'float64') in user_reviews[:, self.i]:\n",
    "                    ## get the reward from rating in the dataset\n",
    "                    ## if the user is rating the item\n",
    "                    user_rec_item_idx = np.where(user_reviews[:, self.i] == float(rec_item_idx))[0][0]\n",
    "                    reward = user_reviews[user_rec_item_idx, self.r]\n",
    "                else:\n",
    "                    if self.config.zero_reward:\n",
    "                        reward = tf.convert_to_tensor(0)\n",
    "                    else:\n",
    "                        reward = self.reward_function(tf.convert_to_tensor(e), rec_item_idx)\n",
    "                \n",
    "                ## track the episode rewards\n",
    "                rewards.append(reward.numpy())\n",
    "                \n",
    "                ## add item to history buffer if positive reviews\n",
    "                if reward > 0:\n",
    "                    history_buffer.push(tf.stop_gradient(rec_item_emb))\n",
    "                    \n",
    "                    next_state = self.state_rep_net(user_emb, tf.stack(history_buffer.to_list()), training=False)\n",
    "                else:\n",
    "                    ## keep the history buffer the same\n",
    "                    ## the next state is the current state\n",
    "                    next_state = tf.stop_gradient(state)\n",
    "                \n",
    "                ## remove new items from future recommendation\n",
    "                ignored_items.append(tf.convert_to_tensor(rec_item_idx))\n",
    "                \n",
    "                ## add the (state, action, reward, next_state)\n",
    "                ## to the experience replay\n",
    "                replay_buffer.push(state, action, next_state, reward)\n",
    "                \n",
    "                ## Inference calling stops here\n",
    "                ## Training start here\n",
    "                if(timesteps > self.config.learning_start) and (len(replay_buffer) >= self.config.batch_size) and (timesteps % self.config.learning_freq == 0):\n",
    "                    critic_loss, actor_loss, critic_params_norm = self.training_step(timesteps,\n",
    "                                                                                     replay_buffer,\n",
    "                                                                                     True\n",
    "                                                                                     )\n",
    "                    ## storing the losses along the time\n",
    "                    actor_losses.append(actor_loss)\n",
    "                    critic_losses.append(critic_loss)\n",
    "                    \n",
    "                    ## outputting the result\n",
    "                    if timesteps % self.config.log_freq == 0:\n",
    "                        if len(rewards) > 0:\n",
    "                            print(\n",
    "                                f'Timestep {timesteps - self.config.learning_start} | '\n",
    "                                f'Episode {epoch} | '\n",
    "                                f'Mean Ep R '\n",
    "                                f'{np.mean(rewards):.4f} | '\n",
    "                                f'Max R {np.max(rewards):.4f} | '\n",
    "                                f'Critic Params Norm {critic_params_norm:.4f} | '\n",
    "                                f'Actor Loss {actor_loss:.4f} | '\n",
    "                                f'Critic Loss {critic_loss:.4f} | ')\n",
    "                            sys.stdout.flush()\n",
    "            \n",
    "                ## housekeeping\n",
    "                t += 1\n",
    "                timesteps += 1\n",
    "            \n",
    "                ## end of timesteps\n",
    "            ## end of episodes\n",
    "            if timesteps - self.config.learning_start > t:\n",
    "                epoch += 1\n",
    "                e_arr.append(epoch)\n",
    "                epi_rewards.append(np.sum(rewards))\n",
    "                epi_avg_rewards.append(np.mean(rewards))\n",
    "        \n",
    "        print(\"Training Finished\")\n",
    "        return actor_losses, critic_losses, epi_avg_rewards\n",
    "    \n",
    "    def training_step(self, t, replay_buffer, training):\n",
    "        ## create batches\n",
    "        ## from utils programs\n",
    "        # Create batches by calling sample methods\n",
    "        transitions, indicies, weights = replay_buffer.sample(self.config.batch_size, beta=self.config.beta)\n",
    "        \n",
    "        weights = tf.convert_to_tensor(weights, dtype='float32')\n",
    "        ## create the tuple using Transition function     \n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        ## preparing the batch for each data\n",
    "        ## the concat function will flatten the data\n",
    "        ## the reshape will reshape the data so that it receive 64 rows\n",
    "        next_state_batch = tf.reshape(tf.concat(batch.next_state, 0), [self.config.batch_size, -1])\n",
    "        state_batch = tf.reshape(tf.concat(batch.state, 0), [self.config.batch_size, -1])\n",
    "        action_batch = tf.reshape(tf.concat(batch.action, 0), [self.config.batch_size, -1])\n",
    "        reward_batch = tf.reshape(tf.concat(tf.cast(batch.reward, dtype='float32'), 0), [self.config.batch_size, -1])\n",
    "        \n",
    "        ## updating the critic networks\n",
    "        with tf.GradientTape() as tape:\n",
    "            critic_loss, new_priorities = self.compute_prioritized_dqn_loss(tf.stop_gradient(state_batch),\n",
    "                                                                            action_batch,\n",
    "                                                                            reward_batch,\n",
    "                                                                            next_state_batch,\n",
    "                                                                            weights)\n",
    "        ## apply the gradient\n",
    "        grads = tape.gradient(critic_loss, self.critic_net.trainable_variables)\n",
    "        \n",
    "        replay_buffer.update_priorities(indicies, new_priorities.numpy())\n",
    "        ## critic norm clipping\n",
    "        critic_param_norm = [tf.clip_by_norm(layer.get_weights()[0] ,self.config.clip_val) for layer in self.critic_net.layers]\n",
    "        ## step the optimizers\n",
    "        self.critic_optimizer.apply_gradients(zip(grads, self.critic_net.trainable_variables))\n",
    "        \n",
    "        ## updating the actor networks\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            actions_pred = self.actor_net(state_batch, training=True)\n",
    "            actor_loss = -tf.reduce_mean(self.critic_net(tf.stop_gradient(state_batch), actions_pred, training=True))\n",
    "        ## compute the gradient\n",
    "        grads = tape.gradient(actor_loss, self.actor_net.trainable_variables)\n",
    "        ## apply the step to the optimizers\n",
    "        self.actor_optimizer.apply_gradients(zip(grads, self.actor_net.trainable_variables))\n",
    "        ## traceback the variables\n",
    "        grads = tape.gradient(actor_loss, self.state_rep_net.trainable_variables)\n",
    "        print(grads)\n",
    "        ## apply the step\n",
    "        self.state_rep_optimizer.apply_gradients(zip(grads, self.state_rep_net.trainable_variables))\n",
    "        ## minimizing the loss\n",
    "        del tape\n",
    "        ## updating the target networks\n",
    "        self.soft_update(self.critic_net, self.target_critic_net, self.config.tau)\n",
    "        self.soft_update(self.actor_net, self.target_actor_net, self.config.tau)\n",
    "        \n",
    "        return critic_loss.item(), actor_loss.item(), critic_param_norm\n",
    "        \n",
    "        \n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: model which the weights will be copied from\n",
    "            target_model: model which weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        \n",
    "        for target_weights, local_weights in zip(reward_function_copy.layers, reward_function.layers):\n",
    "            temp_w = local_weights.get_weights()[0] * tau + (1.0 - tau) * target_weights.get_weights()[0]\n",
    "            target_weights.set_weights([temp_w])\n",
    "    \n",
    "#     @tf.function    \n",
    "    def compute_prioritized_dqn_loss(self,\n",
    "                                     state_batch,\n",
    "                                     action_batch,\n",
    "                                     reward_batch,\n",
    "                                     next_state_batch,\n",
    "                                     weights):\n",
    "        '''\n",
    "        :param state_batch: (tensor) shape = (batch_size x state_dims),\n",
    "                The batched tensor of states collected during\n",
    "                training (i.e. s)\n",
    "        :param action_batch: (LongTensor) shape = (batch_size,)\n",
    "                The actions that you actually took at each step (i.e. a)\n",
    "        :param reward_batch: (tensor) shape = (batch_size,)\n",
    "                The rewards that you actually got at each step (i.e. r)\n",
    "        :param next_state_batch: (tensor) shape = (batch_size x state_dims),\n",
    "                The batched tensor of next states collected during\n",
    "                training (i.e. s')\n",
    "        :param weights: (tensor) shape = (batch_size,)\n",
    "                Weights for each batch item w.r.t. prioritized experience replay buffer\n",
    "        :return: loss: (torch tensor) shape = (1),\n",
    "                 new_priorities: (numpy array) shape = (batch_size,)\n",
    "        '''\n",
    "        ## create batches\n",
    "        ## forward pass through target actor network\n",
    "        next_action = self.target_actor_net(next_state_batch, training=False)\n",
    "        q_target = self.target_critic_net(next_state_batch, next_action, training=False)\n",
    "        ## y or target value that needs to be retreived\n",
    "        y = reward_batch + self.config.gamma * q_target\n",
    "        ## get q values from the current state\n",
    "        q_vals = self.critic_net(state_batch, action_batch, training=True)\n",
    "    \n",
    "        ## calculate loss\n",
    "        loss = tf.convert_to_tensor(y - q_vals)\n",
    "        ## because loss is tensor shape\n",
    "        ## we can extract the numpy value\n",
    "        loss = tf.pow(loss, 2)\n",
    "        weights_ten = tf.convert_to_tensor(weights)\n",
    "        loss = tf.reshape(loss, (self.config.batch_size,)) * weights_ten\n",
    "        ## stop the weights to be gradiented\n",
    "        weights_ten = tf.stop_gradient(weights_ten)\n",
    "        \n",
    "        ## calculate new priorities\n",
    "        new_priorities = tf.stop_gradient(loss + 1e-5)\n",
    "        loss = tf.convert_to_tensor(tf.math.reduce_mean(loss))\n",
    "        \n",
    "        return loss, new_priorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "856fd195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:(80000, 3), Test Data:(20000, 3)\n",
      "user embedding has shape (100,) and item embedding has shape (100,)\n",
      "Actor-Critic model has successfully instantiated\n",
      "DRR Instantiazed\n",
      "Start Training\n",
      "NPRM and History Buffer Initialized\n",
      "[None]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: (['Variable:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\narray([[0.01016856],\n       [0.07533184],\n       [0.04493533],\n       [0.08016576],\n       [0.03985484]], dtype=float32)>),).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15508/3445412696.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Start Training\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15508/1721191729.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m                 \u001b[1;31m## Training start here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimesteps\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_start\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtimesteps\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_freq\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m                     critic_loss, actor_loss, critic_params_norm = self.training_step(timesteps,\n\u001b[0m\u001b[0;32m    244\u001b[0m                                                                                      \u001b[0mreplay_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m                                                                                      \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15508/1721191729.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, t, replay_buffer, training)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;31m## apply the step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_rep_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_rep_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m         \u001b[1;31m## minimizing the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    631\u001b[0m       \u001b[0mRuntimeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mcalled\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcross\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mreplica\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m     \"\"\"\n\u001b[1;32m--> 633\u001b[1;33m     \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_empty_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    634\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizer_v2\\utils.py\u001b[0m in \u001b[0;36mfilter_empty_gradients\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m     71\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mvariable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     raise ValueError(f\"No gradients provided for any variable: {variable}. \"\n\u001b[0m\u001b[0;32m     74\u001b[0m                      f\"Provided `grads_and_vars` is {grads_and_vars}.\")\n\u001b[0;32m     75\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: (['Variable:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\narray([[0.01016856],\n       [0.07533184],\n       [0.04493533],\n       [0.08016576],\n       [0.03985484]], dtype=float32)>),)."
     ]
    }
   ],
   "source": [
    "class config():\n",
    "    ## hyperparameters\n",
    "    ## setting the batch_size\n",
    "    batch_size = 64\n",
    "    gamma = 0.9\n",
    "    replay_buffer_size = 100000\n",
    "    history_buffer_size = 5\n",
    "    learning_start = 32\n",
    "    learning_freq = 1\n",
    "    ## learning rate for each model networks\n",
    "    lr_state_rep = 0.001\n",
    "    lr_actor = 0.0001\n",
    "    lr_critic = 0.001\n",
    "    \n",
    "    eps_start = 1\n",
    "    eps = 0.1\n",
    "    eps_steps = 10000\n",
    "    eps_eval = 0.1\n",
    "    episode_length = 10\n",
    "    \n",
    "    tau = 0.01 # inital 0.001\n",
    "    beta = 0.4\n",
    "    prob_alpha = 0.3\n",
    "    \n",
    "    max_timesteps_train = 260000\n",
    "    max_epochs_offline = 500\n",
    "    max_timesteps_online = 20000\n",
    "    embedding_feature_size = 100\n",
    "    \n",
    "    train_ratio = 0.8\n",
    "    weight_decay = 0.01\n",
    "    clip_val = 1.0\n",
    "    log_freq = 100\n",
    "    saving_freq = 1000\n",
    "    zero_reward = False\n",
    "    \n",
    "## First importing the data\n",
    "users = pickle.load(open('Dataset/user_id_to_num_mov.pkl', 'rb'))\n",
    "items = pickle.load(open('Dataset/movie_id_to_num_mov.pkl', 'rb'))\n",
    "data = np.load('Dataset/data.npy')\n",
    "\n",
    "## hold the length of the data\n",
    "n_users = len(users)\n",
    "n_items = len(items)\n",
    "\n",
    "## don't forget to normalize the data first\n",
    "data[:, 2] = 0.5 * (data[:, 2] - 3)\n",
    "\n",
    "## split and shuffle the data\n",
    "np.random.shuffle(data)\n",
    "## split the data\n",
    "## ratio should be 0.8\n",
    "train_data = tf.convert_to_tensor(data[:int(config.train_ratio * data.shape[0])])\n",
    "test_data = tf.convert_to_tensor(data[int(config.train_ratio * data.shape[0]):])\n",
    "print(\"Train Data:{}, Test Data:{}\".format(np.shape(train_data), np.shape(test_data)))\n",
    "\n",
    "## hold the PMF model\n",
    "## get the user and item embeddings\n",
    "reward_function = PMF(n_users, n_items, config.embedding_feature_size)\n",
    "## need to flow some data to build the model\n",
    "reward_function(1, 1)\n",
    "## loading the whole layer weights\n",
    "reward_function.load_weights('trained/adam/pmf_150_adam')\n",
    "## freeze the model, because it will be used for inference\n",
    "reward_function.trainable = False\n",
    "\n",
    "## take the embedding layers weight\n",
    "## and split the user and item weights\n",
    "user_embeddings = tf.convert_to_tensor(reward_function.user_embedding.get_weights()[0])\n",
    "item_embeddings = tf.convert_to_tensor(reward_function.item_embedding.get_weights()[0])\n",
    "## output\n",
    "print(\"user embedding has shape {} and item embedding has shape {}\"\n",
    "      .format(np.shape(user_embeddings[0]), np.shape(item_embeddings[0])))\n",
    "\n",
    "## hold the model in the variable\n",
    "## so it can be tracked\n",
    "state_rep_function = DRRAveStateRepresentation\n",
    "actor_function = Actor\n",
    "critic_function = Critic\n",
    "\n",
    "## initialize DRRTrain Class\n",
    "trainer = DRRTrainer(config,\n",
    "                     actor_function,\n",
    "                     critic_function,\n",
    "                     state_rep_function,\n",
    "                     reward_function,\n",
    "                     users,\n",
    "                     items,\n",
    "                     train_data,\n",
    "                     test_data,\n",
    "                     user_embeddings,\n",
    "                     item_embeddings)\n",
    "\n",
    "print(\"Start Training\")\n",
    "trainer.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d510268",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_batch = tf.convert_to_tensor(np.random.rand(64, 300), dtype='float32')\n",
    "state_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d05c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state_batch = tf.convert_to_tensor(np.random.rand(64, 300), dtype='float32')\n",
    "next_state_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0886329",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_batch = tf.convert_to_tensor(np.random.rand(64, 1), dtype='float32')\n",
    "reward_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ecad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_batch = tf.convert_to_tensor(np.random.rand(64, 100), dtype='float32')\n",
    "action_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82fa892",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.convert_to_tensor(np.ones(64,), dtype='float32')\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = Actor(300, 100)\n",
    "target_actor_net = Actor(300, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_net = Critic(100, 300, 1)\n",
    "target_critic_net = Critic(100, 300, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddc6652",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c0fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    ## create batches\n",
    "    ## forward pass through target actor network\n",
    "    \n",
    "    ## gradient tape will watch the input\n",
    "#     tape.watch(state_batch)\n",
    "    \n",
    "    next_action = target_actor_net(next_state_batch, training=False)\n",
    "    q_target = target_critic_net(next_state_batch, next_action, training=False)\n",
    "    \n",
    "    ## y or target value that needs to be retreived\n",
    "    y = reward_batch + 0.9 * q_target\n",
    "    \n",
    "    ## get q values from the current state\n",
    "    q_vals = critic_net(state_batch, action_batch, training=True)\n",
    "    \n",
    "    ## calculate loss\n",
    "    loss = tf.convert_to_tensor(y - q_vals)\n",
    "    \n",
    "    ## because loss is tensor shape\n",
    "    ## we can extract the numpy value\n",
    "    loss = tf.pow(loss, 2)\n",
    "    weights_ten = tf.convert_to_tensor(weights)\n",
    "    loss = loss * weights_ten\n",
    "    \n",
    "    ## stop the weights to be gradiented\n",
    "    weights_ten = tf.stop_gradient(weights_ten)\n",
    "    \n",
    "    ## calculate new priorities\n",
    "    new_priorities = tf.stop_gradient(loss + 1e-5)\n",
    "    \n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "## apply the gradient\n",
    "## the gradients dloss,dx\n",
    "grads = tape.gradient(loss, critic_net.trainable_variables)\n",
    "\n",
    "## step the optimizers\n",
    "critic_optimizer.apply_gradients(zip(grads, critic_net.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b8c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.Variable(initial_value=(0.1 * tf.random.uniform((5, 1), minval=0., maxval=1.)),\n",
    "                                             trainable=False,\n",
    "                                             dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2833714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.1 * tf.random.uniform((5, 1), minval=0., maxval=1.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
