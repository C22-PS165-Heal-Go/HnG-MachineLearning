{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db3773cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from model import PMF, DRRAveStateRepresentation, Actor, Critic\n",
    "\n",
    "from utils.prioritized_replay_buffer import NaivePrioritizedReplayMemory, Transition\n",
    "from utils.history_buffer import HistoryBuffer\n",
    "from utils.general import export_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c50c81e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRRTrainer(object):\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 actor_function,\n",
    "                 critic_function,\n",
    "                 state_rep_function,\n",
    "                 reward_function,\n",
    "                 users,\n",
    "                 items,\n",
    "                 train_data,\n",
    "                 test_data,\n",
    "                 user_embeddings,\n",
    "                 item_embeddings):\n",
    "        \n",
    "        ## importing reward function\n",
    "        self.reward_function = reward_function\n",
    "        ## importing training and testing data\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        ## importing users and items\n",
    "        self.users = users\n",
    "        self.items = items\n",
    "        ## importing user and item embeddings\n",
    "        self.user_embeddings = user_embeddings\n",
    "        self.item_embeddings = item_embeddings\n",
    "        ## declaring index identifier for dataset\n",
    "        ## u for user, i for item, r for reward/rating\n",
    "        self.u = 0\n",
    "        self.i = 1\n",
    "        self.r = 2\n",
    "        \n",
    "        ## dimensions\n",
    "        ## self.item_embeddings already hold the weights array\n",
    "        ## this should be 100\n",
    "        self.item_features = len(self.item_embeddings[0][0])\n",
    "        self.user_features = len(self.user_embeddings[0][0])\n",
    "        \n",
    "        ## number of user and items\n",
    "        self.n_items = len(self.item_embeddings[0])\n",
    "        self.n_users = len(self.user_embeddings[0])\n",
    "        \n",
    "        ## the shape of state space, action space\n",
    "        ## this should be 300\n",
    "        self.state_shape = 3 * self.item_features\n",
    "        ## this should be 100\n",
    "        self.action_shape = self.item_features\n",
    "        \n",
    "        self.critic_output_shape = 1\n",
    "        self.config = config\n",
    "        ## Data dimensions Extracted\n",
    "        \n",
    "        ## instantiate a drravestaterepresentation\n",
    "        self.state_rep_net = state_rep_function(self.config.history_buffer_size,\n",
    "                                                self.item_features,\n",
    "                                                self.user_features)\n",
    "        \n",
    "        ## instantiate actor and target actor networks\n",
    "        self.actor_net = actor_function(self.state_shape, self.action_shape)                                \n",
    "        self.target_actor_net = actor_function(self.state_shape, self.action_shape)\n",
    "        \n",
    "        ## instantiate critic and target critics networks\n",
    "        self.critic_net = critic_function(self.action_shape,\n",
    "                                          self.state_shape,\n",
    "                                          self.critic_output_shape)\n",
    "        \n",
    "        self.target_critic_net = critic_function(self.action_shape,\n",
    "                                                 self.state_shape,\n",
    "                                                 self.critic_output_shape)\n",
    "        print(\"Actor-Critic model has successfully instantiated\")\n",
    "        print(\"DRR Instantiazed\")\n",
    "        \n",
    "    def learn(self):\n",
    "        # Initialize buffers\n",
    "        print(\"NPRM and History Buffer Initialized\")\n",
    "        replay_buffer = NaivePrioritizedReplayMemory(self.config.replay_buffer_size,\n",
    "                                                     prob_alpha=self.config.prob_alpha)\n",
    "\n",
    "        history_buffer = HistoryBuffer(self.config.history_buffer_size)\n",
    "        \n",
    "        # Initialize trackers\n",
    "        # initialize timesteps and epoch\n",
    "        timesteps = 0\n",
    "        epoch = 0\n",
    "        ## this variable is for episode\n",
    "        eps_slope = abs(self.config.eps_start - self.config.eps)/self.config.eps_steps\n",
    "        eps = self.config.eps_start\n",
    "        ## this variable is to hold the losses along the time\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        ## this variable is to hold the episodic rewards\n",
    "        epi_rewards = []\n",
    "        epi_avg_rewards = []\n",
    "        \n",
    "        e_arr = []\n",
    "        \n",
    "        ## this variable holds the user index\n",
    "        ## got from the dictionary\n",
    "        user_idxs = np.array(list(self.users.values()))\n",
    "        np.random.shuffle(user_idxs)\n",
    "        \n",
    "        ## loop all the users based on indexes\n",
    "        for idx, e in enumerate(user_idxs):\n",
    "            ## starting the episodes\n",
    "            \n",
    "            ## the loops stop when timesteps-learning_start\n",
    "            ## is bigger than the max timesteps\n",
    "            if timesteps - self.config.learning_start > self.config.max_timesteps_train:\n",
    "                break\n",
    "            \n",
    "            ## extracting positive user reviews\n",
    "            ## e variable is an element right now\n",
    "            user_reviews = self.train_data[self.train_data[:, self.u] == e]\n",
    "            pos_user_reviews = user_reviews[user_reviews[:, self.r] > 0]\n",
    "            \n",
    "            ## check if the user ratings doesn't have enough positive review\n",
    "            ## in this case history_buffer_size is 4\n",
    "            ## get the shape object and 0 denote the row index\n",
    "            if pos_user_reviews.shape[0] < self.config.history_buffer_size:\n",
    "                continue\n",
    "                \n",
    "            candidate_items = self.items_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "856fd195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:(80000, 3), Test Data:(20000, 3)\n",
      "user embedding has shape (943, 100) and item embedding has shape (1682, 100)\n",
      "Actor-Critic model has successfully instantiated\n",
      "DRR Instantiazed\n"
     ]
    }
   ],
   "source": [
    "class config():\n",
    "    ## hyperparameters\n",
    "    ## setting the batch_size\n",
    "    batch_size = 64\n",
    "    gamma = 0.9\n",
    "    replay_buffer_size = 100000\n",
    "    history_buffer_size = 5\n",
    "    learning_start = 32\n",
    "    learning_freq = 1\n",
    "    lr_state_rep = 0.001\n",
    "    lr_actor = 0.0001\n",
    "    lr_critic = 0.001\n",
    "    \n",
    "    eps_start = 1\n",
    "    eps = 0.1\n",
    "    eps_steps = 10000\n",
    "    eps_eval = 0.1\n",
    "    episode_length = 10\n",
    "    \n",
    "    tau = 0.01 # inital 0.001\n",
    "    beta = 0.4\n",
    "    prob_alpha = 0.3\n",
    "    \n",
    "    max_timesteps_train = 260000\n",
    "    max_epochs_offline = 500\n",
    "    max_timesteps_online = 20000\n",
    "    embedding_feature_size = 100\n",
    "    \n",
    "    train_ratio = 0.8\n",
    "    weight_decay = 0.01\n",
    "    clip_val = 1.0\n",
    "    log_freq = 100\n",
    "    saving_freq = 1000\n",
    "    zero_reward = False\n",
    "    \n",
    "## First importing the data\n",
    "users = pickle.load(open('Dataset/user_id_to_num_mov.pkl', 'rb'))\n",
    "items = pickle.load(open('Dataset/movie_id_to_num_mov.pkl', 'rb'))\n",
    "data = np.load('Dataset/data.npy')\n",
    "\n",
    "## hold the length of the data\n",
    "n_users = len(users)\n",
    "n_items = len(items)\n",
    "\n",
    "## don't forget to normalize the data first\n",
    "data[:, 2] = 0.5 * (data[:, 2] - 3)\n",
    "\n",
    "## split and shuffle the data\n",
    "np.random.shuffle(data)\n",
    "## split the data\n",
    "## ratio should be 0.8\n",
    "train_data = tf.convert_to_tensor(data[:int(config.train_ratio * data.shape[0])])\n",
    "test_data = tf.convert_to_tensor(data[int(config.train_ratio * data.shape[0]):])\n",
    "print(\"Train Data:{}, Test Data:{}\".format(np.shape(train_data), np.shape(test_data)))\n",
    "\n",
    "## hold the PMF model\n",
    "## get the user and item embeddings\n",
    "reward_function = PMF(n_users, n_items, config.embedding_feature_size)\n",
    "## need to flow some data to build the model\n",
    "reward_function(1, 1)\n",
    "## loading the whole layer weights\n",
    "reward_function.load_weights('trained/adam/pmf_150_adam')\n",
    "## freeze the model, because it will be used for inference\n",
    "reward_function.trainable = False\n",
    "\n",
    "## take the embedding layers weight\n",
    "## and split the user and item weights\n",
    "user_embeddings = reward_function.user_embedding.get_weights()\n",
    "item_embeddings = reward_function.item_embedding.get_weights()\n",
    "## output\n",
    "print(\"user embedding has shape {} and item embedding has shape {}\"\n",
    "      .format(np.shape(user_embeddings[0]), np.shape(item_embeddings[0])))\n",
    "\n",
    "## hold the model in the variable\n",
    "## so it can be tracked\n",
    "state_rep_function = DRRAveStateRepresentation\n",
    "actor_function = Actor\n",
    "critic_function = Critic\n",
    "\n",
    "## initialize DRRTrain Class\n",
    "trainer = DRRTrainer(config,\n",
    "                     actor_function,\n",
    "                     critic_function,\n",
    "                     state_rep_function,\n",
    "                     reward_function,\n",
    "                     users,\n",
    "                     items,\n",
    "                     train_data,\n",
    "                     test_data,\n",
    "                     user_embeddings,\n",
    "                     item_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
