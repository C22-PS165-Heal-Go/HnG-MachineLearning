{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db3773cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from model import PMF, DRRAveStateRepresentation, Actor, Critic\n",
    "\n",
    "from utils.prioritized_replay_buffer import NaivePrioritizedReplayMemory, Transition\n",
    "from utils.history_buffer import HistoryBuffer\n",
    "from utils.general import export_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c50c81e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRRTrainer(object):\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 actor_function,\n",
    "                 critic_function,\n",
    "                 state_rep_function,\n",
    "                 reward_function,\n",
    "                 users,\n",
    "                 items,\n",
    "                 train_data,\n",
    "                 test_data,\n",
    "                 user_embeddings,\n",
    "                 item_embeddings):\n",
    "        \n",
    "        ## importing reward function\n",
    "        self.reward_function = reward_function\n",
    "        ## importing training and testing data\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        ## importing users and items\n",
    "        self.users = users\n",
    "        self.items = items\n",
    "        ## importing user and item embeddings\n",
    "        self.user_embeddings = user_embeddings\n",
    "        self.item_embeddings = item_embeddings\n",
    "        ## declaring index identifier for dataset\n",
    "        ## u for user, i for item, r for reward/rating\n",
    "        self.u = 0\n",
    "        self.i = 1\n",
    "        self.r = 2\n",
    "        \n",
    "        ## dimensions\n",
    "        ## self.item_embeddings already hold the weights array\n",
    "        ## this should be 100\n",
    "        self.item_features = self.item_embeddings.shape[1]\n",
    "        self.user_features = self.user_embeddings.shape[1]\n",
    "        \n",
    "        ## number of user and items\n",
    "        self.n_items = self.item_embeddings.shape[0]\n",
    "        self.n_users = self.user_embeddings.shape[0]\n",
    "        \n",
    "        ## the shape of state space, action space\n",
    "        ## this should be 300\n",
    "        self.state_shape = 3 * self.item_features\n",
    "        ## this should be 100\n",
    "        self.action_shape = self.item_features\n",
    "        \n",
    "        self.critic_output_shape = 1\n",
    "        self.config = config\n",
    "        ## Data dimensions Extracted\n",
    "        \n",
    "        ## instantiate a drravestaterepresentation\n",
    "        self.state_rep_net = state_rep_function(self.config.history_buffer_size,\n",
    "                                                self.item_features,\n",
    "                                                self.user_features)\n",
    "        \n",
    "        ## instantiate actor and target actor networks\n",
    "        self.actor_net = actor_function(self.state_shape, self.action_shape)                                \n",
    "        self.target_actor_net = actor_function(self.state_shape, self.action_shape)\n",
    "        \n",
    "        ## instantiate critic and target critics networks\n",
    "        self.critic_net = critic_function(self.action_shape,\n",
    "                                          self.state_shape,\n",
    "                                          self.critic_output_shape)\n",
    "        \n",
    "        self.target_critic_net = critic_function(self.action_shape,\n",
    "                                                 self.state_shape,\n",
    "                                                 self.critic_output_shape)\n",
    "        \n",
    "        ## data flow for building the model\n",
    "        flow_item = tf.convert_to_tensor(np.random.rand(5, 100), dtype='float32')\n",
    "        flow_state = tf.convert_to_tensor(np.random.rand(1, 300), dtype='float32')\n",
    "        flow_action = tf.convert_to_tensor(np.random.rand(1, 100), dtype='float32')\n",
    "        \n",
    "        ## flowing the data into the model to build the model\n",
    "        self.state_rep_net(user_embeddings[0], flow_item)\n",
    "        self.actor_net(flow_state)\n",
    "        self.target_actor_net(flow_state)\n",
    "        self.critic_net(flow_state, flow_action)\n",
    "        self.target_critic_net(flow_state, flow_action)\n",
    "        print(\"Actor-Critic model has successfully instantiated\")\n",
    "        \n",
    "        self.state_rep_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=self.config.lr_state_rep)\n",
    "        \n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=self.config.lr_actor)\n",
    "        \n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=self.config.lr_critic)\n",
    "        \n",
    "        print(\"DRR Instantiazed\")\n",
    "        \n",
    "    def learn(self):\n",
    "        # Initialize buffers\n",
    "        print(\"NPRM and History Buffer Initialized\")\n",
    "        replay_buffer = NaivePrioritizedReplayMemory(self.config.replay_buffer_size,\n",
    "                                                     prob_alpha=self.config.prob_alpha)\n",
    "\n",
    "        history_buffer = HistoryBuffer(self.config.history_buffer_size)\n",
    "        \n",
    "        # Initialize trackers\n",
    "        # initialize timesteps and epoch\n",
    "        timesteps = 0\n",
    "        epoch = 0\n",
    "        ## this variable is for episode\n",
    "        eps_slope = abs(self.config.eps_start - self.config.eps)/self.config.eps_steps\n",
    "        eps = self.config.eps_start\n",
    "        ## this variable is to hold the losses along the time\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        ## this variable is to hold the episodic rewards\n",
    "        epi_rewards = []\n",
    "        epi_avg_rewards = []\n",
    "        \n",
    "        e_arr = []\n",
    "        \n",
    "        ## this variable holds the user index\n",
    "        ## got from the dictionary\n",
    "        user_idxs = np.array(list(self.users.values()))\n",
    "        np.random.shuffle(user_idxs)\n",
    "        \n",
    "        ## loop all the users based on indexes\n",
    "        ## enumerates start with zero\n",
    "        for idx, e in enumerate(user_idxs):\n",
    "            ## starting the episodes\n",
    "            \n",
    "            ## the loops stop when timesteps-learning_start\n",
    "            ## is bigger than the max timesteps\n",
    "            if timesteps - self.config.learning_start > self.config.max_timesteps_train:\n",
    "                break\n",
    "            \n",
    "            ## extracting positive user reviews\n",
    "            ## e variable is an element right now\n",
    "            user_reviews = self.train_data[self.train_data[:, self.u] == e]\n",
    "            pos_user_reviews = user_reviews[user_reviews[:, self.r] > 0]\n",
    "            \n",
    "            ## check if the user ratings doesn't have enough positive review\n",
    "            ## in this case history_buffer_size is 4\n",
    "            ## get the shape object and 0 denote the row index\n",
    "            if pos_user_reviews.shape[0] < self.config.history_buffer_size:\n",
    "                continue\n",
    "                \n",
    "            candidate_items = tf.identity(tf.stop_gradient(self.item_embeddings))\n",
    "            \n",
    "            ## extracting user embedding tensors\n",
    "            user_emb = self.user_embeddings[e]\n",
    "            \n",
    "            ## fill history buffer with positive item embeddings\n",
    "            ## and remove item embeddings from candidate item sets\n",
    "            ignored_items = []\n",
    "            \n",
    "            ## history_buffer_size has size of n items\n",
    "            ## in this case 5\n",
    "            for i in range(self.config.history_buffer_size):\n",
    "                emb = candidate_items[int(pos_user_reviews[i, self.i].numpy())]\n",
    "                history_buffer.push(tf.identity(tf.stop_gradient(emb)))\n",
    "                \n",
    "            ## initialize rewards list\n",
    "            rewards = []\n",
    "            \n",
    "            ## starting item index\n",
    "            t = 0\n",
    "            \n",
    "            ## declaring the needed variable\n",
    "            state = None\n",
    "            action = None\n",
    "            reward = None\n",
    "            next_state = None\n",
    "            \n",
    "            while t < self.config.episode_length:\n",
    "                ## observing the current state\n",
    "                ## choose action according to actor network explorations\n",
    "                \n",
    "                ## inference calls start here\n",
    "                \n",
    "                if eps > self.config.eps:\n",
    "                    eps -= eps_slope\n",
    "                else:\n",
    "                    eps = self.config.eps\n",
    "                \n",
    "                ## state is the result of DRRAve model inference\n",
    "                ## history_buffer.to_list get the list of previous items\n",
    "                ## state representaton has the size (300, )\n",
    "                state = self.state_rep_net(user_emb, tf.stack(history_buffer.to_list()))\n",
    "                \n",
    "                if np.random.uniform(0, 1) < eps:\n",
    "                    action = tf.convert_to_tensor(np.random.rand(1, self.action_shape), dtype='float32') \n",
    "                else:\n",
    "                    action = self.actor_net(tf.stop_gradient(state), training=False)\n",
    "                    \n",
    "                ranking_scores = candidate_items @ tf.transpose(action)\n",
    "                ranking_scores = tf.reshape(ranking_scores, (ranking_scores.shape[0],)).numpy()\n",
    "                ## calculating ranking scores accross items, discard ignored items\n",
    "                \n",
    "                if len(ignored_items) > 0:\n",
    "                    rec_items = tf.stack(ignored_items).numpy()\n",
    "                else:\n",
    "                    rec_items = []\n",
    "                \n",
    "                ranking_scores[rec_items] = -float(\"inf\")\n",
    "                \n",
    "                ## get the recommended items\n",
    "                ## first get the maximum value index\n",
    "                ## then get the items by index from candidate items\n",
    "                rec_item_idx = tf.math.argmax(ranking_scores).numpy()\n",
    "                rec_item_emb = candidate_items[rec_item_idx]\n",
    "                \n",
    "                ## add item to history buffer if positive reviews\n",
    "                if rec_item_idx in user_reviews[:, self.i]:\n",
    "                    user_rec_item_idx = np.where(user_reviews[:, self.i] == float(rec_item_idx))[0][0]\n",
    "                    reward = user_reviews[user_rec_item_idx, self.r]\n",
    "                else:\n",
    "                    if config.zero_reward:\n",
    "                        reward = tf.convert_to_tensor(0)\n",
    "                    else:\n",
    "                        reward = reward_function(float(e), float(rec_item_idx))\n",
    "\n",
    "                rewards.append(reward.numpy())\n",
    "                \n",
    "                if reward > 0:\n",
    "                    history_buffer.push(tf.identity(tf.stop_gradient(rec_item_emb)))\n",
    "                    next_state = self.state_rep_net(user_emb, tf.stack(history_buffer.to_list()), training=False)\n",
    "                else:\n",
    "                    next_state = tf.stop_gradient(state)\n",
    "                \n",
    "                ignored_items.append(rec_item_idx)\n",
    "                replay_buffer.push(state, action, next_state, reward)\n",
    "                \n",
    "                ## Inference calling stops here\n",
    "                ## Training start here\n",
    "                if(timesteps > self.config.learning_start) and (len(replay_buffer) >= self.config.batch_size) and (timesteps % self.config.learning_freq == 0):\n",
    "                    \n",
    "                    #### TRAINING ####\n",
    "                    critic_loss, actor_loss, critic_params_norm = self.training_step(timesteps,\n",
    "                                                                                     replay_buffer,\n",
    "                                                                                     True\n",
    "                                                                                     )\n",
    "                    ## storing the losses along the time\n",
    "                    actor_losses.append(actor_loss)\n",
    "                    critic_losses.append(critic_loss)\n",
    "                    \n",
    "                    ## outputting the result\n",
    "                    if timesteps % self.config.log_freq == 0:\n",
    "                        if len(rewards) > 0:\n",
    "                            print(\n",
    "                                f'Timestep {timesteps - self.config.learning_start} | '\n",
    "                                f'Episode {epoch} | '\n",
    "                                f'Mean Ep R '\n",
    "                                f'{np.mean(rewards):.4f} | '\n",
    "                                f'Max R {np.max(rewards):.4f} | '\n",
    "                                f'Critic Params Norm {critic_params_norm:.4f} | '\n",
    "                                f'Actor Loss {actor_loss:.4f} | '\n",
    "                                f'Critic Loss {critic_loss:.4f} | ')\n",
    "                            sys.stdout.flush()\n",
    "            \n",
    "                ## housekeeping\n",
    "                t += 1\n",
    "                timesteps += 1\n",
    "            \n",
    "                ## end of timesteps\n",
    "            ## end of episodes\n",
    "            if timesteps - self.config.learning_start > t:\n",
    "                epoch += 1\n",
    "                e_arr.append(epoch)\n",
    "                epi_rewards.append(np.sum(rewards))\n",
    "                epi_avg_rewards.append(np.mean(rewards))\n",
    "        \n",
    "        print(\"Training Finished\")\n",
    "        return actor_losses, critic_losses, epi_avg_rewards\n",
    "    \n",
    "    def training_step(self, t, replay_buffer, training):\n",
    "        ## create batches\n",
    "        ## from utils programs\n",
    "        # Create batches by calling sample methods\n",
    "        transitions, indicies, weights = replay_buffer.sample(self.config.batch_size, beta=self.config.beta)\n",
    "        \n",
    "        weights = tf.convert_to_tensor(weights, dtype='float32')\n",
    "        ## create the tuple using Transition function     \n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        ## preparing the batch for each data\n",
    "        ## the concat function will flatten the data\n",
    "        ## the reshape will reshape the data so that it receive 64 rows\n",
    "        next_state_batch = tf.reshape(tf.concat(batch.next_state, 0), [self.config.batch_size, -1])\n",
    "        state_batch = tf.reshape(tf.concat(batch.state, 0), [self.config.batch_size, -1])\n",
    "        action_batch = tf.reshape(tf.concat(batch.action, 0), [self.config.batch_size, -1])\n",
    "        reward_batch = tf.reshape(tf.concat(batch.reward, 0), [self.config.batch_size, -1])\n",
    "        \n",
    "        ## updating the critic networks\n",
    "        with tf.GradientTape() as tape:\n",
    "            critic_loss, new_priorities = self.compute_prioritized_dqn_loss(tf.stop_gradient(state_batch),\n",
    "                                                                            action_batch,\n",
    "                                                                            reward_batch,\n",
    "                                                                            next_state_batch,\n",
    "                                                                            weights)\n",
    "        ## apply the gradient\n",
    "        grads = tape.gradient(critic_loss, self.critic_net.trainable_variables)\n",
    "        \n",
    "        replay_buffer.update_priorities(indicies, new_priorities)\n",
    "        ## critic norm clipping\n",
    "        critic_param_norm = [tf.clip_by_norm(layer.get_weights()[0] ,self.config.clip_val) for layer in self.critic_net.layers]\n",
    "        critic_param_norm = tf.norm(critic_param_norm[0])\n",
    "        ## step the optimizers\n",
    "        self.critic_optimizer.apply_gradients(zip(grads, self.critic_net.trainable_variables))\n",
    "        \n",
    "        ## updating the actor networks\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions_pred = self.actor_net(state_batch, training=True)\n",
    "            actor_loss = -tf.reduce_mean(self.critic_net(tf.stop_gradient(state_batch), actions_pred, training=True))\n",
    "            \n",
    "        ## compute the gradient\n",
    "        grads = tape.gradient(actor_loss, self.actor_net.trainable_variables)\n",
    "        ## apply the step to the optimizers\n",
    "        self.actor_optimizer.apply_gradients(zip(grads, self.actor_net.trainable_variables))\n",
    "\n",
    "        ## updating the target networks\n",
    "        self.soft_update(self.critic_net, self.target_critic_net, self.config.tau)\n",
    "        self.soft_update(self.actor_net, self.target_actor_net, self.config.tau)\n",
    "        \n",
    "        return critic_loss.numpy(), actor_loss.numpy(), critic_param_norm\n",
    "         \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: model which the weights will be copied from\n",
    "            target_model: model which weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for t_layer, layer in zip(target_model.layers, local_model.layers):\n",
    "            ## initiate list\n",
    "            temp_w_arr = []\n",
    "            for t_weights, weights in zip(t_layer.get_weights(), layer.get_weights()):\n",
    "                ## fill the array list\n",
    "                temp_w_arr.append(weights * tau + (1.0-tau) * t_weights)\n",
    "            ## copy the weights\n",
    "            t_layer.set_weights(temp_w_arr)\n",
    "      \n",
    "    def compute_prioritized_dqn_loss(self,\n",
    "                                     state_batch,\n",
    "                                     action_batch,\n",
    "                                     reward_batch,\n",
    "                                     next_state_batch,\n",
    "                                     weights):\n",
    "        '''\n",
    "        :param state_batch: (tensor) shape = (batch_size x state_dims),\n",
    "                The batched tensor of states collected during\n",
    "                training (i.e. s)\n",
    "        :param action_batch: (LongTensor) shape = (batch_size,)\n",
    "                The actions that you actually took at each step (i.e. a)\n",
    "        :param reward_batch: (tensor) shape = (batch_size,)\n",
    "                The rewards that you actually got at each step (i.e. r)\n",
    "        :param next_state_batch: (tensor) shape = (batch_size x state_dims),\n",
    "                The batched tensor of next states collected during\n",
    "                training (i.e. s')\n",
    "        :param weights: (tensor) shape = (batch_size,)\n",
    "                Weights for each batch item w.r.t. prioritized experience replay buffer\n",
    "        :return: loss: (torch tensor) shape = (1),\n",
    "                 new_priorities: (numpy array) shape = (batch_size,)\n",
    "        '''\n",
    "        ## create batches\n",
    "        ## forward pass through target actor network\n",
    "        next_action = self.target_actor_net(next_state_batch, training=False)\n",
    "        q_target = self.target_critic_net(next_state_batch, next_action, training=False)\n",
    "        ## y or target value that needs to be retreived\n",
    "        y = reward_batch + self.config.gamma * q_target\n",
    "        ## get q values from the current state\n",
    "        q_vals = self.critic_net(state_batch, action_batch, training=True)\n",
    "    \n",
    "        ## calculate loss\n",
    "        loss = tf.convert_to_tensor(y - q_vals)\n",
    "        ## because loss is tensor shape\n",
    "        ## we can extract the numpy value\n",
    "        loss = tf.pow(loss, 2)\n",
    "        weights_ten = tf.stop_gradient(weights)\n",
    "        loss = tf.reshape(loss, (self.config.batch_size,)) * weights_ten\n",
    "        ## stop the weights to be gradiented\n",
    "        weights_ten = tf.stop_gradient(weights_ten)\n",
    "        ## calculate new priorities\n",
    "        new_priorities = tf.stop_gradient(loss).numpy() + 1e-5\n",
    "        loss = tf.convert_to_tensor(tf.math.reduce_mean(loss))\n",
    "        \n",
    "        return loss, new_priorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "856fd195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:(80000, 3), Test Data:(20000, 3)\n",
      "user embedding has shape (100,) and item embedding has shape (100,)\n",
      "Actor-Critic model has successfully instantiated\n",
      "DRR Instantiazed\n",
      "Start Training\n",
      "NPRM and History Buffer Initialized\n",
      "Timestep 68 | Episode 6 | Mean Ep R -0.3281 | Max R -0.3281 | Critic Params Norm 1.0000 | Actor Loss 0.0010 | Critic Loss 0.0357 | \n",
      "Timestep 168 | Episode 16 | Mean Ep R 0.0997 | Max R 0.0997 | Critic Params Norm 1.0000 | Actor Loss -0.8930 | Critic Loss 0.0331 | \n",
      "Timestep 268 | Episode 26 | Mean Ep R -0.3550 | Max R -0.3550 | Critic Params Norm 1.0000 | Actor Loss -2.4194 | Critic Loss 0.0150 | \n",
      "Timestep 368 | Episode 36 | Mean Ep R -0.4048 | Max R -0.4048 | Critic Params Norm 1.0000 | Actor Loss -1.5831 | Critic Loss 0.0091 | \n",
      "Timestep 468 | Episode 46 | Mean Ep R 0.0930 | Max R 0.0930 | Critic Params Norm 1.0000 | Actor Loss -2.3448 | Critic Loss 0.0112 | \n",
      "Timestep 568 | Episode 56 | Mean Ep R -0.1718 | Max R -0.1718 | Critic Params Norm 1.0000 | Actor Loss -2.3994 | Critic Loss 0.0181 | \n",
      "Timestep 668 | Episode 66 | Mean Ep R -0.7322 | Max R -0.7322 | Critic Params Norm 1.0000 | Actor Loss -3.2378 | Critic Loss 0.0165 | \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10056/4083155237.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Start Training\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10056/2309894670.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m                     \u001b[1;31m#### TRAINING ####\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m                     critic_loss, actor_loss, critic_params_norm = self.training_step(timesteps,\n\u001b[0m\u001b[0;32m    236\u001b[0m                                                                                      \u001b[0mreplay_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m                                                                                      \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10056/2309894670.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, t, replay_buffer, training)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;31m## updating the critic networks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m             critic_loss, new_priorities = self.compute_prioritized_dqn_loss(tf.stop_gradient(state_batch),\n\u001b[0m\u001b[0;32m    293\u001b[0m                                                                             \u001b[0maction_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                                                                             \u001b[0mreward_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10056/2309894670.py\u001b[0m in \u001b[0;36mcompute_prioritized_dqn_loss\u001b[1;34m(self, state_batch, action_batch, reward_batch, next_state_batch, weights)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward_batch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mq_target\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[1;31m## get q values from the current state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m         \u001b[0mq_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;31m## calculate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1081\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1082\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1083\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_keras_call_info_injected'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\CAPSTONE\\DRR-Dest\\model.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1081\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1082\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1083\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_keras_call_info_injected'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\layers\\core\\dense.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m             self.kernel, ids, weights, combiner='sum')\n\u001b[0;32m    198\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m     \u001b[1;31m# Broadcast kernel to inputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1094\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[0;32m   3698\u001b[0m             a, b, adj_x=adjoint_a, adj_y=adjoint_b, Tout=output_type, name=name)\n\u001b[0;32m   3699\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3700\u001b[1;33m         return gen_math_ops.mat_mul(\n\u001b[0m\u001b[0;32m   3701\u001b[0m             a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0;32m   3702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   6010\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6011\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6012\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   6013\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_a\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_b\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6014\u001b[0m         transpose_b)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class config():\n",
    "    ## hyperparameters\n",
    "    ## setting the batch_size\n",
    "    batch_size = 64\n",
    "    gamma = 0.9\n",
    "    replay_buffer_size = 100000\n",
    "    history_buffer_size = 5\n",
    "    learning_start = 32\n",
    "    learning_freq = 1\n",
    "    ## learning rate for each model networks\n",
    "    lr_state_rep = 0.001\n",
    "    lr_actor = 0.0001\n",
    "    lr_critic = 0.001\n",
    "    \n",
    "    eps_start = 1\n",
    "    eps = 0.1\n",
    "    eps_steps = 10000\n",
    "    eps_eval = 0.1\n",
    "    episode_length = 10\n",
    "    \n",
    "    tau = 0.01 # inital 0.001\n",
    "    beta = 0.4\n",
    "    prob_alpha = 0.3\n",
    "    \n",
    "    max_timesteps_train = 260000\n",
    "    max_epochs_offline = 500\n",
    "    max_timesteps_online = 20000\n",
    "    embedding_feature_size = 100\n",
    "    \n",
    "    train_ratio = 0.8\n",
    "    weight_decay = 0.01\n",
    "    clip_val = 1.0\n",
    "    log_freq = 100\n",
    "    saving_freq = 1000\n",
    "    zero_reward = False\n",
    "    \n",
    "## First importing the data\n",
    "users = pickle.load(open('Dataset/user_id_to_num_mov.pkl', 'rb'))\n",
    "items = pickle.load(open('Dataset/movie_id_to_num_mov.pkl', 'rb'))\n",
    "data = np.load('Dataset/data.npy')\n",
    "\n",
    "## hold the length of the data\n",
    "n_users = len(users)\n",
    "n_items = len(items)\n",
    "\n",
    "## don't forget to normalize the data first\n",
    "data[:, 2] = 0.5 * (data[:, 2] - 3)\n",
    "\n",
    "## split and shuffle the data\n",
    "np.random.shuffle(data)\n",
    "## split the data\n",
    "## ratio should be 0.8\n",
    "train_data = tf.convert_to_tensor(data[:int(config.train_ratio * data.shape[0])], dtype='float32')\n",
    "test_data = tf.convert_to_tensor(data[int(config.train_ratio * data.shape[0]):], dtype='float32')\n",
    "print(\"Train Data:{}, Test Data:{}\".format(np.shape(train_data), np.shape(test_data)))\n",
    "\n",
    "## hold the PMF model\n",
    "## get the user and item embeddings\n",
    "reward_function = PMF(n_users, n_items, config.embedding_feature_size)\n",
    "## need to flow some data to build the model\n",
    "reward_function(1, 1)\n",
    "## loading the whole layer weights\n",
    "reward_function.load_weights('trained/adam/pmf_150_adam')\n",
    "## freeze the model, because it will be used for inference\n",
    "reward_function.trainable = False\n",
    "\n",
    "## take the embedding layers weight\n",
    "## and split the user and item weights\n",
    "user_embeddings = tf.convert_to_tensor(reward_function.user_embedding.get_weights()[0])\n",
    "item_embeddings = tf.convert_to_tensor(reward_function.item_embedding.get_weights()[0])\n",
    "## output\n",
    "print(\"user embedding has shape {} and item embedding has shape {}\"\n",
    "      .format(np.shape(user_embeddings[0]), np.shape(item_embeddings[0])))\n",
    "\n",
    "## hold the model in the variable\n",
    "## so it can be tracked\n",
    "state_rep_function = DRRAveStateRepresentation\n",
    "actor_function = Actor\n",
    "critic_function = Critic\n",
    "\n",
    "## initialize DRRTrain Class\n",
    "trainer = DRRTrainer(config,\n",
    "                     actor_function,\n",
    "                     critic_function,\n",
    "                     state_rep_function,\n",
    "                     reward_function,\n",
    "                     users,\n",
    "                     items,\n",
    "                     train_data,\n",
    "                     test_data,\n",
    "                     user_embeddings,\n",
    "                     item_embeddings)\n",
    "\n",
    "print(\"Start Training\")\n",
    "trainer.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7ca6db",
   "metadata": {},
   "source": [
    "### CHECK CRITIC AMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be32413",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_shape = 3\n",
    "in_features = 9\n",
    "out_features = 1\n",
    "combo_features = in_features + action_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c8d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_fn_1 = tf.keras.layers.Dense(in_features, activation='relu')\n",
    "linear_fn_2 = tf.keras.layers.Dense(combo_features, activation='relu')\n",
    "linear_fn_3 = tf.keras.layers.Dense(combo_features, activation='relu')\n",
    "linear_fn_4 = tf.keras.layers.Dense(out_features, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a48e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = tf.convert_to_tensor(np.array([[0.9004, 0.8004, 0.7004], [0.9004, 0.6004, 0.7004]]), dtype='float32')\n",
    "input_st = tf.convert_to_tensor(np.array([[1.0000, 3.0000, 5.0000, 0.5000, 2.1000, 4.5000, 0.5000, 0.7000, 0.9000],\n",
    "                                          [1.0000, 3.0000, 5.0000, 0.5000, 2.1000, 4.5000, 0.5000, 0.7000, 0.9000]]), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f90df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_st.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9730a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "combbo = tf.concat([action, input_st], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936aac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build the model\n",
    "linear_fn_1(input_st)\n",
    "linear_fn_2(combbo)\n",
    "linear_fn_3(combbo)\n",
    "linear_fn_4(combbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f6f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1 = 0.1 * np.ones((9,9))\n",
    "weight2 = 0.1 * np.ones((12,12))\n",
    "weight3 = 0.1 * np.ones((12,12))\n",
    "weight4 = 0.1 * np.ones((12,1))\n",
    "bias1 = np.zeros((9,))\n",
    "bias2 = np.zeros((12,))\n",
    "bias3 = np.zeros((12,))\n",
    "bias4 = np.zeros((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9838bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_fn_1.set_weights([weight1, bias1])\n",
    "linear_fn_2.set_weights([weight2, bias2])\n",
    "linear_fn_3.set_weights([weight3, bias3])\n",
    "linear_fn_4.set_weights([weight4, bias4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = linear_fn_1(input_st)\n",
    "output = tf.concat([action, output], 1)\n",
    "output = linear_fn_2(output)\n",
    "output = linear_fn_3(output)\n",
    "output = linear_fn_4(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb729d69",
   "metadata": {},
   "source": [
    "### CHECK ACTOR AMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3c0717",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = 9\n",
    "out_features = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_fn_1 = tf.keras.layers.Dense(in_features, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.1))\n",
    "linear_fn_2 = tf.keras.layers.Dense(in_features, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.1))\n",
    "linear_fn_3 = tf.keras.layers.Dense(out_features, activation='tanh', kernel_regularizer=tf.keras.regularizers.L2(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d8637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = 0.1 * np.ones((9, 9))\n",
    "bias = np.zeros(9,)\n",
    "weights_3 = 0.1 * np.ones((9, 3))\n",
    "bias_3 = np.zeros(3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0db6515",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_st = tf.convert_to_tensor(np.array([[1.0000, 3.0000, 5.0000, 0.5000, 2.1000, 4.5000, 0.5000, 0.7000, 0.9000]]), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0288d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_st.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee1436",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build the model\n",
    "linear_fn_1(input_st)\n",
    "linear_fn_2(input_st)\n",
    "linear_fn_3(input_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_fn(input_st)\n",
    "linear_fn_1.set_weights([weights, bias])\n",
    "linear_fn_2.set_weights([weights, bias])\n",
    "linear_fn_3.set_weights([weights_3, bias_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf85cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = linear_fn_1(input_st)\n",
    "output = linear_fn_2(output)\n",
    "output = linear_fn_3(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0c19ee",
   "metadata": {},
   "source": [
    "### DRRAVE AMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85081f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = tf.convert_to_tensor(np.array([1, 3, 5]), dtype='float32')\n",
    "items = tf.convert_to_tensor(np.array([[1, 2, 3], [4, 5, 6]]), dtype='float32')\n",
    "attention_weights = tf.Variable(tf.convert_to_tensor(np.array([[0.1], [0.1]]), dtype='float32'))\n",
    "# attention_weights = tf.random.uniform((3, 1), minval=0., maxval=1.)\n",
    "# attention_weights\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0455c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "right = tf.transpose(items) @ attention_weights\n",
    "right = tf.reshape(right, (right.shape[0],))\n",
    "middle = users * right\n",
    "output = tf.concat([users, middle, right], 0)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b964c6c9",
   "metadata": {},
   "source": [
    "### DQN LOSS AMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a45fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.convert_to_tensor(np.array([1, 2, 3, 4]), dtype='float32')\n",
    "q_vals = tf.convert_to_tensor(np.array([2, 2, 2, 2]), dtype='float32')\n",
    "weights = [1., 1., 1., 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5219445",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate loss\n",
    "loss = tf.convert_to_tensor(y - q_vals)\n",
    "## because loss is tensor shape\n",
    "## we can extract the numpy value\n",
    "loss = tf.pow(loss, 2)\n",
    "weights_ten = tf.stop_gradient(weights)\n",
    "loss = tf.reshape(loss, (4,)) * weights_ten\n",
    "## calculate new priorities\n",
    "new_priorities = tf.stop_gradient(loss).numpy() + 1e-5\n",
    "loss = tf.convert_to_tensor(tf.math.reduce_mean(loss))\n",
    "print(new_priorities)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e36d7e5",
   "metadata": {},
   "source": [
    "### Cek Soft Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136cfe03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e86fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "    Params\n",
    "    ======\n",
    "        local_model: model which the weights will be copied from\n",
    "        target_model: model which weights will be copied to\n",
    "        tau (float): interpolation parameter\n",
    "    \"\"\"\n",
    "    for t_layer, layer in zip(target_model.layers, local_model.layers):\n",
    "        ## initiate list\n",
    "        temp_w_arr = []\n",
    "        for t_weights, weights in zip(t_layer.get_weights(), layer.get_weights()):\n",
    "            ## fill the array list\n",
    "            temp_w_arr.append(weights * tau + (1.0 - tau)*t_weights)\n",
    "        ## copy the weights\n",
    "        t_layer.set_weights(temp_w_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
