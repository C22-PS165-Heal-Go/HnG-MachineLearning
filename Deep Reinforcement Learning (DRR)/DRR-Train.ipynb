{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db3773cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import PMF, DRRAveStateRepresentation, Actor, Critic\n",
    "from train import DRRTrainer\n",
    "\n",
    "from utils.prioritized_replay_buffer import NaivePrioritizedReplayMemory, Transition\n",
    "from utils.history_buffer import HistoryBuffer\n",
    "from utils.general import export_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50c81e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRRTrainer(object):\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 actor_function,\n",
    "                 critic_function,\n",
    "                 state_rep_function,\n",
    "                 reward_function,\n",
    "                 users,\n",
    "                 items,\n",
    "                 train_data,\n",
    "                 test_data,\n",
    "                 user_embeddings,\n",
    "                 item_embeddings):\n",
    "        \n",
    "        ## importing reward function\n",
    "        self.reward_function = reward_function\n",
    "        ## importing training and testing data\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        ## importing users and items\n",
    "        self.users = users\n",
    "        self.items = items\n",
    "        ## importing user and item embeddings\n",
    "        self.user_embeddings = user_embeddings\n",
    "        self.item_embeddings = item_embeddings\n",
    "        ## declaring index identifier for dataset\n",
    "        ## u for user, i for item, r for reward/rating\n",
    "        self.u = 0\n",
    "        self.i = 1\n",
    "        self.r = 2\n",
    "        \n",
    "        ## dimensions\n",
    "        ## self.item_embeddings already hold the weights array\n",
    "        ## this should be 100\n",
    "        self.item_features = self.item_embeddings.shape[1]\n",
    "        self.user_features = self.user_embeddings.shape[1]\n",
    "        \n",
    "        ## number of user and items\n",
    "        self.n_items = self.item_embeddings.shape[0]\n",
    "        self.n_users = self.user_embeddings.shape[0]\n",
    "        \n",
    "        ## the shape of state space, action space\n",
    "        ## this should be 300\n",
    "        self.state_shape = 3 * self.item_features\n",
    "        ## this should be 100\n",
    "        self.action_shape = self.item_features\n",
    "        \n",
    "        self.critic_output_shape = 1\n",
    "        self.config = config\n",
    "        ## Data dimensions Extracted\n",
    "        \n",
    "        ## instantiate a drravestaterepresentation\n",
    "        self.state_rep_net = state_rep_function(self.config.history_buffer_size,\n",
    "                                                self.item_features,\n",
    "                                                self.user_features)\n",
    "        \n",
    "        ## instantiate actor and target actor networks\n",
    "        self.actor_net = actor_function(self.state_shape, self.action_shape)                                \n",
    "        self.target_actor_net = actor_function(self.state_shape, self.action_shape)\n",
    "        \n",
    "        ## instantiate critic and target critics networks\n",
    "        self.critic_net = critic_function(self.action_shape,\n",
    "                                          self.state_shape,\n",
    "                                          self.critic_output_shape)\n",
    "        \n",
    "        self.target_critic_net = critic_function(self.action_shape,\n",
    "                                                 self.state_shape,\n",
    "                                                 self.critic_output_shape)\n",
    "        \n",
    "        ## data flow for building the model\n",
    "        flow_item = tf.convert_to_tensor(np.random.rand(5, 100), dtype='float32')\n",
    "        flow_state = tf.convert_to_tensor(np.random.rand(1, 300), dtype='float32')\n",
    "        flow_action = tf.convert_to_tensor(np.random.rand(1, 100), dtype='float32')\n",
    "        \n",
    "        ## flowing the data into the model to build the model\n",
    "        self.state_rep_net(user_embeddings[0], flow_item)\n",
    "        self.actor_net(flow_state)\n",
    "        self.target_actor_net(flow_state)\n",
    "        self.critic_net(flow_state, flow_action)\n",
    "        self.target_critic_net(flow_state, flow_action)\n",
    "        print(\"Actor-Critic model has successfully instantiated\")\n",
    "        \n",
    "        self.state_rep_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=self.config.lr_state_rep)\n",
    "        \n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=self.config.lr_actor)\n",
    "        \n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=self.config.lr_critic)\n",
    "        \n",
    "        print(\"DRR Instantiazed\")\n",
    "        \n",
    "    def learn(self):\n",
    "        # Initialize buffers\n",
    "        print(\"NPRM and History Buffer Initialized\")\n",
    "        replay_buffer = NaivePrioritizedReplayMemory(self.config.replay_buffer_size,\n",
    "                                                     prob_alpha=self.config.prob_alpha)\n",
    "\n",
    "        history_buffer = HistoryBuffer(self.config.history_buffer_size)\n",
    "        \n",
    "        # Initialize trackers\n",
    "        # initialize timesteps and epoch\n",
    "        timesteps = 0\n",
    "        epoch = 0\n",
    "        ## this variable is for episode\n",
    "        eps_slope = abs(self.config.eps_start - self.config.eps)/self.config.eps_steps\n",
    "        eps = self.config.eps_start\n",
    "        ## this variable is to hold the losses along the time\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        ## this variable is to hold the episodic rewards\n",
    "        epi_rewards = []\n",
    "        epi_avg_rewards = []\n",
    "        \n",
    "        e_arr = []\n",
    "        \n",
    "        ## this variable holds the user index\n",
    "        ## got from the dictionary\n",
    "        user_idxs = np.array(list(self.users.values()))\n",
    "        np.random.shuffle(user_idxs)\n",
    "        \n",
    "        ## loop all the users based on indexes\n",
    "        ## enumerates start with zero\n",
    "        for idx, e in enumerate(user_idxs):\n",
    "            ## starting the episodes\n",
    "            \n",
    "            ## the loops stop when timesteps-learning_start\n",
    "            ## is bigger than the max timesteps\n",
    "            if timesteps - self.config.learning_start > self.config.max_timesteps_train:\n",
    "                break\n",
    "            \n",
    "            ## extracting positive user reviews\n",
    "            ## e variable is an element right now\n",
    "            user_reviews = self.train_data[self.train_data[:, self.u] == e]\n",
    "            pos_user_reviews = user_reviews[user_reviews[:, self.r] > 0]\n",
    "            \n",
    "            ## check if the user ratings doesn't have enough positive review\n",
    "            ## in this case history_buffer_size is 4\n",
    "            ## get the shape object and 0 denote the row index\n",
    "            if pos_user_reviews.shape[0] < self.config.history_buffer_size:\n",
    "                continue\n",
    "                \n",
    "            candidate_items = tf.identity(tf.stop_gradient(self.item_embeddings))\n",
    "            \n",
    "            ## extracting user embedding tensors\n",
    "            user_emb = self.user_embeddings[e]\n",
    "            \n",
    "            ## fill history buffer with positive item embeddings\n",
    "            ## and remove item embeddings from candidate item sets\n",
    "            ignored_items = []\n",
    "            \n",
    "            ## history_buffer_size has size of n items\n",
    "            ## in this case 5\n",
    "            for i in range(self.config.history_buffer_size):\n",
    "                emb = candidate_items[int(pos_user_reviews[i, self.i].numpy())]\n",
    "                history_buffer.push(tf.identity(tf.stop_gradient(emb)))\n",
    "                \n",
    "            ## initialize rewards list\n",
    "            rewards = []\n",
    "            \n",
    "            ## starting item index\n",
    "            t = 0\n",
    "            \n",
    "            ## declaring the needed variable\n",
    "            state = None\n",
    "            action = None\n",
    "            reward = None\n",
    "            next_state = None\n",
    "            \n",
    "            while t < self.config.episode_length:\n",
    "                ## observing the current state\n",
    "                ## choose action according to actor network explorations\n",
    "                \n",
    "                ## inference calls start here\n",
    "                \n",
    "                if eps > self.config.eps:\n",
    "                    eps -= eps_slope\n",
    "                else:\n",
    "                    eps = self.config.eps\n",
    "                \n",
    "                ## state is the result of DRRAve model inference\n",
    "                ## history_buffer.to_list get the list of previous items\n",
    "                ## state representaton has the size (300, )\n",
    "                state = self.state_rep_net(user_emb, tf.stack(history_buffer.to_list()))\n",
    "                \n",
    "                if np.random.uniform(0, 1) < eps:\n",
    "                    action = tf.convert_to_tensor(np.random.rand(1, self.action_shape), dtype='float32') \n",
    "                else:\n",
    "                    action = self.actor_net(tf.stop_gradient(state), training=False)\n",
    "                    \n",
    "                ranking_scores = candidate_items @ tf.transpose(action)\n",
    "                ranking_scores = tf.reshape(ranking_scores, (ranking_scores.shape[0],)).numpy()\n",
    "                ## calculating ranking scores accross items, discard ignored items\n",
    "                \n",
    "                if len(ignored_items) > 0:\n",
    "                    rec_items = tf.stack(ignored_items).numpy()\n",
    "                else:\n",
    "                    rec_items = []\n",
    "                \n",
    "                ranking_scores[rec_items] = -float(\"inf\")\n",
    "                \n",
    "                ## get the recommended items\n",
    "                ## first get the maximum value index\n",
    "                ## then get the items by index from candidate items\n",
    "                rec_item_idx = tf.math.argmax(ranking_scores).numpy()\n",
    "                rec_item_emb = candidate_items[rec_item_idx]\n",
    "                \n",
    "                ## add item to history buffer if positive reviews\n",
    "                if rec_item_idx in user_reviews[:, self.i]:\n",
    "                    user_rec_item_idx = np.where(user_reviews[:, self.i] == float(rec_item_idx))[0][0]\n",
    "                    reward = user_reviews[user_rec_item_idx, self.r]\n",
    "                else:\n",
    "                    if config.zero_reward:\n",
    "                        reward = tf.convert_to_tensor(0)\n",
    "                    else:\n",
    "                        reward = reward_function(float(e), float(rec_item_idx))\n",
    "\n",
    "                rewards.append(reward.numpy())\n",
    "                \n",
    "                if reward > 0:\n",
    "                    history_buffer.push(tf.identity(tf.stop_gradient(rec_item_emb)))\n",
    "                    next_state = self.state_rep_net(user_emb, tf.stack(history_buffer.to_list()), training=False)\n",
    "                else:\n",
    "                    next_state = tf.stop_gradient(state)\n",
    "                \n",
    "                ignored_items.append(rec_item_idx)\n",
    "                replay_buffer.push(state, action, next_state, reward)\n",
    "                \n",
    "                ## Inference calling stops here\n",
    "                ## Training start here\n",
    "                if(timesteps > self.config.learning_start) and (len(replay_buffer) >= self.config.batch_size) and (timesteps % self.config.learning_freq == 0):\n",
    "                    \n",
    "                    #### TRAINING ####\n",
    "                    critic_loss, actor_loss, critic_params_norm = self.training_step(timesteps,\n",
    "                                                                                     replay_buffer,\n",
    "                                                                                     True\n",
    "                                                                                     )\n",
    "                    ## storing the losses along the time\n",
    "                    actor_losses.append(actor_loss)\n",
    "                    critic_losses.append(critic_loss)\n",
    "                    \n",
    "                    ## outputting the result\n",
    "                    if timesteps % self.config.log_freq == 0:\n",
    "                        if len(rewards) > 0:\n",
    "                            print(\n",
    "                                f'Timestep {timesteps - self.config.learning_start} | '\n",
    "                                f'Episode {epoch} | '\n",
    "                                f'Mean Ep R '\n",
    "                                f'{np.mean(rewards):.4f} | '\n",
    "                                f'Max R {np.max(rewards):.4f} | '\n",
    "                                f'Critic Params Norm {critic_params_norm:.4f} | '\n",
    "                                f'Actor Loss {actor_loss:.4f} | '\n",
    "                                f'Critic Loss {critic_loss:.4f} | ')\n",
    "                            sys.stdout.flush()\n",
    "            \n",
    "                ## housekeeping\n",
    "                t += 1\n",
    "                timesteps += 1\n",
    "            \n",
    "                ## end of timesteps\n",
    "            ## end of episodes\n",
    "            if timesteps - self.config.learning_start > t:\n",
    "                epoch += 1\n",
    "                e_arr.append(epoch)\n",
    "                epi_rewards.append(np.sum(rewards))\n",
    "                epi_avg_rewards.append(np.mean(rewards))\n",
    "        \n",
    "        \n",
    "        print(\"Training Finished\")\n",
    "        \n",
    "        self.actor_net.save_weights('trained/actor_weights/actor_150')\n",
    "        self.critic_net.save_weights('trained/critic_weights/critic_150')\n",
    "        self.state_rep_net.save_weights('trained/state_rep_weights/state_rep_150')\n",
    "        \n",
    "        return actor_losses, critic_losses, epi_avg_rewards\n",
    "    \n",
    "    def training_step(self, t, replay_buffer, training):\n",
    "        ## Get the created batches\n",
    "        transitions, indicies, weights = replay_buffer.sample(self.config.batch_size, beta=self.config.beta)\n",
    "        weights = tf.convert_to_tensor(weights, dtype='float32')\n",
    "        \n",
    "        ## create the tuple using Transition function     \n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        ## preparing the batch for each data\n",
    "        ## the concat function will flatten the data\n",
    "        ## the reshape will reshape the data so that it receive 64 rows\n",
    "        next_state_batch = tf.reshape(tf.concat(batch.next_state, 0), [self.config.batch_size, -1])\n",
    "        state_batch = tf.reshape(tf.concat(batch.state, 0), [self.config.batch_size, -1])\n",
    "        action_batch = tf.reshape(tf.concat(batch.action, 0), [self.config.batch_size, -1])\n",
    "        reward_batch = tf.reshape(tf.concat(batch.reward, 0), [self.config.batch_size, -1])\n",
    "        \n",
    "        ## updating the critic networks\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            critic_loss, new_priorities = self.compute_prioritized_dqn_loss(tf.stop_gradient(state_batch),\n",
    "                                                                            action_batch,\n",
    "                                                                            reward_batch,\n",
    "                                                                            next_state_batch,\n",
    "                                                                            weights)\n",
    "        ## apply the gradient\n",
    "        grads = tape.gradient(critic_loss, self.critic_net.trainable_variables)\n",
    "        \n",
    "        replay_buffer.update_priorities(indicies, new_priorities)\n",
    "        \n",
    "        ## critic norm clipping\n",
    "        critic_param_norm = [tf.clip_by_norm(layer.get_weights()[0] ,self.config.clip_val) for layer in self.critic_net.layers]\n",
    "        critic_param_norm = tf.norm(critic_param_norm[0])\n",
    "        \n",
    "        ## step the optimizers\n",
    "        self.critic_optimizer.apply_gradients(zip(grads, self.critic_net.trainable_variables))\n",
    "                \n",
    "        \n",
    "        ## updating the actor networks\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(state_batch)\n",
    "            actions_pred = self.actor_net(state_batch, training=True)\n",
    "            actor_loss = -tf.reduce_mean(self.critic_net(tf.stop_gradient(state_batch), actions_pred, training=True))\n",
    "            \n",
    "        ## compute the gradient\n",
    "        grads = tape.gradient(actor_loss, self.actor_net.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(zip(grads, self.actor_net.trainable_variables))\n",
    "        \n",
    "        ## compute another gradient\n",
    "#         grads = tape.gradient(actor_loss, self.state_rep_net.trainable_variables)\n",
    "#         self.state_rep_optimizer.apply_gradients(zip(grads, self.state_rep_net.trainable_variables))\n",
    "        del tape\n",
    "        \n",
    "        ## updating the target networks\n",
    "        self.soft_update(self.critic_net, self.target_critic_net, self.config.tau)\n",
    "        self.soft_update(self.actor_net, self.target_actor_net, self.config.tau)\n",
    "        \n",
    "        return critic_loss.numpy(), actor_loss.numpy(), critic_param_norm\n",
    "         \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: model which the weights will be copied from\n",
    "            target_model: model which weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for t_layer, layer in zip(target_model.layers, local_model.layers):\n",
    "            ## initiate list\n",
    "            temp_w_arr = []\n",
    "            for t_weights, weights in zip(t_layer.get_weights(), layer.get_weights()):\n",
    "                ## fill the array list\n",
    "                temp_w_arr.append(weights * tau + (1.0-tau) * t_weights)\n",
    "            ## copy the weights\n",
    "            t_layer.set_weights(temp_w_arr)\n",
    "      \n",
    "    def compute_prioritized_dqn_loss(self,\n",
    "                                     state_batch,\n",
    "                                     action_batch,\n",
    "                                     reward_batch,\n",
    "                                     next_state_batch,\n",
    "                                     weights):\n",
    "        '''\n",
    "        :param state_batch: (tensor) shape = (batch_size x state_dims),\n",
    "                The batched tensor of states collected during\n",
    "                training (i.e. s)\n",
    "        :param action_batch: (LongTensor) shape = (batch_size,)\n",
    "                The actions that you actually took at each step (i.e. a)\n",
    "        :param reward_batch: (tensor) shape = (batch_size,)\n",
    "                The rewards that you actually got at each step (i.e. r)\n",
    "        :param next_state_batch: (tensor) shape = (batch_size x state_dims),\n",
    "                The batched tensor of next states collected during\n",
    "                training (i.e. s')\n",
    "        :param weights: (tensor) shape = (batch_size,)\n",
    "                Weights for each batch item w.r.t. prioritized experience replay buffer\n",
    "        :return: loss: (torch tensor) shape = (1),\n",
    "                 new_priorities: (numpy array) shape = (batch_size,)\n",
    "        '''\n",
    "        ## create batches\n",
    "        ## forward pass through target actor network\n",
    "        next_action = self.target_actor_net(next_state_batch, training=False)\n",
    "        q_target = self.target_critic_net(next_state_batch, next_action, training=False)\n",
    "        ## y or target value that needs to be retreived\n",
    "        y = reward_batch + self.config.gamma * q_target\n",
    "        ## get q values from the current state\n",
    "        q_vals = self.critic_net(state_batch, action_batch, training=True)\n",
    "    \n",
    "        ## calculate loss\n",
    "        loss = tf.convert_to_tensor(y - q_vals)\n",
    "        ## because loss is tensor shape\n",
    "        ## we can extract the numpy value\n",
    "        loss = tf.pow(loss, 2)\n",
    "        weights_ten = tf.stop_gradient(weights)\n",
    "        loss = tf.reshape(loss, (self.config.batch_size,)) * weights_ten\n",
    "        ## stop the weights to be gradiented\n",
    "        weights_ten = tf.stop_gradient(weights_ten)\n",
    "        ## calculate new priorities\n",
    "        new_priorities = tf.stop_gradient(loss).numpy() + 1e-5\n",
    "        loss = tf.convert_to_tensor(tf.math.reduce_mean(loss))\n",
    "        \n",
    "        return loss, new_priorities\n",
    "    \n",
    "    def load_parameters(self):\n",
    "        self.actor_net.load_weights('trained/actor_weights/actor_150')\n",
    "        self.target_actor_net.load_weights('trained/actor_weights/actor_150')\n",
    "        self.critic_net.load_weights('trained/critic_weights/critic_150')\n",
    "        self.target_critic_net.load_weights('trained/critic_weights/critic_150')\n",
    "        self.state_rep_net.load_weights('trained/state_rep_weights/state_rep_150')\n",
    "    \n",
    "    def offline_evaluate(self, T):\n",
    "        ## loading the parameters\n",
    "        self.load_parameters()\n",
    "        \n",
    "        history_buffer = HistoryBuffer(self.config.history_buffer_size)\n",
    "        \n",
    "        timesteps = 0\n",
    "        epoch = 0\n",
    "        rewards = []\n",
    "        epi_precisions = []\n",
    "        e_arr = []\n",
    "        \n",
    "        ## get users\n",
    "        user_idxs = np.array(list(self.users.values()))\n",
    "        np.random.shuffle(user_idxs)\n",
    "        \n",
    "        for step, e in enumerate(user_idxs):\n",
    "            \n",
    "            if len(e_arr) > self.config.max_epochs_offline:\n",
    "                break\n",
    "            \n",
    "            ## extracting positive user reviews\n",
    "            ## e variable is an element right now\n",
    "            user_reviews = self.train_data[self.train_data[:, self.u] == e]\n",
    "            pos_user_reviews = user_reviews[user_reviews[:, self.r] > 0]\n",
    "            \n",
    "            ## check if the user ratings doesn't have enough positive review\n",
    "            ## in this case history_buffer_size is 4\n",
    "            ## get the shape object and 0 denote the row index\n",
    "            if pos_user_reviews.shape[0] < T or pos_user_reviews.shape[0] < self.config.history_buffer_size:\n",
    "                continue\n",
    "                \n",
    "            candidate_items = tf.identity(tf.stop_gradient(self.item_embeddings))\n",
    "            \n",
    "            int_user_reviews_idx = user_reviews[:, self.i].numpy().astype(int)\n",
    "            user_candidate_items = tf.identity(tf.stop_gradient(tf.gather(self.item_embeddings, indices=int_user_reviews_idx)))\n",
    "            \n",
    "            ## extracting user embedding tensors\n",
    "            user_emb = self.user_embeddings[e]\n",
    "            \n",
    "            ## fill history buffer with positive item embeddings\n",
    "            ## and remove item embeddings from candidate item sets\n",
    "            ignored_items = []\n",
    "            \n",
    "            ## history_buffer_size has size of n items\n",
    "            ## in this case 5\n",
    "            for i in range(self.config.history_buffer_size):\n",
    "                emb = candidate_items[int(pos_user_reviews[i, self.i].numpy())]\n",
    "                history_buffer.push(tf.identity(tf.stop_gradient(emb)))\n",
    "                \n",
    "            ## initialize rewards list\n",
    "            rewards = []\n",
    "            \n",
    "            ## starting item index\n",
    "            t = 0\n",
    "            \n",
    "            ## declaring the needed variable\n",
    "            state = None\n",
    "            action = None\n",
    "            reward = None\n",
    "            next_state = None\n",
    "            \n",
    "            while t < self.config.episode_length:\n",
    "                ## observing the current state\n",
    "                ## choose action according to actor network explorations                \n",
    "                ## state is the result of DRRAve model inference\n",
    "                ## history_buffer.to_list get the list of previous items\n",
    "                ## state representaton has the size (300, )\n",
    "                state = self.state_rep_net(user_emb, tf.stack(history_buffer.to_list()), training=False)\n",
    "                if np.random.uniform(0, 1) < self.config.eps_eval:\n",
    "                    action = tf.convert_to_tensor(np.random.rand(1, self.action_shape), dtype='float32') \n",
    "                else:\n",
    "                    action = self.actor_net(tf.stop_gradient(state), training=False)\n",
    "                    \n",
    "                ranking_scores = candidate_items @ tf.transpose(action)\n",
    "                ranking_scores = tf.reshape(ranking_scores, (ranking_scores.shape[0],)).numpy()\n",
    "                ## calculating ranking scores accross items, discard ignored items\n",
    "                \n",
    "                if len(ignored_items) > 0:\n",
    "                    rec_items = tf.stack(ignored_items).numpy().astype(int)\n",
    "                else:\n",
    "                    rec_items = []\n",
    "                \n",
    "                ranking_scores[rec_items[:, self.i] if len(ignored_items) > 0 else []] = -float(\"inf\")\n",
    "                \n",
    "                ## get the recommended items\n",
    "                ## first get the maximum value index\n",
    "                ## then get the items by index from candidate items\n",
    "                user_ranking_scores = tf.gather(ranking_scores, indices=user_reviews[:, self.i].numpy().astype(int))\n",
    "                \n",
    "                rec_item_idx = tf.math.argmax(user_ranking_scores).numpy()\n",
    "                rec_item_emb = user_candidate_items[rec_item_idx]\n",
    "                \n",
    "                reward = user_reviews[rec_item_idx, self.r]\n",
    "\n",
    "                rewards.append(reward.numpy())\n",
    "                \n",
    "                if reward > 0:\n",
    "                    history_buffer.push(tf.identity(tf.stop_gradient(rec_item_emb)))\n",
    "                    next_state = self.state_rep_net(user_emb, tf.stack(history_buffer.to_list()), training=False)\n",
    "                else:\n",
    "                    next_state = tf.stop_gradient(state)\n",
    "                \n",
    "                ignored_items.append(user_reviews[rec_item_idx])\n",
    "            \n",
    "                ## housekeeping\n",
    "                t += 1\n",
    "                timesteps += 1\n",
    "                \n",
    "                ## end of timesteps\n",
    "            ## end of episodes\n",
    "            \n",
    "            rec_items = tf.stack(ignored_items)\n",
    "            rel_pred = rec_items[rec_items[:, self.r] > 0]\n",
    "            precision_T = len(rel_pred) / len(rec_items)\n",
    "            \n",
    "            epoch += 1\n",
    "            e_arr.append(epoch)\n",
    "            epi_precisions.append(precision_T)\n",
    "            \n",
    "            if timesteps % self.config.log_freq == 0:\n",
    "                if len(rewards) > 0:\n",
    "                    print(f'Episode {epoch} | '\n",
    "                          f'Precision@{T} {precision_T} | '\n",
    "                          f'Avg Precision@{T} {np.mean(epi_precisions):.4f} | '\n",
    "                          )\n",
    "                    sys.stdout.flush()\n",
    "            \n",
    "        print('Offline Evaluation Finished')\n",
    "        print(f'Average Precision@{T}: {np.mean(epi_precisions):.4f} | ')\n",
    "        plt.plot(e_arr, epi_precisions, label=f'Precision@{T}')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Episode (t)')\n",
    "        plt.ylabel('Precesion@T')\n",
    "        plt.title('Precision@T (Offline Evaluation)')\n",
    "        plt.minorticks_on()\n",
    "        \n",
    "        self.load_parameters()\n",
    "            \n",
    "        return np.mean(epi_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856fd195",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    ## hyperparameters\n",
    "    ## setting the batch_size\n",
    "    batch_size = 32\n",
    "    gamma = 0.9\n",
    "    replay_buffer_size = 100000\n",
    "    history_buffer_size = 5\n",
    "    learning_start = 100\n",
    "    learning_freq = 1\n",
    "    ## learning rate for each model networks\n",
    "    lr_state_rep = 0.001\n",
    "    lr_actor = 0.0001\n",
    "    lr_critic = 0.001\n",
    "    \n",
    "    eps_start = 1\n",
    "    eps = 0.1\n",
    "    eps_steps = 10000\n",
    "    eps_eval = 0.1\n",
    "    episode_length = 10\n",
    "    \n",
    "    tau = 0.01 # inital 0.001\n",
    "    beta = 0.4\n",
    "    prob_alpha = 0.3\n",
    "    \n",
    "    max_timesteps_train = 260000\n",
    "    max_epochs_offline = 500\n",
    "    max_timesteps_online = 20000\n",
    "    embedding_feature_size = 100\n",
    "    \n",
    "    train_ratio = 0.8\n",
    "    clip_val = 1.0\n",
    "    log_freq = 100\n",
    "    saving_freq = 1000\n",
    "    zero_reward = False\n",
    "    \n",
    "## First importing the data\n",
    "users = pickle.load(open('dataset_RL/user_id_to_num.pkl', 'rb'))\n",
    "items = pickle.load(open('dataset_RL/item_id_to_num.pkl', 'rb'))\n",
    "data = np.load('dataset_RL/data_RL_25000.npy')\n",
    "\n",
    "## hold the length of the data\n",
    "n_users = len(users)\n",
    "n_items = len(items)\n",
    "\n",
    "## don't forget to normalize the data first\n",
    "data[:, 2] = 0.5 * (data[:, 2] - 3)\n",
    "\n",
    "## split and shuffle the data\n",
    "np.random.shuffle(data)\n",
    "## split the data\n",
    "## ratio should be 0.8\n",
    "train_data = tf.convert_to_tensor(data[:int(config.train_ratio * data.shape[0])], dtype='float32')\n",
    "test_data = tf.convert_to_tensor(data[int(config.train_ratio * data.shape[0]):], dtype='float32')\n",
    "print(\"Train Data:{}, Test Data:{}\".format(np.shape(train_data), np.shape(test_data)))\n",
    "\n",
    "## hold the PMF model\n",
    "## get the user and item embeddings\n",
    "reward_function = PMF(n_users, n_items, config.embedding_feature_size)\n",
    "## need to flow some data to build the model\n",
    "reward_function(1, 1)\n",
    "## loading the whole layer weights\n",
    "reward_function.load_weights('trained/pmf_weights/pmf_150')\n",
    "## freeze the model, because it will be used for inference\n",
    "reward_function.trainable = False\n",
    "\n",
    "## take the embedding layers weight\n",
    "## and split the user and item weights\n",
    "user_embeddings = tf.convert_to_tensor(reward_function.user_embedding.get_weights()[0])\n",
    "item_embeddings = tf.convert_to_tensor(reward_function.item_embedding.get_weights()[0])\n",
    "## output\n",
    "print(\"user embedding has shape {} and item embedding has shape {}\"\n",
    "      .format(np.shape(user_embeddings), np.shape(item_embeddings)))\n",
    "\n",
    "## hold the model in the variable\n",
    "## so it can be tracked\n",
    "state_rep_function = DRRAveStateRepresentation\n",
    "actor_function = Actor\n",
    "critic_function = Critic\n",
    "\n",
    "## initialize DRRTrain Class\n",
    "trainer = DRRTrainer(config,\n",
    "                     actor_function,\n",
    "                     critic_function,\n",
    "                     state_rep_function,\n",
    "                     reward_function,\n",
    "                     users,\n",
    "                     items,\n",
    "                     train_data,\n",
    "                     test_data,\n",
    "                     user_embeddings,\n",
    "                     item_embeddings)\n",
    "\n",
    "print(\"Start Training\")\n",
    "actor_losses, critic_losses, epi_avg_rewards = trainer.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913f9007",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_precisions = [5, 10, 15, 20]\n",
    "\n",
    "for T_precision in T_precisions:\n",
    "    drr_Ts = []\n",
    "    for i in range(20):\n",
    "        # Evaluate\n",
    "        avg_precision = trainer.offline_evaluate(T_precision)\n",
    "        # Append to list\n",
    "        drr_Ts.append(avg_precision)\n",
    "\n",
    "    # Save data\n",
    "    drr_Ts = np.array(drr_Ts)\n",
    "    np.save('logs/' + f'avg_precision@{T_precision}_offline_eval.npy', drr_Ts)\n",
    "\n",
    "    # Save\n",
    "    sourceFile = open(\"results/\" + f'avg_precision@{T_precision}_offline_eval.txt', 'w')\n",
    "    print(f'Average Precision@{T_precision} (Eval): {np.mean(drr_Ts)}', file=sourceFile)\n",
    "    sourceFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324c41c4",
   "metadata": {},
   "source": [
    "### CHECK CRITIC AMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbebac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_shape = 3\n",
    "in_features = 9\n",
    "out_features = 1\n",
    "combo_features = in_features + action_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a92f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_fn_1 = tf.keras.layers.Dense(in_features, activation='relu')\n",
    "linear_fn_2 = tf.keras.layers.Dense(combo_features, activation='relu')\n",
    "linear_fn_3 = tf.keras.layers.Dense(combo_features, activation='relu')\n",
    "linear_fn_4 = tf.keras.layers.Dense(out_features, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f19a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = tf.convert_to_tensor(np.array([[0.9004, 0.8004, 0.7004], [0.9004, 0.6004, 0.7004]]), dtype='float32')\n",
    "input_st = tf.convert_to_tensor(np.array([[1.0000, 3.0000, 5.0000, 0.5000, 2.1000, 4.5000, 0.5000, 0.7000, 0.9000],\n",
    "                                          [1.0000, 3.0000, 5.0000, 0.5000, 2.1000, 4.5000, 0.5000, 0.7000, 0.9000]]), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ae9336",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_st.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0954cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combbo = tf.concat([action, input_st], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b729d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build the model\n",
    "linear_fn_1(input_st)\n",
    "linear_fn_2(combbo)\n",
    "linear_fn_3(combbo)\n",
    "linear_fn_4(combbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b12132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1 = 0.1 * np.ones((9,9))\n",
    "weight2 = 0.1 * np.ones((12,12))\n",
    "weight3 = 0.1 * np.ones((12,12))\n",
    "weight4 = 0.1 * np.ones((12,1))\n",
    "bias1 = np.zeros((9,))\n",
    "bias2 = np.zeros((12,))\n",
    "bias3 = np.zeros((12,))\n",
    "bias4 = np.zeros((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e5fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_fn_1.set_weights([weight1, bias1])\n",
    "linear_fn_2.set_weights([weight2, bias2])\n",
    "linear_fn_3.set_weights([weight3, bias3])\n",
    "linear_fn_4.set_weights([weight4, bias4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f1a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = linear_fn_1(input_st)\n",
    "output = tf.concat([action, output], 1)\n",
    "output = linear_fn_2(output)\n",
    "output = linear_fn_3(output)\n",
    "output = linear_fn_4(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d075a23f",
   "metadata": {},
   "source": [
    "### CHECK ACTOR AMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7582afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = 9\n",
    "out_features = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cec4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_fn_1 = tf.keras.layers.Dense(in_features, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.1))\n",
    "linear_fn_2 = tf.keras.layers.Dense(in_features, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.1))\n",
    "linear_fn_3 = tf.keras.layers.Dense(out_features, activation='tanh', kernel_regularizer=tf.keras.regularizers.L2(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb5210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = 0.1 * np.ones((9, 9))\n",
    "bias = np.zeros(9,)\n",
    "weights_3 = 0.1 * np.ones((9, 3))\n",
    "bias_3 = np.zeros(3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353aa52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_st = tf.convert_to_tensor(np.array([[1.0000, 3.0000, 5.0000, 0.5000, 2.1000, 4.5000, 0.5000, 0.7000, 0.9000]]), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6e93c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_st.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1484e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build the model\n",
    "linear_fn_1(input_st)\n",
    "linear_fn_2(input_st)\n",
    "linear_fn_3(input_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebedb77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_fn(input_st)\n",
    "linear_fn_1.set_weights([weights, bias])\n",
    "linear_fn_2.set_weights([weights, bias])\n",
    "linear_fn_3.set_weights([weights_3, bias_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece4cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = linear_fn_1(input_st)\n",
    "output = linear_fn_2(output)\n",
    "output = linear_fn_3(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d82c856",
   "metadata": {},
   "source": [
    "### DRRAVE AMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ab79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = tf.convert_to_tensor(np.array([1, 3, 5]), dtype='float32')\n",
    "items = tf.convert_to_tensor(np.array([[1, 2, 3], [4, 5, 6]]), dtype='float32')\n",
    "attention_weights = tf.Variable(tf.convert_to_tensor(np.array([[0.1], [0.1]]), dtype='float32'))\n",
    "# attention_weights = tf.random.uniform((3, 1), minval=0., maxval=1.)\n",
    "# attention_weights\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a26294",
   "metadata": {},
   "outputs": [],
   "source": [
    "right = tf.transpose(items) @ attention_weights\n",
    "right = tf.reshape(right, (right.shape[0],))\n",
    "middle = users * right\n",
    "output = tf.concat([users, middle, right], 0)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a9d709",
   "metadata": {},
   "source": [
    "### DQN LOSS AMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4f929",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.convert_to_tensor(np.array([1, 2, 3, 4]), dtype='float32')\n",
    "q_vals = tf.convert_to_tensor(np.array([2, 2, 2, 2]), dtype='float32')\n",
    "weights = [1., 1., 1., 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0eafce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate loss\n",
    "loss = tf.convert_to_tensor(y - q_vals)\n",
    "## because loss is tensor shape\n",
    "## we can extract the numpy value\n",
    "loss = tf.pow(loss, 2)\n",
    "weights_ten = tf.stop_gradient(weights)\n",
    "loss = tf.reshape(loss, (4,)) * weights_ten\n",
    "## calculate new priorities\n",
    "new_priorities = tf.stop_gradient(loss).numpy() + 1e-5\n",
    "loss = tf.convert_to_tensor(tf.math.reduce_mean(loss))\n",
    "print(new_priorities)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57042f32",
   "metadata": {},
   "source": [
    "### Cek Soft Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed429466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "    Params\n",
    "    ======\n",
    "        local_model: model which the weights will be copied from\n",
    "        target_model: model which weights will be copied to\n",
    "        tau (float): interpolation parameter\n",
    "    \"\"\"\n",
    "    for t_layer, layer in zip(target_model.layers, local_model.layers):\n",
    "        ## initiate list\n",
    "        temp_w_arr = []\n",
    "        for t_weights, weights in zip(t_layer.get_weights(), layer.get_weights()):\n",
    "            ## fill the array list\n",
    "            temp_w_arr.append(weights * tau + (1.0 - tau)*t_weights)\n",
    "        ## copy the weights\n",
    "        t_layer.set_weights(temp_w_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
