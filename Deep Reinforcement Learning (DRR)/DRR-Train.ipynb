{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db3773cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from model import PMF, DRRAveStateRepresentation, Actor, Critic\n",
    "\n",
    "from utils.prioritized_replay_buffer import NaivePrioritizedReplayMemory, Transition\n",
    "from utils.history_buffer import HistoryBuffer\n",
    "from utils.general import export_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c50c81e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRRTrainer(object):\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 actor_function,\n",
    "                 critic_function,\n",
    "                 state_rep_function,\n",
    "                 reward_function,\n",
    "                 users,\n",
    "                 items,\n",
    "                 train_data,\n",
    "                 test_data,\n",
    "                 user_embeddings,\n",
    "                 item_embeddings):\n",
    "        \n",
    "        ## importing reward function\n",
    "        self.reward_function = reward_function\n",
    "        ## importing training and testing data\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        ## importing users and items\n",
    "        self.users = users\n",
    "        self.items = items\n",
    "        ## importing user and item embeddings\n",
    "        self.user_embeddings = user_embeddings\n",
    "        self.item_embeddings = item_embeddings\n",
    "        ## declaring index identifier for dataset\n",
    "        ## u for user, i for item, r for reward/rating\n",
    "        self.u = 0\n",
    "        self.i = 1\n",
    "        self.r = 2\n",
    "        \n",
    "        ## dimensions\n",
    "        ## self.item_embeddings already hold the weights array\n",
    "        ## this should be 100\n",
    "        self.item_features = self.item_embeddings.shape[1]\n",
    "        self.user_features = self.user_embeddings.shape[1]\n",
    "        \n",
    "        ## number of user and items\n",
    "        self.n_items = self.item_embeddings.shape[0]\n",
    "        self.n_users = self.user_embeddings.shape[0]\n",
    "        \n",
    "        ## the shape of state space, action space\n",
    "        ## this should be 300\n",
    "        self.state_shape = 3 * self.item_features\n",
    "        ## this should be 100\n",
    "        self.action_shape = self.item_features\n",
    "        \n",
    "        self.critic_output_shape = 1\n",
    "        self.config = config\n",
    "        ## Data dimensions Extracted\n",
    "        \n",
    "        ## instantiate a drravestaterepresentation\n",
    "        self.state_rep_net = state_rep_function(self.config.history_buffer_size,\n",
    "                                                self.item_features,\n",
    "                                                self.user_features)\n",
    "        \n",
    "        ## instantiate actor and target actor networks\n",
    "        self.actor_net = actor_function(self.state_shape, self.action_shape)                                \n",
    "        self.target_actor_net = actor_function(self.state_shape, self.action_shape)\n",
    "        \n",
    "        ## instantiate critic and target critics networks\n",
    "        self.critic_net = critic_function(self.action_shape,\n",
    "                                          self.state_shape,\n",
    "                                          self.critic_output_shape)\n",
    "        \n",
    "        self.target_critic_net = critic_function(self.action_shape,\n",
    "                                                 self.state_shape,\n",
    "                                                 self.critic_output_shape)\n",
    "        print(\"Actor-Critic model has successfully instantiated\")\n",
    "        \n",
    "        self.state_rep_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=self.config.lr_state_rep)\n",
    "        \n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=self.config.lr_actor)\n",
    "        \n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=self.config.lr_critic)\n",
    "        \n",
    "        print(\"DRR Instantiazed\")\n",
    "        \n",
    "    def learn(self):\n",
    "        # Initialize buffers\n",
    "        print(\"NPRM and History Buffer Initialized\")\n",
    "        replay_buffer = NaivePrioritizedReplayMemory(self.config.replay_buffer_size,\n",
    "                                                     prob_alpha=self.config.prob_alpha)\n",
    "\n",
    "        history_buffer = HistoryBuffer(self.config.history_buffer_size)\n",
    "        \n",
    "        # Initialize trackers\n",
    "        # initialize timesteps and epoch\n",
    "        timesteps = 0\n",
    "        epoch = 0\n",
    "        ## this variable is for episode\n",
    "        eps_slope = abs(self.config.eps_start - self.config.eps)/self.config.eps_steps\n",
    "        eps = self.config.eps_start\n",
    "        ## this variable is to hold the losses along the time\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        ## this variable is to hold the episodic rewards\n",
    "        epi_rewards = []\n",
    "        epi_avg_rewards = []\n",
    "        \n",
    "        e_arr = []\n",
    "        \n",
    "        ## this variable holds the user index\n",
    "        ## got from the dictionary\n",
    "        user_idxs = np.array(list(self.users.values()))\n",
    "        np.random.shuffle(user_idxs)\n",
    "        \n",
    "        ## loop all the users based on indexes\n",
    "        for idx, e in enumerate(user_idxs):\n",
    "            ## starting the episodes\n",
    "            \n",
    "            ## the loops stop when timesteps-learning_start\n",
    "            ## is bigger than the max timesteps\n",
    "            if timesteps - self.config.learning_start > self.config.max_timesteps_train:\n",
    "                break\n",
    "            \n",
    "            ## extracting positive user reviews\n",
    "            ## e variable is an element right now\n",
    "            user_reviews = self.train_data[self.train_data[:, self.u] == e]\n",
    "            pos_user_reviews = user_reviews[user_reviews[:, self.r] > 0]\n",
    "            \n",
    "            ## check if the user ratings doesn't have enough positive review\n",
    "            ## in this case history_buffer_size is 4\n",
    "            ## get the shape object and 0 denote the row index\n",
    "            if pos_user_reviews.shape[0] < self.config.history_buffer_size:\n",
    "                continue\n",
    "                \n",
    "            candidate_items = tf.identity(tf.stop_gradient(self.item_embeddings))\n",
    "            \n",
    "            ## extracting user embedding tensors\n",
    "            user_emb = self.user_embeddings[e]\n",
    "            \n",
    "            ## fill history buffer with positive item embeddings\n",
    "            ## and remove item embeddings from candidate item sets\n",
    "            ignored_items = []\n",
    "            \n",
    "            ## history_buffer_size has size of n items\n",
    "            ## in this case 5\n",
    "            for i in range(self.config.history_buffer_size):\n",
    "                emb = candidate_items[tf.cast(pos_user_reviews[i, self.i], dtype='int32')]\n",
    "                history_buffer.push(tf.identity(tf.stop_gradient(emb)))\n",
    "                \n",
    "            ## initialize rewards list\n",
    "            rewards = []\n",
    "            \n",
    "            ## starting item index\n",
    "            t = 0\n",
    "            \n",
    "            ## declaring the needed variable\n",
    "            state = None\n",
    "            action = None\n",
    "            reward = None\n",
    "            next_state = None\n",
    "            \n",
    "            while t < self.config.episode_length:\n",
    "                ## observing the current state\n",
    "                ## choose action according to actor network explorations\n",
    "                \n",
    "                ## inference calls start here\n",
    "                \n",
    "                if eps > self.config.eps:\n",
    "                    eps -= eps_slope\n",
    "                else:\n",
    "                    eps = self.config.eps\n",
    "                \n",
    "                ## state is the result of DRRAve model inference\n",
    "                ## history_buffer.to_list get the list of previous items\n",
    "                ## state representaton has the size (300, )\n",
    "                state = self.state_rep_net(user_emb, tf.stack(history_buffer.to_list()))\n",
    "                if np.random.uniform(0, 1) < eps:\n",
    "                    action = tf.convert_to_tensor(np.random.rand(self.action_shape), dtype='float32')\n",
    "                    ranking_scores = candidate_items @ tf.reshape(action, (action.shape[0], 1))\n",
    "                else:\n",
    "                    action = self.actor_net(tf.stop_gradient(state), training=False)\n",
    "                    ranking_scores = candidate_items @ tf.reshape(action, (action.shape[1], 1))\n",
    "                    \n",
    "                ## calculating ranking scores accross items, discard ignored items\n",
    "                \n",
    "                if len(ignored_items) > 0:\n",
    "                    rec_items = tf.stack(ignored_items)\n",
    "                else:\n",
    "                    rec_items = []\n",
    "                \n",
    "                ## setting value of negative infinite\n",
    "#                 ranking_scores[rec_items] = -sys.maxsize -1\n",
    "                \n",
    "                ## get the recommended items\n",
    "                ## first get the maximum value index\n",
    "                ## then get the items by index from candidate items\n",
    "                ranking_scores = tf.reshape(ranking_scores, (ranking_scores.shape[0],))\n",
    "                rec_item_idx = tf.math.argmax(ranking_scores)\n",
    "                rec_item_emb = candidate_items[rec_item_idx]\n",
    "                \n",
    "                ## get the item reward\n",
    "                if tf.cast(rec_item_idx, 'float32') in user_reviews[:, self.i]:\n",
    "                    ## get the reward from rating in the dataset\n",
    "                    ## if the user is rating the item\n",
    "                    reward = user_reviews[user_reviews[:, self.i] == rec_item_idx, self.r]\n",
    "                else:\n",
    "                    if self.config.zero_reward:\n",
    "                        reward = tf.convert_to_tensor(0)\n",
    "                    else:\n",
    "                        reward = self.reward_function(tf.convert_to_tensor(e), rec_item_idx)\n",
    "                \n",
    "                ## track the episode rewards\n",
    "                rewards.append(reward.item())\n",
    "                \n",
    "                ## add item to history buffer if positive reviews\n",
    "                if reward > 0:\n",
    "                    history_buffer.push(tf.stop_gradient(rec_item_emb))\n",
    "                    \n",
    "                    next_state = self.state_rep_net(user_emb, tf.stack(history_buffer.to_list()), training=False)\n",
    "                else:\n",
    "                    ## keep the history buffer the same\n",
    "                    ## the next state is the current state\n",
    "                    next_state = tf.stop_gradient(state)\n",
    "                \n",
    "                ## remove new items from future recommendation\n",
    "                ignored_items.append(tf.conver_to_tensor(rec_item_idx))\n",
    "                \n",
    "                ## add the (state, action, reward, next_state)\n",
    "                ## to the experience replay\n",
    "                replay_buffer.push(state, action, next_state, reward)\n",
    "                \n",
    "                ## Inference calling stops here\n",
    "                ## Training start here\n",
    "                if(timesteps > self.config.learning_start) and (len(replay_buffer) >= self.config.batch_size) and (timesteps % self.config.learning_freq == 0):\n",
    "                    critic_loss, actor_loss, critic_params_norm = self.training_step(timesteps,\n",
    "                                                                                     replay_buffer,\n",
    "                                                                                     True\n",
    "                                                                                     )\n",
    "                    ## storing the losses along the time\n",
    "                    actor_losses.append(actor_loss)\n",
    "                    critic_losses.append(critic_loss)\n",
    "                    \n",
    "                    ## outputting the result\n",
    "                    if timesteps % self.config.log_freq == 0:\n",
    "                        if len(rewards) > 0:\n",
    "                            print(\n",
    "                                f'Timestep {timesteps - self.config.learning_start} | '\n",
    "                                f'Episode {epoch} | '\n",
    "                                f'Mean Ep R '\n",
    "                                f'{np.mean(rewards):.4f} | '\n",
    "                                f'Max R {np.max(rewards):.4f} | '\n",
    "                                f'Critic Params Norm {critic_params_norm:.4f} | '\n",
    "                                f'Actor Loss {actor_loss:.4f} | '\n",
    "                                f'Critic Loss {critic_loss:.4f} | ')\n",
    "                            sys.stdout.flush()\n",
    "            \n",
    "                ## housekeeping\n",
    "                t += 1\n",
    "                timesteps += 1\n",
    "            \n",
    "                ## end of timesteps\n",
    "            ## end of episodes\n",
    "            if timesteps - self.config.learning_start > t:\n",
    "                epoch += 1\n",
    "                e_arr.append(epoch)\n",
    "                epi_rewards.append(np.sum(rewards))\n",
    "                epi_avg_rewards.append(np.mean(rewards))\n",
    "        \n",
    "        print(\"Training Finished\")\n",
    "        return actor_losses, critic_losses, epi_avg_rewards\n",
    "    \n",
    "    def training_step(self, t, replay_buffer, training):\n",
    "        ## create batches\n",
    "        ## from utils programs\n",
    "        # Create batches by calling sample methods\n",
    "        transitions, indicies, weights = replay_buffer.sample(self.config.batch_size, beta=self.config.beta)\n",
    "        ## create the tuple using Transition function\n",
    "        batch = Transition(*zip(*transtitions))\n",
    "        \n",
    "        ## preparing the batch for each data\n",
    "        ## the concat function will flatten the data\n",
    "        ## the reshape will reshape the data so that it receive 64 rows\n",
    "        next_state_batch = tf.reshape(tf.concat(batch.next_state), (self.config.batch_size, -1))\n",
    "        state_batch = tf.reshape(tf.concat(batch.state), (self.config.batch_size, -1))\n",
    "        action_batch = tf.reshape(tf.concat(batch.action), (self.config.batch_size, -1))\n",
    "        reward_batch = tf.reshape(tf.concat(batch.reward), (self.config.batch_size, -1))\n",
    "        \n",
    "        ## updating the critic networks\n",
    "        with tf.GradientTape() as tape:\n",
    "            critic_loss, new_priorities = self.compute_prioritized_dqn_loss(state_batch.detach(),\n",
    "                                                                            action_batch,\n",
    "                                                                            reward_batch,\n",
    "                                                                            next_state_batch,\n",
    "                                                                            weights)\n",
    "        ## apply the gradient\n",
    "        grads = tape.gradient(critic_loss, self.critic_net.trainable_variables)\n",
    "        ## step the optimizers\n",
    "        self.critic_optimizer.apply_gradients(zip(grads, self.critic_net.trainable_variables))\n",
    "        \n",
    "        ## updating the actor networks\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions_pred = self.actor_net(state_batch)\n",
    "            actor_loss = -self.critic_net(tf.stop_gradient(state_batch), actions_pred).numpy().mean()\n",
    "        \n",
    "        ## compute the gradient\n",
    "        grads = tape.gradient(actor_loss, self.actor_net.trainable_variables)\n",
    "        ## apply the step to the optimizers\n",
    "        self.actor_optimizer.apply_gradients(zip(grads, self.actor_net.trainable_variables))\n",
    "        self.state_rep_optimizer.apply_gradients(zip(grads, self.state_rep_net.trainable_variables))\n",
    "        ## minimizing the loss\n",
    "        \n",
    "        self.soft_update(self.critic_net, self.target_critic_net, self.config.tau)\n",
    "        self.soft_update(self.actor_net, self.target_actor_net, self.config.tau)\n",
    "        \n",
    "        return critic_loss.item(), actor_loss.item(), critic_param_norm\n",
    "        \n",
    "        \n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_weights, local_weights in zip(reward_function_copy.layers, reward_function.layers):\n",
    "            temp_w = local_weights.get_weights()[0] * tau + (1.0 - tau) * target_weights.get_weights()[0]\n",
    "            target_weights.set_weights([temp_w])\n",
    "    \n",
    "    @tf.function    \n",
    "    def compute_prioritized_dqn_loss(self,\n",
    "                                     state_batch,\n",
    "                                     action_batch,\n",
    "                                     reward_batch,\n",
    "                                     next_state_batch,\n",
    "                                     weights):\n",
    "        '''\n",
    "        :param state_batch: (tensor) shape = (batch_size x state_dims),\n",
    "                The batched tensor of states collected during\n",
    "                training (i.e. s)\n",
    "        :param action_batch: (LongTensor) shape = (batch_size,)\n",
    "                The actions that you actually took at each step (i.e. a)\n",
    "        :param reward_batch: (tensor) shape = (batch_size,)\n",
    "                The rewards that you actually got at each step (i.e. r)\n",
    "        :param next_state_batch: (tensor) shape = (batch_size x state_dims),\n",
    "                The batched tensor of next states collected during\n",
    "                training (i.e. s')\n",
    "        :param weights: (tensor) shape = (batch_size,)\n",
    "                Weights for each batch item w.r.t. prioritized experience replay buffer\n",
    "        :return: loss: (torch tensor) shape = (1),\n",
    "                 new_priorities: (numpy array) shape = (batch_size,)\n",
    "        '''\n",
    "        ## create batches\n",
    "        next_action = self.target_actor_net(next_state_batch, training=False)\n",
    "        q_target = self.target_critic_net(next_state_batch, next_action, training=False)\n",
    "        \n",
    "        ## logits\n",
    "        logits = reward_batch + self.config.gamma * q_target\n",
    "        \n",
    "        ## get q values from the current state\n",
    "        q_vals = self.critic_net(state.batch, action_batch)\n",
    "        \n",
    "        ## calculate loss\n",
    "        loss = y - q_vals\n",
    "        ## because loss is tensor shape\n",
    "        ## we can extract the numpy value\n",
    "        loss = np.power(loss.numpy().flatten(), 2)\n",
    "        weights_ten = tf.stop_grad(tf.convert_to_tensor(weights))\n",
    "        loss = loss * weights_ten\n",
    "        \n",
    "        ## calculate new priorities\n",
    "        new_priorities = tf.stop_grad(loss + 1e-5)\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        return loss, new_priorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fde18b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_scores = item_embeddings @ tf.reshape(action, (action.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ae407170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.009489409>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(ranking_scores, (ranking_scores.shape[0],))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a140b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_scores = tf.reshape(ranking_scores, (ranking_scores.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "612fafeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1682,), dtype=float32, numpy=\n",
       "array([ 0.00948941,  0.04992265, -0.03665902, ...,  0.19314665,\n",
       "       -0.01089569, -0.21394743], dtype=float32)>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9a08f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_idx = tf.math.argmax(ranking_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed12bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = Actor(300, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "286627e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = actor_net(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3699b676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 100])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7aa78578",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = tf.convert_to_tensor(np.random.rand(300, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb4d81f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1682, 100), dtype=float32, numpy=\n",
       "array([[ 0.02927838, -0.01858752, -0.01971002, ...,  0.01666964,\n",
       "         0.01175865,  0.01821153],\n",
       "       [ 0.04730224, -0.01002619, -0.04633345, ..., -0.05041706,\n",
       "        -0.0197012 , -0.05130218],\n",
       "       [-0.01143864, -0.01942264, -0.02575671, ...,  0.0525948 ,\n",
       "        -0.00352616,  0.01375635],\n",
       "       ...,\n",
       "       [ 0.0363218 , -0.00957389,  0.01001419, ...,  0.00554959,\n",
       "         0.0330762 ,  0.05551627],\n",
       "       [ 0.11699422,  0.04615022, -0.0305032 , ..., -0.00461498,\n",
       "         0.0034758 , -0.06877374],\n",
       "       [ 0.09200613,  0.00553454,  0.02183542, ...,  0.04724813,\n",
       "        -0.09510388,  0.11581472]], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_embeddings @ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "856fd195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:(80000, 3), Test Data:(20000, 3)\n",
      "user embedding has shape (100,) and item embedding has shape (100,)\n",
      "Actor-Critic model has successfully instantiated\n",
      "DRR Instantiazed\n",
      "Start Training\n",
      "NPRM and History Buffer Initialized\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor conversion requested dtype float64 for Tensor with dtype float32: <tf.Tensor: shape=(), dtype=float32, numpy=1645.0>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3656/3445412696.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Start Training\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3656/1229169271.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m                 \u001b[1;31m## get the item reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrec_item_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0muser_reviews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m                     \u001b[1;31m## get the reward from rating in the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m                     \u001b[1;31m## if the user is rating the item\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1586\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1587\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1588\u001b[1;33m       raise ValueError(\n\u001b[0m\u001b[0;32m   1589\u001b[0m           \u001b[1;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1590\u001b[0m           (dtype.name, value.dtype.name, value))\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor conversion requested dtype float64 for Tensor with dtype float32: <tf.Tensor: shape=(), dtype=float32, numpy=1645.0>"
     ]
    }
   ],
   "source": [
    "class config():\n",
    "    ## hyperparameters\n",
    "    ## setting the batch_size\n",
    "    batch_size = 64\n",
    "    gamma = 0.9\n",
    "    replay_buffer_size = 100000\n",
    "    history_buffer_size = 5\n",
    "    learning_start = 32\n",
    "    learning_freq = 1\n",
    "    ## learning rate for each model networks\n",
    "    lr_state_rep = 0.001\n",
    "    lr_actor = 0.0001\n",
    "    lr_critic = 0.001\n",
    "    \n",
    "    eps_start = 1\n",
    "    eps = 0.1\n",
    "    eps_steps = 10000\n",
    "    eps_eval = 0.1\n",
    "    episode_length = 10\n",
    "    \n",
    "    tau = 0.01 # inital 0.001\n",
    "    beta = 0.4\n",
    "    prob_alpha = 0.3\n",
    "    \n",
    "    max_timesteps_train = 260000\n",
    "    max_epochs_offline = 500\n",
    "    max_timesteps_online = 20000\n",
    "    embedding_feature_size = 100\n",
    "    \n",
    "    train_ratio = 0.8\n",
    "    weight_decay = 0.01\n",
    "    clip_val = 1.0\n",
    "    log_freq = 100\n",
    "    saving_freq = 1000\n",
    "    zero_reward = False\n",
    "    \n",
    "## First importing the data\n",
    "users = pickle.load(open('Dataset/user_id_to_num_mov.pkl', 'rb'))\n",
    "items = pickle.load(open('Dataset/movie_id_to_num_mov.pkl', 'rb'))\n",
    "data = np.load('Dataset/data.npy')\n",
    "\n",
    "## hold the length of the data\n",
    "n_users = len(users)\n",
    "n_items = len(items)\n",
    "\n",
    "## don't forget to normalize the data first\n",
    "data[:, 2] = 0.5 * (data[:, 2] - 3)\n",
    "\n",
    "## split and shuffle the data\n",
    "np.random.shuffle(data)\n",
    "## split the data\n",
    "## ratio should be 0.8\n",
    "train_data = tf.convert_to_tensor(data[:int(config.train_ratio * data.shape[0])])\n",
    "test_data = tf.convert_to_tensor(data[int(config.train_ratio * data.shape[0]):])\n",
    "print(\"Train Data:{}, Test Data:{}\".format(np.shape(train_data), np.shape(test_data)))\n",
    "\n",
    "## hold the PMF model\n",
    "## get the user and item embeddings\n",
    "reward_function = PMF(n_users, n_items, config.embedding_feature_size)\n",
    "## need to flow some data to build the model\n",
    "reward_function(1, 1)\n",
    "## loading the whole layer weights\n",
    "reward_function.load_weights('trained/adam/pmf_150_adam')\n",
    "## freeze the model, because it will be used for inference\n",
    "reward_function.trainable = False\n",
    "\n",
    "## take the embedding layers weight\n",
    "## and split the user and item weights\n",
    "user_embeddings = tf.convert_to_tensor(reward_function.user_embedding.get_weights()[0])\n",
    "item_embeddings = tf.convert_to_tensor(reward_function.item_embedding.get_weights()[0])\n",
    "## output\n",
    "print(\"user embedding has shape {} and item embedding has shape {}\"\n",
    "      .format(np.shape(user_embeddings[0]), np.shape(item_embeddings[0])))\n",
    "\n",
    "## hold the model in the variable\n",
    "## so it can be tracked\n",
    "state_rep_function = DRRAveStateRepresentation\n",
    "actor_function = Actor\n",
    "critic_function = Critic\n",
    "\n",
    "## initialize DRRTrain Class\n",
    "trainer = DRRTrainer(config,\n",
    "                     actor_function,\n",
    "                     critic_function,\n",
    "                     state_rep_function,\n",
    "                     reward_function,\n",
    "                     users,\n",
    "                     items,\n",
    "                     train_data,\n",
    "                     test_data,\n",
    "                     user_embeddings,\n",
    "                     item_embeddings)\n",
    "\n",
    "print(\"Start Training\")\n",
    "trainer.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64c0bf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.7724931>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_function_copy = PMF(n_users, n_items, config.embedding_feature_size)\n",
    "reward_function_copy(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb9d51c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4862dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.convert_to_tensor([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ca03389",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_weights, local_weights in zip(reward_function_copy.layers, reward_function.layers):\n",
    "    temp_w = local_weights.get_weights()[0] * tau + (1.0 - tau) * target_weights.get_weights()[0]\n",
    "    target_weights.set_weights([temp_w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9614a128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.02264013, -0.00803596, -0.00766289, ...,  0.00541727,\n",
      "         0.01059815, -0.01396531],\n",
      "       [ 0.00427928,  0.00162332,  0.05373245, ...,  0.03625406,\n",
      "        -0.07006924,  0.01003331],\n",
      "       [ 0.00060505, -0.0082522 ,  0.0349106 , ..., -0.01670074,\n",
      "        -0.00933639,  0.00602781],\n",
      "       ...,\n",
      "       [-0.00582577, -0.04501645, -0.0346372 , ..., -0.0845314 ,\n",
      "        -0.04627491,  0.05608604],\n",
      "       [-0.04641426,  0.01256191,  0.01490307, ..., -0.00078481,\n",
      "         0.03249994, -0.03189057],\n",
      "       [ 0.01732369, -0.01391567, -0.00765261, ...,  0.0080509 ,\n",
      "        -0.02056003,  0.0187523 ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "reward_function_copy.layers[0].set_weights(reward_function.layers[0].get_weights())\n",
    "print([reward_function_copy.layers[0].get_weights()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f988d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.2640128e-03, -8.0359605e-04, -7.6628890e-04, ...,\n",
       "         5.4172706e-04,  1.0598153e-03, -1.3965314e-03],\n",
       "       [ 4.2792820e-04,  1.6233191e-04,  5.3732456e-03, ...,\n",
       "         3.6254060e-03, -7.0069241e-03,  1.0033314e-03],\n",
       "       [ 6.0504572e-05, -8.2521979e-04,  3.4910606e-03, ...,\n",
       "        -1.6700743e-03, -9.3363895e-04,  6.0278119e-04],\n",
       "       ...,\n",
       "       [-5.8257702e-04, -4.5016450e-03, -3.4637197e-03, ...,\n",
       "        -8.4531410e-03, -4.6274913e-03,  5.6086038e-03],\n",
       "       [-4.6414263e-03,  1.2561911e-03,  1.4903073e-03, ...,\n",
       "        -7.8481113e-05,  3.2499942e-03, -3.1890569e-03],\n",
       "       [ 1.7323690e-03, -1.3915672e-03, -7.6526118e-04, ...,\n",
       "         8.0509018e-04, -2.0560033e-03,  1.8752298e-03]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_w = reward_function.layers[0].get_weights()[0] * 0.1\n",
    "temp_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c17117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "195c1924",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight.append(temp_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8ed0cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-2.2640128e-03, -8.0359605e-04, -7.6628890e-04, ...,\n",
       "          5.4172706e-04,  1.0598153e-03, -1.3965314e-03],\n",
       "        [ 4.2792820e-04,  1.6233191e-04,  5.3732456e-03, ...,\n",
       "          3.6254060e-03, -7.0069241e-03,  1.0033314e-03],\n",
       "        [ 6.0504572e-05, -8.2521979e-04,  3.4910606e-03, ...,\n",
       "         -1.6700743e-03, -9.3363895e-04,  6.0278119e-04],\n",
       "        ...,\n",
       "        [-5.8257702e-04, -4.5016450e-03, -3.4637197e-03, ...,\n",
       "         -8.4531410e-03, -4.6274913e-03,  5.6086038e-03],\n",
       "        [-4.6414263e-03,  1.2561911e-03,  1.4903073e-03, ...,\n",
       "         -7.8481113e-05,  3.2499942e-03, -3.1890569e-03],\n",
       "        [ 1.7323690e-03, -1.3915672e-03, -7.6526118e-04, ...,\n",
       "          8.0509018e-04, -2.0560033e-03,  1.8752298e-03]], dtype=float32)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73f83d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_function_copy.layers[0].set_weights(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b93d71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-2.2640128e-03, -8.0359605e-04, -7.6628890e-04, ...,\n",
       "          5.4172706e-04,  1.0598153e-03, -1.3965314e-03],\n",
       "        [ 4.2792820e-04,  1.6233191e-04,  5.3732456e-03, ...,\n",
       "          3.6254060e-03, -7.0069241e-03,  1.0033314e-03],\n",
       "        [ 6.0504572e-05, -8.2521979e-04,  3.4910606e-03, ...,\n",
       "         -1.6700743e-03, -9.3363895e-04,  6.0278119e-04],\n",
       "        ...,\n",
       "        [-5.8257702e-04, -4.5016450e-03, -3.4637197e-03, ...,\n",
       "         -8.4531410e-03, -4.6274913e-03,  5.6086038e-03],\n",
       "        [-4.6414263e-03,  1.2561911e-03,  1.4903073e-03, ...,\n",
       "         -7.8481113e-05,  3.2499942e-03, -3.1890569e-03],\n",
       "        [ 1.7323690e-03, -1.3915672e-03, -7.6526118e-04, ...,\n",
       "          8.0509018e-04, -2.0560033e-03,  1.8752298e-03]], dtype=float32)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_function_copy.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcbfa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_layers, layers in zip(reward_function_copy.layers, reward_function.layers):\n",
    "    target_layers.set_weights(layers.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd07fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_layers in reward_function_copy.layers:\n",
    "    print(target_layers.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b3ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.power(a.numpy().flatten(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reshape(tf.concat([item_1, item_2, item_3], 0), (3, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0ff5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_e = user_embeddings[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
