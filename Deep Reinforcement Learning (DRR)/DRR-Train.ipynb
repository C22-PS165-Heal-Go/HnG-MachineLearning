{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db3773cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from model import PMF, DRRAveStateRepresentation, Actor, Critic\n",
    "\n",
    "from utils.prioritized_replay_buffer import NaivePrioritizedReplayMemory, Transition\n",
    "from utils.history_buffer import HistoryBuffer\n",
    "from utils.general import export_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c50c81e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRRTrainer(object):\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 actor_function,\n",
    "                 critic_function,\n",
    "                 state_rep_function,\n",
    "                 reward_function,\n",
    "                 users,\n",
    "                 items,\n",
    "                 train_data,\n",
    "                 test_data,\n",
    "                 user_embeddings,\n",
    "                 item_embeddings):\n",
    "        \n",
    "        ## importing reward function\n",
    "        self.reward_function = reward_function\n",
    "        ## importing training and testing data\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        ## importing users and items\n",
    "        self.users = users\n",
    "        self.items = items\n",
    "        ## importing user and item embeddings\n",
    "        self.user_embeddings = user_embeddings\n",
    "        self.item_embeddings = item_embeddings\n",
    "        ## declaring index identifier for dataset\n",
    "        ## u for user, i for item, r for reward/rating\n",
    "        self.u = 0\n",
    "        self.i = 1\n",
    "        self.r = 2\n",
    "        \n",
    "        ## dimensions\n",
    "        ## self.item_embeddings already hold the weights array\n",
    "        ## this should be 100\n",
    "        self.item_features = self.item_embeddings.shape[1]\n",
    "        self.user_features = self.user_embeddings.shape[1]\n",
    "        \n",
    "        ## number of user and items\n",
    "        self.n_items = self.item_embeddings.shape[0]\n",
    "        self.n_users = self.user_embeddings.shape[0]\n",
    "        \n",
    "        ## the shape of state space, action space\n",
    "        ## this should be 300\n",
    "        self.state_shape = 3 * self.item_features\n",
    "        ## this should be 100\n",
    "        self.action_shape = self.item_features\n",
    "        \n",
    "        self.critic_output_shape = 1\n",
    "        self.config = config\n",
    "        ## Data dimensions Extracted\n",
    "        \n",
    "        ## instantiate a drravestaterepresentation\n",
    "        self.state_rep_net = state_rep_function(self.config.history_buffer_size,\n",
    "                                                self.item_features,\n",
    "                                                self.user_features)\n",
    "        \n",
    "        ## instantiate actor and target actor networks\n",
    "        self.actor_net = actor_function(self.state_shape, self.action_shape)                                \n",
    "        self.target_actor_net = actor_function(self.state_shape, self.action_shape)\n",
    "        \n",
    "        ## instantiate critic and target critics networks\n",
    "        self.critic_net = critic_function(self.action_shape,\n",
    "                                          self.state_shape,\n",
    "                                          self.critic_output_shape)\n",
    "        \n",
    "        self.target_critic_net = critic_function(self.action_shape,\n",
    "                                                 self.state_shape,\n",
    "                                                 self.critic_output_shape)\n",
    "        \n",
    "        ## data flow for building the model\n",
    "        flow_item = tf.convert_to_tensor(np.random.rand(5, 100), dtype='float32')\n",
    "        flow_state = tf.convert_to_tensor(np.random.rand(1, 300), dtype='float32')\n",
    "        flow_action = tf.convert_to_tensor(np.random.rand(1, 100), dtype='float32')\n",
    "        \n",
    "        ## flowing the data into the model to build the model\n",
    "        self.state_rep_net(user_embeddings[0], flow_item)\n",
    "        self.actor_net(flow_state)\n",
    "        self.target_actor_net(flow_state)\n",
    "        self.critic_net(flow_state, flow_action)\n",
    "        self.target_critic_net(flow_state, flow_action)\n",
    "        print(\"Actor-Critic model has successfully instantiated\")\n",
    "        \n",
    "        self.state_rep_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=self.config.lr_state_rep)\n",
    "        \n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=self.config.lr_actor)\n",
    "        \n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=self.config.lr_critic)\n",
    "        \n",
    "        print(\"DRR Instantiazed\")\n",
    "        \n",
    "    def learn(self):\n",
    "        # Initialize buffers\n",
    "        print(\"NPRM and History Buffer Initialized\")\n",
    "        replay_buffer = NaivePrioritizedReplayMemory(self.config.replay_buffer_size,\n",
    "                                                     prob_alpha=self.config.prob_alpha)\n",
    "\n",
    "        history_buffer = HistoryBuffer(self.config.history_buffer_size)\n",
    "        \n",
    "        # Initialize trackers\n",
    "        # initialize timesteps and epoch\n",
    "        timesteps = 0\n",
    "        epoch = 0\n",
    "        ## this variable is for episode\n",
    "        eps_slope = abs(self.config.eps_start - self.config.eps)/self.config.eps_steps\n",
    "        eps = self.config.eps_start\n",
    "        ## this variable is to hold the losses along the time\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        ## this variable is to hold the episodic rewards\n",
    "        epi_rewards = []\n",
    "        epi_avg_rewards = []\n",
    "        \n",
    "        e_arr = []\n",
    "        \n",
    "        ## this variable holds the user index\n",
    "        ## got from the dictionary\n",
    "        user_idxs = np.array(list(self.users.values()))\n",
    "        np.random.shuffle(user_idxs)\n",
    "        \n",
    "        ## loop all the users based on indexes\n",
    "        for idx, e in enumerate(user_idxs):\n",
    "            ## starting the episodes\n",
    "            \n",
    "            ## the loops stop when timesteps-learning_start\n",
    "            ## is bigger than the max timesteps\n",
    "            if timesteps - self.config.learning_start > self.config.max_timesteps_train:\n",
    "                break\n",
    "            \n",
    "            ## extracting positive user reviews\n",
    "            ## e variable is an element right now\n",
    "            user_reviews = self.train_data[self.train_data[:, self.u] == e]\n",
    "            pos_user_reviews = user_reviews[user_reviews[:, self.r] > 0]\n",
    "            \n",
    "            ## check if the user ratings doesn't have enough positive review\n",
    "            ## in this case history_buffer_size is 4\n",
    "            ## get the shape object and 0 denote the row index\n",
    "            if pos_user_reviews.shape[0] < self.config.history_buffer_size:\n",
    "                continue\n",
    "                \n",
    "            candidate_items = tf.identity(tf.stop_gradient(self.item_embeddings))\n",
    "            \n",
    "            ## extracting user embedding tensors\n",
    "            user_emb = self.user_embeddings[e]\n",
    "            \n",
    "            ## fill history buffer with positive item embeddings\n",
    "            ## and remove item embeddings from candidate item sets\n",
    "            ignored_items = []\n",
    "            \n",
    "            ## history_buffer_size has size of n items\n",
    "            ## in this case 5\n",
    "            for i in range(self.config.history_buffer_size):\n",
    "                emb = candidate_items[tf.cast(pos_user_reviews[i, self.i], dtype='int32')]\n",
    "                history_buffer.push(tf.identity(tf.stop_gradient(emb)))\n",
    "                \n",
    "            ## initialize rewards list\n",
    "            rewards = []\n",
    "            \n",
    "            ## starting item index\n",
    "            t = 0\n",
    "            \n",
    "            ## declaring the needed variable\n",
    "            state = None\n",
    "            action = None\n",
    "            reward = None\n",
    "            next_state = None\n",
    "            \n",
    "            while t < self.config.episode_length:\n",
    "                ## observing the current state\n",
    "                ## choose action according to actor network explorations\n",
    "                \n",
    "                ## inference calls start here\n",
    "                \n",
    "                if eps > self.config.eps:\n",
    "                    eps -= eps_slope\n",
    "                else:\n",
    "                    eps = self.config.eps\n",
    "                \n",
    "                ## state is the result of DRRAve model inference\n",
    "                ## history_buffer.to_list get the list of previous items\n",
    "                ## state representaton has the size (300, )\n",
    "                state = self.state_rep_net(user_emb, tf.stack(history_buffer.to_list()))\n",
    "                if np.random.uniform(0, 1) < eps:\n",
    "                    action = tf.convert_to_tensor(np.random.rand(1, self.action_shape), dtype='float32') \n",
    "                else:\n",
    "                    action = self.actor_net(tf.reshape(tf.stop_gradient(state), [1, state.shape[0]]), training=False)\n",
    "                    \n",
    "                ranking_scores = candidate_items @ tf.reshape(action, (action.shape[1], 1))\n",
    "                ## calculating ranking scores accross items, discard ignored items\n",
    "                \n",
    "                if len(ignored_items) > 0:\n",
    "                    rec_items = tf.stack(ignored_items)\n",
    "                else:\n",
    "                    rec_items = []\n",
    "            \n",
    "                ## setting value of negative infinite\n",
    "                ranking_scores[rec_items] = -99999999\n",
    "                \n",
    "                ## get the recommended items\n",
    "                ## first get the maximum value index\n",
    "                ## then get the items by index from candidate items\n",
    "                ranking_scores = tf.reshape(ranking_scores, (ranking_scores.shape[0],))\n",
    "                rec_item_idx = tf.math.argmax(ranking_scores)\n",
    "                rec_item_emb = candidate_items[rec_item_idx]\n",
    "                \n",
    "                ## get the item reward\n",
    "                if tf.cast(rec_item_idx, 'float64') in user_reviews[:, self.i]:\n",
    "                    ## get the reward from rating in the dataset\n",
    "                    ## if the user is rating the item\n",
    "                    user_rec_item_idx = np.where(user_reviews[:, self.i] == float(rec_item_idx))[0][0]\n",
    "                    reward = user_reviews[user_rec_item_idx, self.r]\n",
    "                else:\n",
    "                    if self.config.zero_reward:\n",
    "                        reward = tf.convert_to_tensor(0)\n",
    "                    else:\n",
    "                        reward = self.reward_function(tf.convert_to_tensor(e), rec_item_idx)\n",
    "                \n",
    "                ## track the episode rewards\n",
    "                rewards.append(reward.numpy())\n",
    "                \n",
    "                ## add item to history buffer if positive reviews\n",
    "                if reward > 0:\n",
    "                    history_buffer.push(tf.stop_gradient(rec_item_emb))\n",
    "                    \n",
    "                    next_state = self.state_rep_net(user_emb, tf.stack(history_buffer.to_list()), training=False)\n",
    "                else:\n",
    "                    ## keep the history buffer the same\n",
    "                    ## the next state is the current state\n",
    "                    next_state = tf.stop_gradient(state)\n",
    "                \n",
    "                ## remove new items from future recommendation\n",
    "                ignored_items.append(tf.convert_to_tensor(rec_item_idx))\n",
    "                \n",
    "                ## add the (state, action, reward, next_state)\n",
    "                ## to the experience replay\n",
    "                replay_buffer.push(state, action, next_state, reward)\n",
    "                \n",
    "                ## Inference calling stops here\n",
    "                ## Training start here\n",
    "                if(timesteps > self.config.learning_start) and (len(replay_buffer) >= self.config.batch_size) and (timesteps % self.config.learning_freq == 0):\n",
    "                    critic_loss, actor_loss, critic_params_norm = self.training_step(timesteps,\n",
    "                                                                                     replay_buffer,\n",
    "                                                                                     True\n",
    "                                                                                     )\n",
    "                    ## storing the losses along the time\n",
    "                    actor_losses.append(actor_loss)\n",
    "                    critic_losses.append(critic_loss)\n",
    "                    \n",
    "                    ## outputting the result\n",
    "                    if timesteps % self.config.log_freq == 0:\n",
    "                        if len(rewards) > 0:\n",
    "                            print(\n",
    "                                f'Timestep {timesteps - self.config.learning_start} | '\n",
    "                                f'Episode {epoch} | '\n",
    "                                f'Mean Ep R '\n",
    "                                f'{np.mean(rewards):.4f} | '\n",
    "                                f'Max R {np.max(rewards):.4f} | '\n",
    "                                f'Critic Params Norm {critic_params_norm:.4f} | '\n",
    "                                f'Actor Loss {actor_loss:.4f} | '\n",
    "                                f'Critic Loss {critic_loss:.4f} | ')\n",
    "                            sys.stdout.flush()\n",
    "            \n",
    "                ## housekeeping\n",
    "                t += 1\n",
    "                timesteps += 1\n",
    "            \n",
    "                ## end of timesteps\n",
    "            ## end of episodes\n",
    "            if timesteps - self.config.learning_start > t:\n",
    "                epoch += 1\n",
    "                e_arr.append(epoch)\n",
    "                epi_rewards.append(np.sum(rewards))\n",
    "                epi_avg_rewards.append(np.mean(rewards))\n",
    "        \n",
    "        print(\"Training Finished\")\n",
    "        return actor_losses, critic_losses, epi_avg_rewards\n",
    "    \n",
    "    def training_step(self, t, replay_buffer, training):\n",
    "        ## create batches\n",
    "        ## from utils programs\n",
    "        # Create batches by calling sample methods\n",
    "        transitions, indicies, weights = replay_buffer.sample(self.config.batch_size, beta=self.config.beta)\n",
    "        \n",
    "        weights = tf.convert_to_tensor(weights, dtype='float32')\n",
    "        ## create the tuple using Transition function     \n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        ## preparing the batch for each data\n",
    "        ## the concat function will flatten the data\n",
    "        ## the reshape will reshape the data so that it receive 64 rows\n",
    "        next_state_batch = tf.reshape(tf.concat(batch.next_state, 0), [self.config.batch_size, -1])\n",
    "        state_batch = tf.reshape(tf.concat(batch.state, 0), [self.config.batch_size, -1])\n",
    "        action_batch = tf.reshape(tf.concat(batch.action, 0), [self.config.batch_size, -1])\n",
    "        reward_batch = tf.reshape(tf.concat(batch.reward, 0), [self.config.batch_size, -1])\n",
    "        \n",
    "        ## updating the critic networks\n",
    "        with tf.GradientTape() as tape:\n",
    "            critic_loss, new_priorities = self.compute_prioritized_dqn_loss(tf.stop_gradient(state_batch),\n",
    "                                                                            action_batch,\n",
    "                                                                            reward_batch,\n",
    "                                                                            next_state_batch,\n",
    "                                                                            weights)\n",
    "        ## apply the gradient\n",
    "        grads = tape.gradient(critic_loss, self.critic_net.trainable_variables)\n",
    "        \n",
    "        replay_buffer.update_priorities(indicies, new_priorities.numpy())\n",
    "        ## critic norm clipping\n",
    "        critic_param_norm = [tf.clip_by_norm(layer.get_weights()[0] ,self.config.clip_val) for layer in self.critic_net.layers]\n",
    "        critic_param_norm = tf.norm(critic_param_norm[0])\n",
    "        ## step the optimizers\n",
    "        self.critic_optimizer.apply_gradients(zip(grads, self.critic_net.trainable_variables))\n",
    "        \n",
    "        ## updating the actor networks\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            actions_pred = self.actor_net(state_batch, training=True)\n",
    "            actor_loss = -tf.reduce_mean(self.critic_net(tf.stop_gradient(state_batch), actions_pred, training=True))\n",
    "            \n",
    "        ## compute the gradient\n",
    "        grads = tape.gradient(actor_loss, self.actor_net.trainable_variables)\n",
    "        ## apply the step to the optimizers\n",
    "        self.actor_optimizer.apply_gradients(zip(grads, self.actor_net.trainable_variables))\n",
    "        ## traceback the variables\n",
    "#         grads = tape.gradient(actor_loss, self.state_rep_net.trainable_variables)\n",
    "        ## apply the step\n",
    "#         self.state_rep_optimizer.apply_gradients(zip(grads, self.state_rep_net.trainable_variables))\n",
    "        ## minimizing the loss\n",
    "        del tape\n",
    "        ## updating the target networks\n",
    "        self.soft_update(self.critic_net, self.target_critic_net, self.config.tau)\n",
    "        self.soft_update(self.actor_net, self.target_actor_net, self.config.tau)\n",
    "        \n",
    "        return critic_loss.numpy(), actor_loss.numpy(), critic_param_norm\n",
    "         \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: model which the weights will be copied from\n",
    "            target_model: model which weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for t_layer, layer in zip(target_model.layers, local_model.layers):\n",
    "            ## initiate list\n",
    "            temp_w_arr = []\n",
    "            for t_weights, weights in zip(t_layer.get_weights(), layer.get_weights()):\n",
    "                ## fill the array list\n",
    "                temp_w_arr.append(weights * tau + (1.0 + tau) * t_weights)\n",
    "            ## copy the weights\n",
    "            t_layer.set_weights(temp_w_arr)\n",
    "    \n",
    "#     @tf.function    \n",
    "    def compute_prioritized_dqn_loss(self,\n",
    "                                     state_batch,\n",
    "                                     action_batch,\n",
    "                                     reward_batch,\n",
    "                                     next_state_batch,\n",
    "                                     weights):\n",
    "        '''\n",
    "        :param state_batch: (tensor) shape = (batch_size x state_dims),\n",
    "                The batched tensor of states collected during\n",
    "                training (i.e. s)\n",
    "        :param action_batch: (LongTensor) shape = (batch_size,)\n",
    "                The actions that you actually took at each step (i.e. a)\n",
    "        :param reward_batch: (tensor) shape = (batch_size,)\n",
    "                The rewards that you actually got at each step (i.e. r)\n",
    "        :param next_state_batch: (tensor) shape = (batch_size x state_dims),\n",
    "                The batched tensor of next states collected during\n",
    "                training (i.e. s')\n",
    "        :param weights: (tensor) shape = (batch_size,)\n",
    "                Weights for each batch item w.r.t. prioritized experience replay buffer\n",
    "        :return: loss: (torch tensor) shape = (1),\n",
    "                 new_priorities: (numpy array) shape = (batch_size,)\n",
    "        '''\n",
    "        ## create batches\n",
    "        ## forward pass through target actor network\n",
    "        next_action = self.target_actor_net(next_state_batch, training=False)\n",
    "        q_target = self.target_critic_net(next_state_batch, next_action, training=False)\n",
    "        ## y or target value that needs to be retreived\n",
    "        y = reward_batch + self.config.gamma * q_target\n",
    "        ## get q values from the current state\n",
    "        q_vals = self.critic_net(state_batch, action_batch, training=True)\n",
    "    \n",
    "        ## calculate loss\n",
    "        loss = tf.convert_to_tensor(y - q_vals)\n",
    "        ## because loss is tensor shape\n",
    "        ## we can extract the numpy value\n",
    "        loss = tf.pow(loss, 2)\n",
    "        weights_ten = tf.convert_to_tensor(weights)\n",
    "        loss = tf.reshape(loss, (self.config.batch_size,)) * weights_ten\n",
    "        ## stop the weights to be gradiented\n",
    "        weights_ten = tf.stop_gradient(weights_ten)\n",
    "        \n",
    "        ## calculate new priorities\n",
    "        new_priorities = tf.stop_gradient(loss + 1e-5)\n",
    "        loss = tf.convert_to_tensor(tf.math.reduce_mean(loss))\n",
    "        \n",
    "        return loss, new_priorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "856fd195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:(80000, 3), Test Data:(20000, 3)\n",
      "user embedding has shape (100,) and item embedding has shape (100,)\n",
      "Actor-Critic model has successfully instantiated\n",
      "DRR Instantiazed\n",
      "Start Training\n",
      "NPRM and History Buffer Initialized\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15508/3445412696.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Start Training\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15508/547705499.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;31m## setting value of negative infinite\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mranking_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrec_items\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m99999999\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;31m## get the recommended items\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "class config():\n",
    "    ## hyperparameters\n",
    "    ## setting the batch_size\n",
    "    batch_size = 64\n",
    "    gamma = 0.9\n",
    "    replay_buffer_size = 100000\n",
    "    history_buffer_size = 5\n",
    "    learning_start = 32\n",
    "    learning_freq = 1\n",
    "    ## learning rate for each model networks\n",
    "    lr_state_rep = 0.001\n",
    "    lr_actor = 0.0001\n",
    "    lr_critic = 0.001\n",
    "    \n",
    "    eps_start = 1\n",
    "    eps = 0.1\n",
    "    eps_steps = 10000\n",
    "    eps_eval = 0.1\n",
    "    episode_length = 10\n",
    "    \n",
    "    tau = 0.01 # inital 0.001\n",
    "    beta = 0.4\n",
    "    prob_alpha = 0.3\n",
    "    \n",
    "    max_timesteps_train = 260000\n",
    "    max_epochs_offline = 500\n",
    "    max_timesteps_online = 20000\n",
    "    embedding_feature_size = 100\n",
    "    \n",
    "    train_ratio = 0.8\n",
    "    weight_decay = 0.01\n",
    "    clip_val = 1.0\n",
    "    log_freq = 100\n",
    "    saving_freq = 1000\n",
    "    zero_reward = False\n",
    "    \n",
    "## First importing the data\n",
    "users = pickle.load(open('Dataset/user_id_to_num_mov.pkl', 'rb'))\n",
    "items = pickle.load(open('Dataset/movie_id_to_num_mov.pkl', 'rb'))\n",
    "data = np.load('Dataset/data.npy')\n",
    "\n",
    "## hold the length of the data\n",
    "n_users = len(users)\n",
    "n_items = len(items)\n",
    "\n",
    "## don't forget to normalize the data first\n",
    "data[:, 2] = 0.5 * (data[:, 2] - 3)\n",
    "\n",
    "## split and shuffle the data\n",
    "np.random.shuffle(data)\n",
    "## split the data\n",
    "## ratio should be 0.8\n",
    "train_data = tf.convert_to_tensor(data[:int(config.train_ratio * data.shape[0])])\n",
    "test_data = tf.convert_to_tensor(data[int(config.train_ratio * data.shape[0]):])\n",
    "print(\"Train Data:{}, Test Data:{}\".format(np.shape(train_data), np.shape(test_data)))\n",
    "\n",
    "## hold the PMF model\n",
    "## get the user and item embeddings\n",
    "reward_function = PMF(n_users, n_items, config.embedding_feature_size)\n",
    "## need to flow some data to build the model\n",
    "reward_function(1, 1)\n",
    "## loading the whole layer weights\n",
    "reward_function.load_weights('trained/adam/pmf_150_adam')\n",
    "## freeze the model, because it will be used for inference\n",
    "reward_function.trainable = False\n",
    "\n",
    "## take the embedding layers weight\n",
    "## and split the user and item weights\n",
    "user_embeddings = tf.convert_to_tensor(reward_function.user_embedding.get_weights()[0])\n",
    "item_embeddings = tf.convert_to_tensor(reward_function.item_embedding.get_weights()[0])\n",
    "## output\n",
    "print(\"user embedding has shape {} and item embedding has shape {}\"\n",
    "      .format(np.shape(user_embeddings[0]), np.shape(item_embeddings[0])))\n",
    "\n",
    "## hold the model in the variable\n",
    "## so it can be tracked\n",
    "state_rep_function = DRRAveStateRepresentation\n",
    "actor_function = Actor\n",
    "critic_function = Critic\n",
    "\n",
    "## initialize DRRTrain Class\n",
    "trainer = DRRTrainer(config,\n",
    "                     actor_function,\n",
    "                     critic_function,\n",
    "                     state_rep_function,\n",
    "                     reward_function,\n",
    "                     users,\n",
    "                     items,\n",
    "                     train_data,\n",
    "                     test_data,\n",
    "                     user_embeddings,\n",
    "                     item_embeddings)\n",
    "\n",
    "print(\"Start Training\")\n",
    "trainer.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d510268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 300), dtype=float32, numpy=\n",
       "array([[0.8003254 , 0.1580655 , 0.8195448 , ..., 0.45767805, 0.61804754,\n",
       "        0.1651512 ],\n",
       "       [0.9722305 , 0.5930467 , 0.72755915, ..., 0.04491422, 0.9201831 ,\n",
       "        0.29480112],\n",
       "       [0.52159023, 0.3600026 , 0.74447376, ..., 0.18060647, 0.76685077,\n",
       "        0.8315051 ],\n",
       "       ...,\n",
       "       [0.1523655 , 0.41763544, 0.87739044, ..., 0.11766145, 0.9641829 ,\n",
       "        0.36040053],\n",
       "       [0.00584211, 0.08027299, 0.67973286, ..., 0.87598336, 0.7782308 ,\n",
       "        0.51407045],\n",
       "       [0.97023594, 0.35064337, 0.39672542, ..., 0.47496226, 0.8215673 ,\n",
       "        0.44155088]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_batch = tf.convert_to_tensor(np.random.rand(64, 300), dtype='float32')\n",
    "state_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d05c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state_batch = tf.convert_to_tensor(np.random.rand(64, 300), dtype='float32')\n",
    "next_state_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0886329",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_batch = tf.convert_to_tensor(np.random.rand(64, 1), dtype='float32')\n",
    "reward_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ecad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_batch = tf.convert_to_tensor(np.random.rand(64, 100), dtype='float32')\n",
    "action_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e82fa892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = tf.convert_to_tensor(np.ones(64,), dtype='float32')\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1a8060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = Actor(300, 100)\n",
    "target_actor_net = Actor(300, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05ea949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_net = Critic(100, 300, 1)\n",
    "target_critic_net = Critic(100, 300, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9ae78a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = actor_net(state_batch)\n",
    "t_action = target_actor_net(state_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f5fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_net(state_batch, action_batch)\n",
    "target_critic_net(state_batch, action_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22f67fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(local_model, target_model, tau):\n",
    "    for target_weights, local_weights in zip(target_model.layers, local_model.layers):\n",
    "        temp_w = local_weights.get_weights()[0] * tau + (1.0 - tau) * target_weights.get_weights()[0]\n",
    "        target_weights.set_weights([temp_w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03f0afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_layer, layer in zip(target_model.layers, local_model.layers):\n",
    "    temp_w_arr = []\n",
    "    for t_weights, weights in zip(t_layer.get_weights(), layer.get_weights()):\n",
    "        temp_w_arr.append(weights * tau + (1.0 + tau) * t_weights)\n",
    "    t_layer.set_weights(temp_w_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783726a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_update(actor_net, target_actor_net, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddc6652",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c0fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    ## create batches\n",
    "    ## forward pass through target actor network\n",
    "    \n",
    "    ## gradient tape will watch the input\n",
    "#     tape.watch(state_batch)\n",
    "    \n",
    "    next_action = target_actor_net(next_state_batch, training=False)\n",
    "    q_target = target_critic_net(next_state_batch, next_action, training=False)\n",
    "    \n",
    "    ## y or target value that needs to be retreived\n",
    "    y = reward_batch + 0.9 * q_target\n",
    "    \n",
    "    ## get q values from the current state\n",
    "    q_vals = critic_net(state_batch, action_batch, training=True)\n",
    "    \n",
    "    ## calculate loss\n",
    "    loss = tf.convert_to_tensor(y - q_vals)\n",
    "    \n",
    "    ## because loss is tensor shape\n",
    "    ## we can extract the numpy value\n",
    "    loss = tf.pow(loss, 2)\n",
    "    weights_ten = tf.convert_to_tensor(weights)\n",
    "    loss = loss * weights_ten\n",
    "    \n",
    "    ## stop the weights to be gradiented\n",
    "    weights_ten = tf.stop_gradient(weights_ten)\n",
    "    \n",
    "    ## calculate new priorities\n",
    "    new_priorities = tf.stop_gradient(loss + 1e-5)\n",
    "    \n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "## apply the gradient\n",
    "## the gradients dloss,dx\n",
    "grads = tape.gradient(loss, critic_net.trainable_variables)\n",
    "\n",
    "## step the optimizers\n",
    "critic_optimizer.apply_gradients(zip(grads, critic_net.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "85d0294b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<model.Actor at 0x1c1f043d8b0>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e429e7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"actor_54\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_370 (Dense)           multiple                  90300     \n",
      "                                                                 \n",
      " dense_371 (Dense)           multiple                  90300     \n",
      "                                                                 \n",
      " dense_372 (Dense)           multiple                  30100     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 210,700\n",
      "Trainable params: 210,700\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor_net.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c039739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1eaddc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net.layers[0].set_weights([state_weights[0], state_weights[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "929a9229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.12920594,  0.04746147,  0.01159474, ..., -0.04332206,\n",
      "        -0.0399523 , -0.01784163],\n",
      "       [ 0.04280023,  0.07425916, -0.02163045, ..., -0.01079362,\n",
      "         0.02177624, -0.05013077],\n",
      "       [ 0.00946331, -0.0721656 ,  0.0362711 , ...,  0.0158407 ,\n",
      "        -0.103851  ,  0.02021474],\n",
      "       ...,\n",
      "       [-0.01317058,  0.07530317,  0.01302899, ...,  0.05384679,\n",
      "         0.03160053, -0.04293234],\n",
      "       [-0.10235567,  0.07339463,  0.04874316, ...,  0.09172105,\n",
      "         0.06981438,  0.01396277],\n",
      "       [ 0.00879064, -0.02434867,  0.03486311, ...,  0.03629652,\n",
      "        -0.0247658 ,  0.07643944]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[array([[ 0.02913821, -0.02950867,  0.02917098, ...,  0.01557899,\n",
      "        -0.04544451, -0.02526606],\n",
      "       [-0.01082601, -0.02602279, -0.07682656, ...,  0.01470678,\n",
      "        -0.03286941,  0.0922284 ],\n",
      "       [ 0.02381009, -0.05295213,  0.03098512, ..., -0.06068801,\n",
      "        -0.01722701, -0.05388951],\n",
      "       ...,\n",
      "       [ 0.00720855, -0.05661403, -0.00034677, ...,  0.00109477,\n",
      "        -0.03769559, -0.05506221],\n",
      "       [ 0.01632335,  0.07433993, -0.05980262, ...,  0.01812037,\n",
      "         0.0378899 , -0.06497333],\n",
      "       [ 0.0369696 , -0.01994047, -0.10535786, ...,  0.07418621,\n",
      "         0.07324029, -0.05000011]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[array([[-9.7755194e-03,  2.1755984e-02,  9.1994151e-02, ...,\n",
      "         1.2759504e-02, -3.3016137e-03, -6.9290355e-02],\n",
      "       [ 2.9328972e-02, -5.3083658e-02,  4.9076628e-02, ...,\n",
      "         4.4752944e-02, -3.0728299e-02,  5.1715635e-02],\n",
      "       [-1.8879686e-02,  6.7772858e-02, -8.5171461e-03, ...,\n",
      "        -8.4321685e-02,  1.4447316e-02, -1.7779930e-03],\n",
      "       ...,\n",
      "       [-5.4745745e-02,  1.5489702e-02,  8.9863211e-02, ...,\n",
      "         2.9688619e-02,  4.1324727e-02,  1.9214015e-02],\n",
      "       [-4.4668805e-02,  6.7384616e-03, -3.7444398e-02, ...,\n",
      "         3.1108974e-02,  3.7885818e-04,  1.7410144e-05],\n",
      "       [ 9.2347249e-02, -1.0036185e-02,  1.8601753e-03, ...,\n",
      "        -1.7488932e-02, -7.2049230e-02, -9.2773251e-02]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for layer in actor_net.layers:\n",
    "    print(layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4db05b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "29b504f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.0022795 ,  0.00252163,  0.00229887, ...,  0.00260937,\n",
       "          0.00362607,  0.00277955],\n",
       "        [ 0.00257418,  0.00293488,  0.00259659, ...,  0.00376521,\n",
       "          0.00504805,  0.00370631],\n",
       "        [ 0.00374366,  0.0041279 ,  0.00382073, ...,  0.00620195,\n",
       "          0.00979411,  0.00718564],\n",
       "        ...,\n",
       "        [ 0.0024325 ,  0.00281001,  0.00251335, ...,  0.00360964,\n",
       "          0.00461915,  0.0034384 ],\n",
       "        [ 0.00378533,  0.00419091,  0.00395471, ...,  0.00732636,\n",
       "          0.01150232,  0.00886642],\n",
       "        [ 0.00121229,  0.00153037,  0.00136205, ...,  0.00023458,\n",
       "         -0.00045106, -0.00034669]], dtype=float32),\n",
       " array([ 1.20913200e-01,  1.16120107e-01,  1.01726502e-01,  1.39423817e-01,\n",
       "         1.37664318e-01,  9.64591429e-02,  1.01730131e-01,  1.16164431e-01,\n",
       "         1.24847375e-01,  8.63403752e-02,  1.35276631e-01,  1.38140172e-01,\n",
       "         1.34562790e-01,  8.05982649e-02,  1.41219109e-01,  1.48850337e-01,\n",
       "         1.43470481e-01,  1.05250999e-01,  1.30354658e-01,  1.03301100e-01,\n",
       "        -7.39551888e-06,  9.35804695e-02,  9.30575132e-02,  1.52146846e-01,\n",
       "         1.24387294e-01,  1.27451837e-01, -1.71018095e-04,  9.49506015e-02,\n",
       "         1.25757501e-01, -1.10791341e-04,  1.48564681e-01,  1.24473892e-01,\n",
       "         1.55899405e-01,  1.64660320e-01,  1.43549621e-01,  8.91482607e-02,\n",
       "         1.39375895e-01,  1.11924149e-01,  1.02612525e-01,  8.91846269e-02,\n",
       "         1.37632936e-01,  1.41061291e-01,  1.21290796e-01,  1.32544249e-01,\n",
       "         9.74584743e-02,  1.47293478e-01,  1.10591441e-01,  1.21229738e-01,\n",
       "         1.04292981e-01,  1.47399947e-01,  1.22437358e-01,  8.41929540e-02,\n",
       "         1.20803125e-01,  1.01116031e-01,  1.33928418e-01,  1.48644909e-01,\n",
       "         9.91287455e-02,  1.09686457e-01,  1.31216809e-01,  1.36637241e-01,\n",
       "         8.93324837e-02,  1.48559496e-01,  1.18284725e-01,  1.27078459e-01,\n",
       "         8.11604932e-02,  1.28460288e-01,  1.14240564e-01,  1.32341579e-01,\n",
       "         1.34690478e-01,  1.23188160e-01,  1.09248593e-01,  1.06364027e-01,\n",
       "         1.13505863e-01,  1.13820195e-01,  1.28739208e-01,  9.60279554e-02,\n",
       "         1.27829835e-01,  7.72511363e-02,  1.14223622e-01,  1.13842331e-01,\n",
       "         1.04524672e-01,  1.14913590e-01,  1.40775070e-01,  8.00227821e-02,\n",
       "         1.12629302e-01,  1.29536539e-01,  1.27606615e-01,  1.39444262e-01,\n",
       "         1.09289646e-01,  8.80848765e-02,  9.75922272e-02,  1.16385885e-01,\n",
       "         1.16501115e-01,  8.57257545e-02,  1.34852856e-01,  1.26341522e-01,\n",
       "         1.39421761e-01,  1.49068132e-01, -2.55268009e-04,  1.32955879e-01,\n",
       "         1.30767241e-01,  1.53033704e-01,  1.38052598e-01,  8.51272643e-02,\n",
       "         8.64468887e-02,  1.22984573e-01,  1.44195125e-01,  1.08929843e-01,\n",
       "         1.10047899e-01,  1.23531684e-01,  8.91832560e-02,  1.11851618e-01,\n",
       "         1.01069510e-01,  1.25404805e-01,  1.25877276e-01,  1.30662546e-01,\n",
       "         1.07661352e-01,  9.89091173e-02,  1.29728466e-01,  1.50393650e-01,\n",
       "         9.60762799e-02,  9.69406813e-02,  1.04130052e-01,  1.47309363e-01,\n",
       "         1.05380155e-01,  1.17754199e-01,  1.33037314e-01,  1.42274022e-01,\n",
       "         9.35841352e-02,  1.28449127e-01,  9.35780331e-02,  9.42427516e-02,\n",
       "         1.37122899e-01,  1.20725311e-01,  1.25858262e-01,  1.12628229e-01,\n",
       "         1.38062745e-01,  1.17396921e-01,  1.46978110e-01,  1.20778568e-01,\n",
       "         1.44427806e-01,  9.85948443e-02,  9.79648829e-02,  1.32240415e-01,\n",
       "         1.12848505e-01,  1.37177095e-01,  1.27929077e-01,  1.44719556e-01,\n",
       "         1.09184317e-01,  9.57164764e-02,  1.39040083e-01,  1.38231680e-01,\n",
       "         1.22429110e-01,  1.23501949e-01,  1.36017680e-01,  1.13740884e-01,\n",
       "         1.21917106e-01,  1.15012296e-01,  1.22179441e-01,  1.42904401e-01,\n",
       "         1.15419261e-01,  1.37045190e-01,  1.05041079e-01,  1.44437745e-01,\n",
       "         1.11108892e-01,  1.34809822e-01,  1.14389606e-01,  1.09104551e-01,\n",
       "         1.49320409e-01,  1.48294583e-01,  1.47057787e-01,  1.19650766e-01,\n",
       "         1.10284969e-01,  9.88769904e-02,  1.30141601e-01,  1.48591086e-01,\n",
       "         1.42361566e-01,  1.21632166e-01,  1.34866491e-01,  1.24605983e-01,\n",
       "         1.33607760e-01,  1.33534908e-01,  1.23485528e-01,  1.34574041e-01,\n",
       "         1.22416295e-01,  1.10175826e-01,  1.07264444e-01,  1.15167677e-01,\n",
       "         1.35753557e-01,  9.68686864e-02, -3.23043023e-05,  1.08083665e-01,\n",
       "         1.22044340e-01,  1.17047884e-01,  1.45883679e-01,  1.03070326e-01,\n",
       "         1.09731853e-01,  9.76786911e-02,  1.53808430e-01,  1.20685875e-01,\n",
       "         1.38446644e-01, -9.32205730e-05,  1.35759324e-01,  1.21415481e-01,\n",
       "         1.38633892e-01,  8.45540389e-02,  1.15951307e-01,  9.38214958e-02,\n",
       "         1.37271121e-01,  1.52867094e-01,  1.34684145e-01,  1.27990678e-01,\n",
       "        -1.00156387e-06,  1.16814800e-01,  1.28962129e-01,  1.02605075e-01,\n",
       "         1.20482214e-01,  1.13560565e-01,  1.00084595e-01,  1.47288188e-01,\n",
       "         1.20705210e-01,  1.24977008e-01,  1.02946803e-01,  1.17321536e-01,\n",
       "         9.88623798e-02,  8.28469247e-02, -1.37398747e-05,  1.35203660e-01,\n",
       "         8.89704749e-02,  9.99623984e-02,  1.40248194e-01,  1.11462504e-01,\n",
       "         8.46205428e-02,  1.32906124e-01,  1.32095560e-01,  1.22444235e-01,\n",
       "         1.14492878e-01,  1.27613038e-01,  1.08278818e-01,  1.10465594e-01,\n",
       "         8.43377262e-02,  1.31462842e-01,  1.22237943e-01,  1.46654591e-01,\n",
       "         1.28467783e-01,  9.75520536e-02,  1.17553562e-01,  9.52511951e-02,\n",
       "         1.07713260e-01,  1.15695357e-01,  1.07966170e-01,  1.23516887e-01,\n",
       "         1.07555903e-01,  1.03948399e-01,  1.39779225e-01,  1.26255766e-01,\n",
       "         1.34149343e-01,  1.11674406e-01,  8.96748304e-02,  1.19043365e-01,\n",
       "         1.37615755e-01,  1.24697991e-01,  1.07065275e-01,  1.29405901e-01,\n",
       "         1.43800303e-01,  8.34162906e-02,  1.26768783e-01,  1.31915405e-01,\n",
       "         1.33073047e-01,  9.26856101e-02,  9.69970152e-02,  1.28930569e-01,\n",
       "        -7.54447537e-05,  1.08793043e-01,  1.26901507e-01,  1.18704788e-01,\n",
       "         1.36195049e-01,  1.18547872e-01,  1.24739237e-01,  1.01232097e-01,\n",
       "         1.33783251e-01,  9.60614160e-02,  1.16262585e-01,  1.34640694e-01,\n",
       "         1.30064785e-01,  1.19684584e-01,  9.58834887e-02,  7.36986995e-02,\n",
       "         1.05311781e-01,  1.01259470e-01,  9.66226235e-02,  1.15851611e-01,\n",
       "         1.35810390e-01,  1.16721526e-01,  1.38052881e-01,  7.79757574e-02,\n",
       "         1.34638712e-01,  1.15795664e-01,  7.79021978e-02,  1.34864137e-01],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_net.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "af86d4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0022795 ,  0.00252163,  0.00229887, ...,  0.00260937,\n",
       "         0.00362607,  0.00277955],\n",
       "       [ 0.00257418,  0.00293488,  0.00259659, ...,  0.00376521,\n",
       "         0.00504805,  0.00370631],\n",
       "       [ 0.00374366,  0.0041279 ,  0.00382073, ...,  0.00620195,\n",
       "         0.00979411,  0.00718564],\n",
       "       ...,\n",
       "       [ 0.0024325 ,  0.00281001,  0.00251335, ...,  0.00360964,\n",
       "         0.00461915,  0.0034384 ],\n",
       "       [ 0.00378533,  0.00419091,  0.00395471, ...,  0.00732636,\n",
       "         0.01150232,  0.00886642],\n",
       "       [ 0.00121229,  0.00153037,  0.00136205, ...,  0.00023458,\n",
       "        -0.00045106, -0.00034669]], dtype=float32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_weights = np.load('trained/actor_net_weight.npy', allow_pickle=True)\n",
    "# state_weights = np.reshape(state_weights, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "685a8e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0022795 ,  0.00252163,  0.00229887, ...,  0.00260937,\n",
       "         0.00362607,  0.00277955],\n",
       "       [ 0.00257418,  0.00293488,  0.00259659, ...,  0.00376521,\n",
       "         0.00504805,  0.00370631],\n",
       "       [ 0.00374366,  0.0041279 ,  0.00382073, ...,  0.00620195,\n",
       "         0.00979411,  0.00718564],\n",
       "       ...,\n",
       "       [ 0.0024325 ,  0.00281001,  0.00251335, ...,  0.00360964,\n",
       "         0.00461915,  0.0034384 ],\n",
       "       [ 0.00378533,  0.00419091,  0.00395471, ...,  0.00732636,\n",
       "         0.01150232,  0.00886642],\n",
       "       [ 0.00121229,  0.00153037,  0.00136205, ...,  0.00023458,\n",
       "        -0.00045106, -0.00034669]], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "576c9536",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_rep_net = DRRAveStateRepresentation(100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b9b27afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_rep_net.set_weights([state_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d1138b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_rep_net.save_weights('state_rep_weights')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
