{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "774487c3",
   "metadata": {},
   "source": [
    "### Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da2f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from model import PMF, DRRAveStateRepresentation, Actor, Critic\n",
    "\n",
    "from utils.prioritized_replay_buffer import NaivePrioritizedReplayMemory, Transition\n",
    "from utils.history_buffer import HistoryBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bc27621",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    ## hyperparameters\n",
    "    ## setting the batch_size\n",
    "    batch_size = 64\n",
    "    gamma = 0.9\n",
    "    replay_buffer_size = 100000\n",
    "    history_buffer_size = 5\n",
    "    learning_start = 250\n",
    "    learning_freq = 1\n",
    "    ## learning rate for each model networks\n",
    "    lr_state_rep = 0.001\n",
    "    lr_actor = 0.0001\n",
    "    lr_critic = 0.001\n",
    "    \n",
    "    eps_start = 1\n",
    "    eps = 0.1\n",
    "    eps_steps = 10000\n",
    "    eps_eval = 0.1\n",
    "    episode_length = 10\n",
    "    \n",
    "    tau = 0.01 # inital 0.001\n",
    "    beta = 0.4\n",
    "    prob_alpha = 0.3\n",
    "    \n",
    "    max_timesteps_train = 260000\n",
    "    max_epochs_offline = 500\n",
    "    max_timesteps_online = 20000\n",
    "    embedding_feature_size = 100\n",
    "    \n",
    "    train_ratio = 0.8\n",
    "    weight_decay = 0.01\n",
    "    clip_val = 1.0\n",
    "    log_freq = 100\n",
    "    saving_freq = 1000\n",
    "    zero_reward = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71f61d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inisialisasi variable\n",
    "users = pickle.load(open('Dataset/user_id_to_num_mov.pkl', 'rb'))\n",
    "items = pickle.load(open('Dataset/movie_id_to_num_mov.pkl', 'rb'))\n",
    "data = np.load('Dataset/data.npy')\n",
    "\n",
    "data[:, 2] = 0.5 * (data[:, 2] - 3)\n",
    "\n",
    "train_data = tf.convert_to_tensor(data[:int(config.train_ratio * data.shape[0])], dtype='float32')\n",
    "\n",
    "n_items = len(items)\n",
    "n_users = len(users)\n",
    "\n",
    "reward_function = PMF(n_users, n_items, config.embedding_feature_size)\n",
    "reward_function(1, 1)\n",
    "reward_function.load_weights('trained/adam/pmf_150_adam')\n",
    "\n",
    "user_embeddings = tf.convert_to_tensor(reward_function.user_embedding.get_weights()[0])\n",
    "item_embeddings = tf.convert_to_tensor(reward_function.item_embedding.get_weights()[0])\n",
    "\n",
    "item_features = item_embeddings.shape[1] ## 100\n",
    "user_features = user_embeddings.shape[1] ## 100\n",
    "state_shape = 3 * item_features  # dimensionality 300\n",
    "action_shape = item_features #100\n",
    "critic_output_shape = 1 #1\n",
    "\n",
    "u_id = 0\n",
    "r_id = 2\n",
    "i_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c3408f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(80000,), dtype=float32, numpy=array([ 0. ,  0. , -1. , ..., -0.5, -1. ,  0. ], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3465c517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor-Critic model has successfully instantiated\n"
     ]
    }
   ],
   "source": [
    "## instantiate a drravestaterepresentation\n",
    "reward_function = PMF(n_users, n_items, config.embedding_feature_size)\n",
    "reward_function(1, 1)\n",
    "reward_function.load_weights('trained/adam/pmf_150_adam')\n",
    "\n",
    "user_embeddings = tf.convert_to_tensor(reward_function.user_embedding.get_weights()[0])\n",
    "item_embeddings = tf.convert_to_tensor(reward_function.item_embedding.get_weights()[0])\n",
    "\n",
    "state_rep_net = DRRAveStateRepresentation(config.history_buffer_size, item_features, user_features)\n",
    "\n",
    "## instantiate actor and target actor networks\n",
    "actor_net = Actor(state_shape, action_shape)                                \n",
    "target_actor_net = Actor(state_shape, action_shape)\n",
    "\n",
    "## instantiate critic and target critics networks\n",
    "critic_net = Critic(action_shape, state_shape, critic_output_shape)\n",
    "target_critic_net = Critic(action_shape, state_shape, critic_output_shape)\n",
    "\n",
    "## data flow for building the model\n",
    "flow_item = tf.convert_to_tensor(np.random.rand(5, 100), dtype='float32')\n",
    "flow_state = tf.convert_to_tensor(np.random.rand(1, 300), dtype='float32')\n",
    "flow_action = tf.convert_to_tensor(np.random.rand(1, 100), dtype='float32')\n",
    "\n",
    "## flowing the data into the model to build the model\n",
    "state_rep_net(user_embeddings[0], flow_item)\n",
    "actor_net(flow_state)\n",
    "target_actor_net(flow_state)\n",
    "critic_net(flow_state, flow_action)\n",
    "target_critic_net(flow_state, flow_action)\n",
    "print(\"Actor-Critic model has successfully instantiated\")\n",
    "\n",
    "state_rep_optimizer = tf.keras.optimizers.Adam(learning_rate=config.lr_state_rep)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=config.lr_actor)\n",
    "critic_optimizer = tf.keras.optimizers.Adam(learning_rate=config.lr_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf7cfe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = NaivePrioritizedReplayMemory(config.replay_buffer_size, prob_alpha=config.prob_alpha)\n",
    "history_buffer = HistoryBuffer(config.history_buffer_size)\n",
    "\n",
    "timesteps = 0\n",
    "epoch = 0\n",
    "\n",
    "## this variable is for episode\n",
    "eps_slope = abs(config.eps_start - config.eps)/config.eps_steps\n",
    "eps = config.eps_start\n",
    "\n",
    "actor_losses = []\n",
    "critic_losses = []\n",
    "\n",
    "## this variable is to hold the episodic rewards\n",
    "epi_rewards = []\n",
    "epi_avg_rewards = []\n",
    "e_arr = []\n",
    "\n",
    "## this variable holds the user index\n",
    "## got from the dictionary\n",
    "user_idxs = np.array(list(users.values()))\n",
    "np.random.shuffle(user_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c9ee3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 0\n",
    "\n",
    "user_reviews = train_data[train_data[:, u_id] == e] # 53\n",
    "pos_user_reviews = user_reviews[user_reviews[:, r_id] > 0] #25\n",
    "\n",
    "## ngambil embedding items dan user\n",
    "candidate_items = tf.identity(tf.stop_gradient(item_embeddings))\n",
    "user_emb = user_embeddings[e]\n",
    "\n",
    "## masih gatau kenapa ada variable ini\n",
    "ignored_items = []\n",
    "\n",
    "## 5\n",
    "for i in range(config.history_buffer_size):\n",
    "    emb = candidate_items[int(pos_user_reviews[i, i_id].numpy())]\n",
    "    history_buffer.push(tf.identity(tf.stop_gradient(emb)))\n",
    "\n",
    "## initialize rewards list\n",
    "rewards = []\n",
    "\n",
    "## starting item index\n",
    "t = 0\n",
    "\n",
    "## declaring the needed variable\n",
    "state = None\n",
    "action = None\n",
    "reward = None\n",
    "next_state = None\n",
    "\n",
    "## while di sini\n",
    "    \n",
    "\n",
    "if eps > config.eps:\n",
    "    eps -= eps_slope\n",
    "else:\n",
    "    eps = config.eps\n",
    "\n",
    "state = state_rep_net(user_emb, tf.stack(history_buffer.to_list()))\n",
    "state = tf.reshape(tf.stop_gradient(state), [1, state.shape[0]])\n",
    "action = actor_net(tf.stop_gradient(state), training=False)\n",
    "\n",
    "## calculate candidate items\n",
    "ranking_scores = candidate_items @ tf.transpose(action)\n",
    "\n",
    "## flatten the result\n",
    "ranking_scores = tf.reshape(ranking_scores, (ranking_scores.shape[0],)).numpy()\n",
    "\n",
    "if len(ignored_items) > 0:\n",
    "    rec_items = tf.stack(ignored_items)\n",
    "else:\n",
    "    rec_items = []\n",
    "\n",
    "ranking_scores[rec_items] = -float(\"inf\")\n",
    "\n",
    "rec_item_idx = tf.math.argmax(ranking_scores).numpy()\n",
    "rec_item_emb = candidate_items[rec_item_idx]\n",
    "\n",
    "## intinya ngambil reward, kalau gaada dibikin dari PMF\n",
    "if rec_item_idx in user_reviews[:, i_id]:\n",
    "    user_rec_item_idx = np.where(user_reviews[:, i_id] == float(rec_item_idx))[0][0]\n",
    "    reward = user_reviews[user_rec_item_idx, r_id]\n",
    "else:\n",
    "    if config.zero_reward:\n",
    "        reward = tf.convert_to_tensor(0)\n",
    "    else:\n",
    "        reward = reward_function(float(e), float(rec_item_idx))\n",
    "\n",
    "rewards.append(reward.numpy())\n",
    "\n",
    "if reward > 0:\n",
    "    history_buffer.push(tf.identity(tf.stop_gradient(rec_item_emb)))\n",
    "    next_state = state_rep_net(user_emb, tf.stack(history_buffer.to_list()), training=False)\n",
    "else:\n",
    "    next_state = tf.stop_gradient(state)\n",
    "\n",
    "ignored_items.append(tf.convert_to_tensor(rec_item_idx))\n",
    "replay_buffer.push(state, action, next_state, reward)\n",
    "\n",
    "if (timesteps > config.learning_start) and (len(replay_buffer) >= config.batch_size) and (timesteps % config.learning_freq == 0):\n",
    "    print(\"OK\")\n",
    "\n",
    "t += 1\n",
    "timesteps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "598351cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.5>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_reviews[3, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d3ffb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_rec_item_idx = np.where(user_reviews[:, i_id] == 159)[0][0]\n",
    "user_rec_item_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff2d058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_emb = user_embeddings[0]\n",
    "\n",
    "state = state_rep_net(user_emb, tf.stack(history_buffer.to_list()))\n",
    "state = tf.reshape(tf.stop_gradient(state), [1, state.shape[0]])\n",
    "action = actor_net(tf.stop_gradient(state), training=False)\n",
    "\n",
    "## calculate candidate items\n",
    "ranking_scores = candidate_items @ tf.transpose(action)\n",
    "\n",
    "## flatten the result\n",
    "ranking_scores = tf.reshape(ranking_scores, (ranking_scores.shape[0],)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ec3a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_items = [1, 2, 3, 4]\n",
    "rec_items = tf.stack(ignored_items).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22b4e10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_scores[rec_items] = -float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a37e175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0846074e-03,          -inf,          -inf, ..., 2.7740351e-03,\n",
       "       3.3080969e-03, 5.8118429e-05], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e6d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loop all the users based on indexes\n",
    "## enumerates start with zero\n",
    "for idx, e in enumerate(user_idxs):\n",
    "    ## starting the episodes\n",
    "\n",
    "    ## learning_start = 250\n",
    "    if timesteps - config.learning_start > config.max_timesteps_train:\n",
    "        break\n",
    "\n",
    "    ## extracting positive user reviews\n",
    "    ## e variable is an element right now\n",
    "    user_reviews = train_data[train_data[:, u] == e]\n",
    "    pos_user_reviews = user_reviews[user_reviews[:, r] > 0]\n",
    "\n",
    "    ## check if the user ratings doesn't have enough positive review\n",
    "    ## in this case history_buffer_size is 4\n",
    "    ## get the shape object and 0 denote the row index\n",
    "    if pos_user_reviews.shape[0] < config.history_buffer_size:\n",
    "        continue\n",
    "\n",
    "    candidate_items = tf.identity(tf.stop_gradient(item_embeddings))\n",
    "\n",
    "    ## extracting user embedding tensors\n",
    "    user_emb = user_embeddings[e]\n",
    "\n",
    "    ## fill history buffer with positive item embeddings\n",
    "    ## and remove item embeddings from candidate item sets\n",
    "    ignored_items = []\n",
    "\n",
    "    ## history_buffer_size has size of n items\n",
    "    ## in this case 5\n",
    "    for i in range(config.history_buffer_size):\n",
    "        emb = candidate_items[tf.cast(pos_user_reviews[i, i], dtype='int32')]\n",
    "        history_buffer.push(tf.identity(tf.stop_gradient(emb)))\n",
    "\n",
    "    ## initialize rewards list\n",
    "    rewards = []\n",
    "\n",
    "    ## starting item index\n",
    "    t = 0\n",
    "\n",
    "    ## declaring the needed variable\n",
    "    state = None\n",
    "    action = None\n",
    "    reward = None\n",
    "    next_state = None\n",
    "\n",
    "    while t < config.episode_length:\n",
    "        ## observing the current state\n",
    "        ## choose action according to actor network explorations\n",
    "\n",
    "        ## inference calls start here\n",
    "\n",
    "        if eps > config.eps:\n",
    "            eps -= eps_slope\n",
    "        else:\n",
    "            eps = config.eps\n",
    "\n",
    "        ## state is the result of DRRAve model inference\n",
    "        ## history_buffer.to_list get the list of previous items\n",
    "        ## state representaton has the size (300, )\n",
    "        state = state_rep_net(user_emb, tf.stack(history_buffer.to_list()))\n",
    "        if np.random.uniform(0, 1) < eps:\n",
    "            action = tf.convert_to_tensor(np.random.rand(1, action_shape), dtype='float32') \n",
    "        else:\n",
    "            action = actor_net(tf.reshape(tf.stop_gradient(state), [1, state.shape[0]]), training=False)\n",
    "\n",
    "        ranking_scores = candidate_items @ tf.reshape(action, (action.shape[1], 1))\n",
    "        ## calculating ranking scores accross items, discard ignored items\n",
    "\n",
    "        if len(ignored_items) > 0:\n",
    "            rec_items = tf.stack(ignored_items)\n",
    "        else:\n",
    "            rec_items = []\n",
    "\n",
    "        ## setting value of negative infinite\n",
    "\n",
    "#       ranking_scores[rec_items] = -99999999\n",
    "\n",
    "        ## get the recommended items\n",
    "        ## first get the maximum value index\n",
    "        ## then get the items by index from candidate items\n",
    "        ranking_scores = tf.reshape(ranking_scores, (ranking_scores.shape[0],))\n",
    "        rec_item_idx = tf.math.argmax(ranking_scores)\n",
    "        rec_item_emb = candidate_items[rec_item_idx]\n",
    "\n",
    "        ## get the item reward\n",
    "        if tf.cast(rec_item_idx, 'float64') in user_reviews[:, i]:\n",
    "            ## get the reward from rating in the dataset\n",
    "            ## if the user is rating the item\n",
    "            user_rec_item_idx = np.where(user_reviews[:, i] == float(rec_item_idx))[0][0]\n",
    "            reward = user_reviews[user_rec_item_idx, r]\n",
    "        else:\n",
    "            if config.zero_reward:\n",
    "                reward = tf.convert_to_tensor(0)\n",
    "            else:\n",
    "                reward = reward_function(tf.convert_to_tensor(e), rec_item_idx)\n",
    "\n",
    "        ## track the episode rewards\n",
    "        rewards.append(reward.numpy())\n",
    "\n",
    "        ## add item to history buffer if positive reviews\n",
    "        if reward > 0:\n",
    "            history_buffer.push(tf.stop_gradient(rec_item_emb))\n",
    "\n",
    "            next_state = state_rep_net(user_emb, tf.stack(history_buffer.to_list()), training=False)\n",
    "        else:\n",
    "            ## keep the history buffer the same\n",
    "            ## the next state is the current state\n",
    "            next_state = tf.stop_gradient(state)\n",
    "\n",
    "        ## remove new items from future recommendation\n",
    "        ignored_items.append(tf.convert_to_tensor(rec_item_idx))\n",
    "\n",
    "        ## add the (state, action, reward, next_state)\n",
    "        ## to the experience replay\n",
    "        replay_buffer.push(state, action, next_state, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c2f11",
   "metadata": {},
   "source": [
    "### CHECK CRITIC AMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7240548",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_shape = 3\n",
    "in_features = 9\n",
    "out_features = 1\n",
    "combo_features = in_features + action_shape\n",
    "\n",
    "linear_fn_1 = tf.keras.layers.Dense(in_features, activation='relu')\n",
    "linear_fn_2 = tf.keras.layers.Dense(combo_features, activation='relu')\n",
    "linear_fn_3 = tf.keras.layers.Dense(combo_features, activation='relu')\n",
    "linear_fn_4 = tf.keras.layers.Dense(out_features, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830b3ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = tf.convert_to_tensor(np.array([[0.9004, 0.8004, 0.7004], [0.9004, 0.6004, 0.7004]]), dtype='float32')\n",
    "input_st = tf.convert_to_tensor(np.array([[1.0000, 3.0000, 5.0000, 0.5000, 2.1000, 4.5000, 0.5000, 0.7000, 0.9000],\n",
    "                                          [1.0000, 3.0000, 5.0000, 0.5000, 2.1000, 4.5000, 0.5000, 0.7000, 0.9000]]), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_st.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d267e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combbo = tf.concat([action, input_st], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build the model\n",
    "linear_fn_1(input_st)\n",
    "linear_fn_2(combbo)\n",
    "linear_fn_3(combbo)\n",
    "linear_fn_4(combbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c14032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1 = 0.1 * np.ones((9,9))\n",
    "weight2 = 0.1 * np.ones((12,12))\n",
    "weight3 = 0.1 * np.ones((12,12))\n",
    "weight4 = 0.1 * np.ones((12,1))\n",
    "bias1 = np.zeros((9,))\n",
    "bias2 = np.zeros((12,))\n",
    "bias3 = np.zeros((12,))\n",
    "bias4 = np.zeros((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a3fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_fn_1.set_weights([weight1, bias1])\n",
    "linear_fn_2.set_weights([weight2, bias2])\n",
    "linear_fn_3.set_weights([weight3, bias3])\n",
    "linear_fn_4.set_weights([weight4, bias4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e3d068",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = linear_fn_1(input_st)\n",
    "output = tf.concat([action, output], 1)\n",
    "output = linear_fn_2(output)\n",
    "output = linear_fn_3(output)\n",
    "output = linear_fn_4(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ecc5a1",
   "metadata": {},
   "source": [
    "### CHECK ACTOR AMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e039e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = 9\n",
    "out_features = 3\n",
    "\n",
    "linear_fn_1 = tf.keras.layers.Dense(in_features, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.1))\n",
    "linear_fn_2 = tf.keras.layers.Dense(in_features, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.1))\n",
    "linear_fn_3 = tf.keras.layers.Dense(out_features, activation='tanh', kernel_regularizer=tf.keras.regularizers.L2(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b52ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = 0.1 * np.ones((9, 9))\n",
    "bias = np.zeros(9,)\n",
    "weights_3 = 0.1 * np.ones((9, 3))\n",
    "bias_3 = np.zeros(3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1161d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_st = tf.convert_to_tensor(np.array([[1.0000, 3.0000, 5.0000, 0.5000, 2.1000, 4.5000, 0.5000, 0.7000, 0.9000]]), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e6382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_st.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build the model\n",
    "linear_fn_1(input_st)\n",
    "linear_fn_2(input_st)\n",
    "linear_fn_3(input_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199bd412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_fn(input_st)\n",
    "linear_fn_1.set_weights([weights, bias])\n",
    "linear_fn_2.set_weights([weights, bias])\n",
    "linear_fn_3.set_weights([weights_3, bias_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22afefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = linear_fn_1(input_st)\n",
    "output = linear_fn_2(output)\n",
    "output = linear_fn_3(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d01a44",
   "metadata": {},
   "source": [
    "### DRRAVE AMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518e526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = tf.convert_to_tensor(np.array([1, 3, 5]), dtype='float32')\n",
    "items = tf.convert_to_tensor(np.array([[1, 2, 3], [4, 5, 6]]), dtype='float32')\n",
    "attention_weights = tf.Variable(tf.convert_to_tensor(np.array([[0.1], [0.1]]), dtype='float32'))\n",
    "# attention_weights = tf.random.uniform((3, 1), minval=0., maxval=1.)\n",
    "# attention_weights\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f60a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "right = tf.transpose(items) @ attention_weights\n",
    "right = tf.reshape(right, (right.shape[0],))\n",
    "middle = users * right\n",
    "output = tf.concat([users, middle, right], 0)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bdbb03",
   "metadata": {},
   "source": [
    "### DQN LOSS AMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9795e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.convert_to_tensor(np.array([1, 2, 3, 4]), dtype='float32')\n",
    "q_vals = tf.convert_to_tensor(np.array([2, 2, 2, 2]), dtype='float32')\n",
    "weights = [1., 1., 1., 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9bf080",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate loss\n",
    "loss = tf.convert_to_tensor(y - q_vals)\n",
    "## because loss is tensor shape\n",
    "## we can extract the numpy value\n",
    "loss = tf.pow(loss, 2)\n",
    "weights_ten = tf.stop_gradient(weights)\n",
    "loss = tf.reshape(loss, (4,)) * weights_ten\n",
    "## calculate new priorities\n",
    "new_priorities = tf.stop_gradient(loss).numpy() + 1e-5\n",
    "loss = tf.convert_to_tensor(tf.math.reduce_mean(loss))\n",
    "print(new_priorities)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff46b2b",
   "metadata": {},
   "source": [
    "### Cek Soft Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4016ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "    Params\n",
    "    ======\n",
    "        local_model: model which the weights will be copied from\n",
    "        target_model: model which weights will be copied to\n",
    "        tau (float): interpolation parameter\n",
    "    \"\"\"\n",
    "    for t_layer, layer in zip(target_model.layers, local_model.layers):\n",
    "        ## initiate list\n",
    "        temp_w_arr = []\n",
    "        for t_weights, weights in zip(t_layer.get_weights(), layer.get_weights()):\n",
    "            ## fill the array list\n",
    "            temp_w_arr.append(weights * tau + (1.0 - tau)*t_weights)\n",
    "        ## copy the weights\n",
    "        t_layer.set_weights(temp_w_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
