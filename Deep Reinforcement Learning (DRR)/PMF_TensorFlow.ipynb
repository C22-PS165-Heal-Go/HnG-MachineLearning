{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a928ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "from model import PMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41ed32de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== TRAINING PMF =====\n",
      "===== Dataset has been loaded =====\n",
      "===== Preprocess the data has been finished =====\n",
      "===== Model Instantiated =====\n",
      "Model: \"pmf_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     multiple                  82200     \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     multiple                  14200     \n",
      "                                                                 \n",
      " embedding_6 (Embedding)     multiple                  822       \n",
      "                                                                 \n",
      " embedding_7 (Embedding)     multiple                  142       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 97,364\n",
      "Trainable params: 97,364\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "===== Training Model =====\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 1.0567\n",
      "Training loss (for one batch) at step 200: 1.4944\n",
      "Training loss (for one batch) at step 400: 0.7501\n",
      "Training loss (for one batch) at step 600: 0.9474\n",
      "Training epoch: 1, training rmse: 1.082397, vali rmse:1.109077\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.9549\n",
      "Training loss (for one batch) at step 200: 1.3564\n",
      "Training loss (for one batch) at step 400: 0.6800\n",
      "Training loss (for one batch) at step 600: 0.8654\n",
      "Training epoch: 2, training rmse: 1.029967, vali rmse:1.055068\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 0.8671\n",
      "Training loss (for one batch) at step 200: 1.2402\n",
      "Training loss (for one batch) at step 400: 0.6250\n",
      "Training loss (for one batch) at step 600: 0.7991\n",
      "Training epoch: 3, training rmse: 0.984206, vali rmse:1.008024\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 0.7957\n",
      "Training loss (for one batch) at step 200: 1.1428\n",
      "Training loss (for one batch) at step 400: 0.5819\n",
      "Training loss (for one batch) at step 600: 0.7453\n",
      "Training epoch: 4, training rmse: 0.944217, vali rmse:0.967028\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 0.7373\n",
      "Training loss (for one batch) at step 200: 1.0606\n",
      "Training loss (for one batch) at step 400: 0.5482\n",
      "Training loss (for one batch) at step 600: 0.7014\n",
      "Training epoch: 5, training rmse: 0.909192, vali rmse:0.931259\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 0.6895\n",
      "Training loss (for one batch) at step 200: 0.9909\n",
      "Training loss (for one batch) at step 400: 0.5218\n",
      "Training loss (for one batch) at step 600: 0.6653\n",
      "Training epoch: 6, training rmse: 0.878428, vali rmse:0.899996\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 0.6501\n",
      "Training loss (for one batch) at step 200: 0.9314\n",
      "Training loss (for one batch) at step 400: 0.5012\n",
      "Training loss (for one batch) at step 600: 0.6356\n",
      "Training epoch: 7, training rmse: 0.851313, vali rmse:0.872606\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 0.6174\n",
      "Training loss (for one batch) at step 200: 0.8803\n",
      "Training loss (for one batch) at step 400: 0.4850\n",
      "Training loss (for one batch) at step 600: 0.6107\n",
      "Training epoch: 8, training rmse: 0.827318, vali rmse:0.848543\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 0.5902\n",
      "Training loss (for one batch) at step 200: 0.8362\n",
      "Training loss (for one batch) at step 400: 0.4723\n",
      "Training loss (for one batch) at step 600: 0.5898\n",
      "Training epoch: 9, training rmse: 0.805991, vali rmse:0.827333\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 0.5674\n",
      "Training loss (for one batch) at step 200: 0.7979\n",
      "Training loss (for one batch) at step 400: 0.4624\n",
      "Training loss (for one batch) at step 600: 0.5721\n",
      "Training epoch: 10, training rmse: 0.786942, vali rmse:0.808567\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 0.5480\n",
      "Training loss (for one batch) at step 200: 0.7644\n",
      "Training loss (for one batch) at step 400: 0.4544\n",
      "Training loss (for one batch) at step 600: 0.5569\n",
      "Training epoch: 11, training rmse: 0.769841, vali rmse:0.791897\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 0.5315\n",
      "Training loss (for one batch) at step 200: 0.7349\n",
      "Training loss (for one batch) at step 400: 0.4481\n",
      "Training loss (for one batch) at step 600: 0.5437\n",
      "Training epoch: 12, training rmse: 0.754407, vali rmse:0.777021\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 0.5173\n",
      "Training loss (for one batch) at step 200: 0.7088\n",
      "Training loss (for one batch) at step 400: 0.4430\n",
      "Training loss (for one batch) at step 600: 0.5321\n",
      "Training epoch: 13, training rmse: 0.740403, vali rmse:0.763687\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 0.5050\n",
      "Training loss (for one batch) at step 200: 0.6855\n",
      "Training loss (for one batch) at step 400: 0.4388\n",
      "Training loss (for one batch) at step 600: 0.5218\n",
      "Training epoch: 14, training rmse: 0.727628, vali rmse:0.751676\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 0.4942\n",
      "Training loss (for one batch) at step 200: 0.6647\n",
      "Training loss (for one batch) at step 400: 0.4353\n",
      "Training loss (for one batch) at step 600: 0.5126\n",
      "Training epoch: 15, training rmse: 0.715911, vali rmse:0.740806\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 0.4846\n",
      "Training loss (for one batch) at step 200: 0.6459\n",
      "Training loss (for one batch) at step 400: 0.4323\n",
      "Training loss (for one batch) at step 600: 0.5042\n",
      "Training epoch: 16, training rmse: 0.705111, vali rmse:0.730919\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 0.4760\n",
      "Training loss (for one batch) at step 200: 0.6289\n",
      "Training loss (for one batch) at step 400: 0.4296\n",
      "Training loss (for one batch) at step 600: 0.4965\n",
      "Training epoch: 17, training rmse: 0.695107, vali rmse:0.721884\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 0.4682\n",
      "Training loss (for one batch) at step 200: 0.6134\n",
      "Training loss (for one batch) at step 400: 0.4273\n",
      "Training loss (for one batch) at step 600: 0.4893\n",
      "Training epoch: 18, training rmse: 0.685796, vali rmse:0.713587\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 0.4612\n",
      "Training loss (for one batch) at step 200: 0.5992\n",
      "Training loss (for one batch) at step 400: 0.4251\n",
      "Training loss (for one batch) at step 600: 0.4827\n",
      "Training epoch: 19, training rmse: 0.677094, vali rmse:0.705935\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 0.4547\n",
      "Training loss (for one batch) at step 200: 0.5861\n",
      "Training loss (for one batch) at step 400: 0.4230\n",
      "Training loss (for one batch) at step 600: 0.4764\n",
      "Training epoch: 20, training rmse: 0.668928, vali rmse:0.698846\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 0.4486\n",
      "Training loss (for one batch) at step 200: 0.5741\n",
      "Training loss (for one batch) at step 400: 0.4210\n",
      "Training loss (for one batch) at step 600: 0.4704\n",
      "Training epoch: 21, training rmse: 0.661237, vali rmse:0.692251\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 0.4430\n",
      "Training loss (for one batch) at step 200: 0.5629\n",
      "Training loss (for one batch) at step 400: 0.4191\n",
      "Training loss (for one batch) at step 600: 0.4647\n",
      "Training epoch: 22, training rmse: 0.653969, vali rmse:0.686092\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 0.4377\n",
      "Training loss (for one batch) at step 200: 0.5524\n",
      "Training loss (for one batch) at step 400: 0.4171\n",
      "Training loss (for one batch) at step 600: 0.4592\n",
      "Training epoch: 23, training rmse: 0.647079, vali rmse:0.680320\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 0.4327\n",
      "Training loss (for one batch) at step 200: 0.5427\n",
      "Training loss (for one batch) at step 400: 0.4152\n",
      "Training loss (for one batch) at step 600: 0.4540\n",
      "Training epoch: 24, training rmse: 0.640530, vali rmse:0.674892\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 0.4279\n",
      "Training loss (for one batch) at step 200: 0.5335\n",
      "Training loss (for one batch) at step 400: 0.4133\n",
      "Training loss (for one batch) at step 600: 0.4489\n",
      "Training epoch: 25, training rmse: 0.634289, vali rmse:0.669771\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 0.4234\n",
      "Training loss (for one batch) at step 200: 0.5249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 400: 0.4114\n",
      "Training loss (for one batch) at step 600: 0.4440\n",
      "Training epoch: 26, training rmse: 0.628329, vali rmse:0.664926\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 0.4190\n",
      "Training loss (for one batch) at step 200: 0.5168\n",
      "Training loss (for one batch) at step 400: 0.4094\n",
      "Training loss (for one batch) at step 600: 0.4392\n",
      "Training epoch: 27, training rmse: 0.622626, vali rmse:0.660330\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 0.4148\n",
      "Training loss (for one batch) at step 200: 0.5091\n",
      "Training loss (for one batch) at step 400: 0.4074\n",
      "Training loss (for one batch) at step 600: 0.4346\n",
      "Training epoch: 28, training rmse: 0.617158, vali rmse:0.655960\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 0.4108\n",
      "Training loss (for one batch) at step 200: 0.5018\n",
      "Training loss (for one batch) at step 400: 0.4054\n",
      "Training loss (for one batch) at step 600: 0.4300\n",
      "Training epoch: 29, training rmse: 0.611909, vali rmse:0.651795\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 0.4069\n",
      "Training loss (for one batch) at step 200: 0.4949\n",
      "Training loss (for one batch) at step 400: 0.4034\n",
      "Training loss (for one batch) at step 600: 0.4257\n",
      "Training epoch: 30, training rmse: 0.606862, vali rmse:0.647819\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 0.4031\n",
      "Training loss (for one batch) at step 200: 0.4883\n",
      "Training loss (for one batch) at step 400: 0.4014\n",
      "Training loss (for one batch) at step 600: 0.4214\n",
      "Training epoch: 31, training rmse: 0.602003, vali rmse:0.644016\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 0.3994\n",
      "Training loss (for one batch) at step 200: 0.4820\n",
      "Training loss (for one batch) at step 400: 0.3994\n",
      "Training loss (for one batch) at step 600: 0.4172\n",
      "Training epoch: 32, training rmse: 0.597320, vali rmse:0.640372\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 0.3959\n",
      "Training loss (for one batch) at step 200: 0.4759\n",
      "Training loss (for one batch) at step 400: 0.3974\n",
      "Training loss (for one batch) at step 600: 0.4131\n",
      "Training epoch: 33, training rmse: 0.592803, vali rmse:0.636876\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 0.3925\n",
      "Training loss (for one batch) at step 200: 0.4702\n",
      "Training loss (for one batch) at step 400: 0.3954\n",
      "Training loss (for one batch) at step 600: 0.4091\n",
      "Training epoch: 34, training rmse: 0.588441, vali rmse:0.633517\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 0.3891\n",
      "Training loss (for one batch) at step 200: 0.4647\n",
      "Training loss (for one batch) at step 400: 0.3933\n",
      "Training loss (for one batch) at step 600: 0.4053\n",
      "Training epoch: 35, training rmse: 0.584226, vali rmse:0.630287\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 0.3859\n",
      "Training loss (for one batch) at step 200: 0.4594\n",
      "Training loss (for one batch) at step 400: 0.3913\n",
      "Training loss (for one batch) at step 600: 0.4015\n",
      "Training epoch: 36, training rmse: 0.580150, vali rmse:0.627176\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 0.3827\n",
      "Training loss (for one batch) at step 200: 0.4543\n",
      "Training loss (for one batch) at step 400: 0.3893\n",
      "Training loss (for one batch) at step 600: 0.3978\n",
      "Training epoch: 37, training rmse: 0.576206, vali rmse:0.624179\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 0.3796\n",
      "Training loss (for one batch) at step 200: 0.4494\n",
      "Training loss (for one batch) at step 400: 0.3873\n",
      "Training loss (for one batch) at step 600: 0.3942\n",
      "Training epoch: 38, training rmse: 0.572388, vali rmse:0.621287\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 0.3766\n",
      "Training loss (for one batch) at step 200: 0.4447\n",
      "Training loss (for one batch) at step 400: 0.3853\n",
      "Training loss (for one batch) at step 600: 0.3906\n",
      "Training epoch: 39, training rmse: 0.568689, vali rmse:0.618496\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 0.3737\n",
      "Training loss (for one batch) at step 200: 0.4401\n",
      "Training loss (for one batch) at step 400: 0.3834\n",
      "Training loss (for one batch) at step 600: 0.3872\n",
      "Training epoch: 40, training rmse: 0.565105, vali rmse:0.615800\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 0.3709\n",
      "Training loss (for one batch) at step 200: 0.4358\n",
      "Training loss (for one batch) at step 400: 0.3814\n",
      "Training loss (for one batch) at step 600: 0.3838\n",
      "Training epoch: 41, training rmse: 0.561630, vali rmse:0.613194\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 0.3681\n",
      "Training loss (for one batch) at step 200: 0.4315\n",
      "Training loss (for one batch) at step 400: 0.3795\n",
      "Training loss (for one batch) at step 600: 0.3805\n",
      "Training epoch: 42, training rmse: 0.558260, vali rmse:0.610675\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 0.3655\n",
      "Training loss (for one batch) at step 200: 0.4275\n",
      "Training loss (for one batch) at step 400: 0.3776\n",
      "Training loss (for one batch) at step 600: 0.3773\n",
      "Training epoch: 43, training rmse: 0.554990, vali rmse:0.608237\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 0.3628\n",
      "Training loss (for one batch) at step 200: 0.4235\n",
      "Training loss (for one batch) at step 400: 0.3757\n",
      "Training loss (for one batch) at step 600: 0.3742\n",
      "Training epoch: 44, training rmse: 0.551817, vali rmse:0.605877\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 0.3603\n",
      "Training loss (for one batch) at step 200: 0.4197\n",
      "Training loss (for one batch) at step 400: 0.3739\n",
      "Training loss (for one batch) at step 600: 0.3712\n",
      "Training epoch: 45, training rmse: 0.548737, vali rmse:0.603592\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 0.3578\n",
      "Training loss (for one batch) at step 200: 0.4161\n",
      "Training loss (for one batch) at step 400: 0.3720\n",
      "Training loss (for one batch) at step 600: 0.3682\n",
      "Training epoch: 46, training rmse: 0.545746, vali rmse:0.601379\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 0.3554\n",
      "Training loss (for one batch) at step 200: 0.4125\n",
      "Training loss (for one batch) at step 400: 0.3702\n",
      "Training loss (for one batch) at step 600: 0.3653\n",
      "Training epoch: 47, training rmse: 0.542842, vali rmse:0.599235\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 0.3531\n",
      "Training loss (for one batch) at step 200: 0.4091\n",
      "Training loss (for one batch) at step 400: 0.3685\n",
      "Training loss (for one batch) at step 600: 0.3625\n",
      "Training epoch: 48, training rmse: 0.540020, vali rmse:0.597157\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 0.3508\n",
      "Training loss (for one batch) at step 200: 0.4057\n",
      "Training loss (for one batch) at step 400: 0.3667\n",
      "Training loss (for one batch) at step 600: 0.3597\n",
      "Training epoch: 49, training rmse: 0.537279, vali rmse:0.595142\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 0.3486\n",
      "Training loss (for one batch) at step 200: 0.4025\n",
      "Training loss (for one batch) at step 400: 0.3650\n",
      "Training loss (for one batch) at step 600: 0.3570\n",
      "Training epoch: 50, training rmse: 0.534614, vali rmse:0.593189\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 0.3464\n",
      "Training loss (for one batch) at step 200: 0.3994\n",
      "Training loss (for one batch) at step 400: 0.3634\n",
      "Training loss (for one batch) at step 600: 0.3544\n",
      "Training epoch: 51, training rmse: 0.532025, vali rmse:0.591295\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 0.3443\n",
      "Training loss (for one batch) at step 200: 0.3964\n",
      "Training loss (for one batch) at step 400: 0.3617\n",
      "Training loss (for one batch) at step 600: 0.3519\n",
      "Training epoch: 52, training rmse: 0.529508, vali rmse:0.589458\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 0.3422\n",
      "Training loss (for one batch) at step 200: 0.3934\n",
      "Training loss (for one batch) at step 400: 0.3601\n",
      "Training loss (for one batch) at step 600: 0.3494\n",
      "Training epoch: 53, training rmse: 0.527061, vali rmse:0.587675\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 0.3403\n",
      "Training loss (for one batch) at step 200: 0.3906\n",
      "Training loss (for one batch) at step 400: 0.3585\n",
      "Training loss (for one batch) at step 600: 0.3469\n",
      "Training epoch: 54, training rmse: 0.524681, vali rmse:0.585946\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 0.3383\n",
      "Training loss (for one batch) at step 200: 0.3878\n",
      "Training loss (for one batch) at step 400: 0.3570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 600: 0.3446\n",
      "Training epoch: 55, training rmse: 0.522368, vali rmse:0.584268\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 0.3364\n",
      "Training loss (for one batch) at step 200: 0.3852\n",
      "Training loss (for one batch) at step 400: 0.3555\n",
      "Training loss (for one batch) at step 600: 0.3422\n",
      "Training epoch: 56, training rmse: 0.520117, vali rmse:0.582640\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 0.3346\n",
      "Training loss (for one batch) at step 200: 0.3826\n",
      "Training loss (for one batch) at step 400: 0.3540\n",
      "Training loss (for one batch) at step 600: 0.3400\n",
      "Training epoch: 57, training rmse: 0.517928, vali rmse:0.581060\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 0.3328\n",
      "Training loss (for one batch) at step 200: 0.3800\n",
      "Training loss (for one batch) at step 400: 0.3526\n",
      "Training loss (for one batch) at step 600: 0.3378\n",
      "Training epoch: 58, training rmse: 0.515798, vali rmse:0.579527\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 0.3311\n",
      "Training loss (for one batch) at step 200: 0.3776\n",
      "Training loss (for one batch) at step 400: 0.3512\n",
      "Training loss (for one batch) at step 600: 0.3357\n",
      "Training epoch: 59, training rmse: 0.513727, vali rmse:0.578039\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 0.3294\n",
      "Training loss (for one batch) at step 200: 0.3752\n",
      "Training loss (for one batch) at step 400: 0.3498\n",
      "Training loss (for one batch) at step 600: 0.3336\n",
      "Training epoch: 60, training rmse: 0.511711, vali rmse:0.576594\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 0.3277\n",
      "Training loss (for one batch) at step 200: 0.3729\n",
      "Training loss (for one batch) at step 400: 0.3484\n",
      "Training loss (for one batch) at step 600: 0.3315\n",
      "Training epoch: 61, training rmse: 0.509749, vali rmse:0.575192\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 0.3261\n",
      "Training loss (for one batch) at step 200: 0.3707\n",
      "Training loss (for one batch) at step 400: 0.3471\n",
      "Training loss (for one batch) at step 600: 0.3296\n",
      "Training epoch: 62, training rmse: 0.507841, vali rmse:0.573830\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 0.3246\n",
      "Training loss (for one batch) at step 200: 0.3685\n",
      "Training loss (for one batch) at step 400: 0.3458\n",
      "Training loss (for one batch) at step 600: 0.3276\n",
      "Training epoch: 63, training rmse: 0.505983, vali rmse:0.572509\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 0.3231\n",
      "Training loss (for one batch) at step 200: 0.3664\n",
      "Training loss (for one batch) at step 400: 0.3446\n",
      "Training loss (for one batch) at step 600: 0.3258\n",
      "Training epoch: 64, training rmse: 0.504175, vali rmse:0.571226\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 0.3216\n",
      "Training loss (for one batch) at step 200: 0.3644\n",
      "Training loss (for one batch) at step 400: 0.3433\n",
      "Training loss (for one batch) at step 600: 0.3239\n",
      "Training epoch: 65, training rmse: 0.502415, vali rmse:0.569981\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 0.3202\n",
      "Training loss (for one batch) at step 200: 0.3624\n",
      "Training loss (for one batch) at step 400: 0.3421\n",
      "Training loss (for one batch) at step 600: 0.3221\n",
      "Training epoch: 66, training rmse: 0.500702, vali rmse:0.568772\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 0.3188\n",
      "Training loss (for one batch) at step 200: 0.3604\n",
      "Training loss (for one batch) at step 400: 0.3410\n",
      "Training loss (for one batch) at step 600: 0.3204\n",
      "Training epoch: 67, training rmse: 0.499035, vali rmse:0.567599\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 0.3174\n",
      "Training loss (for one batch) at step 200: 0.3585\n",
      "Training loss (for one batch) at step 400: 0.3398\n",
      "Training loss (for one batch) at step 600: 0.3187\n",
      "Training epoch: 68, training rmse: 0.497411, vali rmse:0.566460\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 0.3161\n",
      "Training loss (for one batch) at step 200: 0.3567\n",
      "Training loss (for one batch) at step 400: 0.3387\n",
      "Training loss (for one batch) at step 600: 0.3170\n",
      "Training epoch: 69, training rmse: 0.495830, vali rmse:0.565354\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 0.3148\n",
      "Training loss (for one batch) at step 200: 0.3549\n",
      "Training loss (for one batch) at step 400: 0.3376\n",
      "Training loss (for one batch) at step 600: 0.3154\n",
      "Training epoch: 70, training rmse: 0.494291, vali rmse:0.564280\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 0.3136\n",
      "Training loss (for one batch) at step 200: 0.3532\n",
      "Training loss (for one batch) at step 400: 0.3366\n",
      "Training loss (for one batch) at step 600: 0.3139\n",
      "Training epoch: 71, training rmse: 0.492792, vali rmse:0.563238\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 0.3124\n",
      "Training loss (for one batch) at step 200: 0.3515\n",
      "Training loss (for one batch) at step 400: 0.3355\n",
      "Training loss (for one batch) at step 600: 0.3123\n",
      "Training epoch: 72, training rmse: 0.491333, vali rmse:0.562226\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 0.3112\n",
      "Training loss (for one batch) at step 200: 0.3499\n",
      "Training loss (for one batch) at step 400: 0.3345\n",
      "Training loss (for one batch) at step 600: 0.3108\n",
      "Training epoch: 73, training rmse: 0.489912, vali rmse:0.561244\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 0.3100\n",
      "Training loss (for one batch) at step 200: 0.3483\n",
      "Training loss (for one batch) at step 400: 0.3335\n",
      "Training loss (for one batch) at step 600: 0.3094\n",
      "Training epoch: 74, training rmse: 0.488528, vali rmse:0.560291\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 0.3089\n",
      "Training loss (for one batch) at step 200: 0.3468\n",
      "Training loss (for one batch) at step 400: 0.3326\n",
      "Training loss (for one batch) at step 600: 0.3080\n",
      "Training epoch: 75, training rmse: 0.487179, vali rmse:0.559365\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 0.3078\n",
      "Training loss (for one batch) at step 200: 0.3453\n",
      "Training loss (for one batch) at step 400: 0.3316\n",
      "Training loss (for one batch) at step 600: 0.3066\n",
      "Training epoch: 76, training rmse: 0.485866, vali rmse:0.558467\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 0.3068\n",
      "Training loss (for one batch) at step 200: 0.3438\n",
      "Training loss (for one batch) at step 400: 0.3307\n",
      "Training loss (for one batch) at step 600: 0.3052\n",
      "Training epoch: 77, training rmse: 0.484587, vali rmse:0.557595\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 0.3058\n",
      "Training loss (for one batch) at step 200: 0.3424\n",
      "Training loss (for one batch) at step 400: 0.3298\n",
      "Training loss (for one batch) at step 600: 0.3039\n",
      "Training epoch: 78, training rmse: 0.483341, vali rmse:0.556749\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 0.3048\n",
      "Training loss (for one batch) at step 200: 0.3410\n",
      "Training loss (for one batch) at step 400: 0.3289\n",
      "Training loss (for one batch) at step 600: 0.3026\n",
      "Training epoch: 79, training rmse: 0.482128, vali rmse:0.555928\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 0.3038\n",
      "Training loss (for one batch) at step 200: 0.3397\n",
      "Training loss (for one batch) at step 400: 0.3281\n",
      "Training loss (for one batch) at step 600: 0.3014\n",
      "Training epoch: 80, training rmse: 0.480945, vali rmse:0.555130\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 0.3028\n",
      "Training loss (for one batch) at step 200: 0.3384\n",
      "Training loss (for one batch) at step 400: 0.3273\n",
      "Training loss (for one batch) at step 600: 0.3002\n",
      "Training epoch: 81, training rmse: 0.479793, vali rmse:0.554357\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 0.3019\n",
      "Training loss (for one batch) at step 200: 0.3371\n",
      "Training loss (for one batch) at step 400: 0.3265\n",
      "Training loss (for one batch) at step 600: 0.2990\n",
      "Training epoch: 82, training rmse: 0.478671, vali rmse:0.553606\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 0.3010\n",
      "Training loss (for one batch) at step 200: 0.3358\n",
      "Training loss (for one batch) at step 400: 0.3257\n",
      "Training loss (for one batch) at step 600: 0.2978\n",
      "Training epoch: 83, training rmse: 0.477577, vali rmse:0.552877\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 0.3002\n",
      "Training loss (for one batch) at step 200: 0.3346\n",
      "Training loss (for one batch) at step 400: 0.3249\n",
      "Training loss (for one batch) at step 600: 0.2967\n",
      "Training epoch: 84, training rmse: 0.476511, vali rmse:0.552170\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 0.2993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 200: 0.3335\n",
      "Training loss (for one batch) at step 400: 0.3242\n",
      "Training loss (for one batch) at step 600: 0.2956\n",
      "Training epoch: 85, training rmse: 0.475472, vali rmse:0.551483\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 0.2985\n",
      "Training loss (for one batch) at step 200: 0.3323\n",
      "Training loss (for one batch) at step 400: 0.3234\n",
      "Training loss (for one batch) at step 600: 0.2945\n",
      "Training epoch: 86, training rmse: 0.474460, vali rmse:0.550817\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 0.2977\n",
      "Training loss (for one batch) at step 200: 0.3312\n",
      "Training loss (for one batch) at step 400: 0.3227\n",
      "Training loss (for one batch) at step 600: 0.2935\n",
      "Training epoch: 87, training rmse: 0.473473, vali rmse:0.550171\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 0.2969\n",
      "Training loss (for one batch) at step 200: 0.3301\n",
      "Training loss (for one batch) at step 400: 0.3220\n",
      "Training loss (for one batch) at step 600: 0.2924\n",
      "Training epoch: 88, training rmse: 0.472512, vali rmse:0.549544\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 0.2962\n",
      "Training loss (for one batch) at step 200: 0.3291\n",
      "Training loss (for one batch) at step 400: 0.3213\n",
      "Training loss (for one batch) at step 600: 0.2914\n",
      "Training epoch: 89, training rmse: 0.471574, vali rmse:0.548936\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 0.2954\n",
      "Training loss (for one batch) at step 200: 0.3280\n",
      "Training loss (for one batch) at step 400: 0.3207\n",
      "Training loss (for one batch) at step 600: 0.2905\n",
      "Training epoch: 90, training rmse: 0.470660, vali rmse:0.548345\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 0.2947\n",
      "Training loss (for one batch) at step 200: 0.3271\n",
      "Training loss (for one batch) at step 400: 0.3200\n",
      "Training loss (for one batch) at step 600: 0.2895\n",
      "Training epoch: 91, training rmse: 0.469770, vali rmse:0.547773\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 0.2940\n",
      "Training loss (for one batch) at step 200: 0.3261\n",
      "Training loss (for one batch) at step 400: 0.3194\n",
      "Training loss (for one batch) at step 600: 0.2886\n",
      "Training epoch: 92, training rmse: 0.468901, vali rmse:0.547217\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 0.2933\n",
      "Training loss (for one batch) at step 200: 0.3251\n",
      "Training loss (for one batch) at step 400: 0.3188\n",
      "Training loss (for one batch) at step 600: 0.2877\n",
      "Training epoch: 93, training rmse: 0.468054, vali rmse:0.546678\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 0.2926\n",
      "Training loss (for one batch) at step 200: 0.3242\n",
      "Training loss (for one batch) at step 400: 0.3182\n",
      "Training loss (for one batch) at step 600: 0.2868\n",
      "Training epoch: 94, training rmse: 0.467229, vali rmse:0.546155\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 0.2920\n",
      "Training loss (for one batch) at step 200: 0.3233\n",
      "Training loss (for one batch) at step 400: 0.3176\n",
      "Training loss (for one batch) at step 600: 0.2860\n",
      "Training epoch: 95, training rmse: 0.466424, vali rmse:0.545648\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 0.2914\n",
      "Training loss (for one batch) at step 200: 0.3224\n",
      "Training loss (for one batch) at step 400: 0.3171\n",
      "Training loss (for one batch) at step 600: 0.2851\n",
      "Training epoch: 96, training rmse: 0.465639, vali rmse:0.545155\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 0.2908\n",
      "Training loss (for one batch) at step 200: 0.3216\n",
      "Training loss (for one batch) at step 400: 0.3165\n",
      "Training loss (for one batch) at step 600: 0.2843\n",
      "Training epoch: 97, training rmse: 0.464873, vali rmse:0.544678\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 0.2902\n",
      "Training loss (for one batch) at step 200: 0.3208\n",
      "Training loss (for one batch) at step 400: 0.3160\n",
      "Training loss (for one batch) at step 600: 0.2835\n",
      "Training epoch: 98, training rmse: 0.464126, vali rmse:0.544215\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 0.2896\n",
      "Training loss (for one batch) at step 200: 0.3199\n",
      "Training loss (for one batch) at step 400: 0.3154\n",
      "Training loss (for one batch) at step 600: 0.2827\n",
      "Training epoch: 99, training rmse: 0.463397, vali rmse:0.543766\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 0.2890\n",
      "Training loss (for one batch) at step 200: 0.3192\n",
      "Training loss (for one batch) at step 400: 0.3149\n",
      "Training loss (for one batch) at step 600: 0.2820\n",
      "Training epoch: 100, training rmse: 0.462687, vali rmse:0.543330\n",
      "\n",
      "Start of epoch 100\n",
      "Training loss (for one batch) at step 0: 0.2885\n",
      "Training loss (for one batch) at step 200: 0.3184\n",
      "Training loss (for one batch) at step 400: 0.3144\n",
      "Training loss (for one batch) at step 600: 0.2812\n",
      "Training epoch: 101, training rmse: 0.461994, vali rmse:0.542908\n",
      "\n",
      "Start of epoch 101\n",
      "Training loss (for one batch) at step 0: 0.2879\n",
      "Training loss (for one batch) at step 200: 0.3176\n",
      "Training loss (for one batch) at step 400: 0.3139\n",
      "Training loss (for one batch) at step 600: 0.2805\n",
      "Training epoch: 102, training rmse: 0.461317, vali rmse:0.542498\n",
      "\n",
      "Start of epoch 102\n",
      "Training loss (for one batch) at step 0: 0.2874\n",
      "Training loss (for one batch) at step 200: 0.3169\n",
      "Training loss (for one batch) at step 400: 0.3135\n",
      "Training loss (for one batch) at step 600: 0.2798\n",
      "Training epoch: 103, training rmse: 0.460657, vali rmse:0.542101\n",
      "\n",
      "Start of epoch 103\n",
      "Training loss (for one batch) at step 0: 0.2869\n",
      "Training loss (for one batch) at step 200: 0.3162\n",
      "Training loss (for one batch) at step 400: 0.3130\n",
      "Training loss (for one batch) at step 600: 0.2791\n",
      "Training epoch: 104, training rmse: 0.460013, vali rmse:0.541716\n",
      "\n",
      "Start of epoch 104\n",
      "Training loss (for one batch) at step 0: 0.2864\n",
      "Training loss (for one batch) at step 200: 0.3155\n",
      "Training loss (for one batch) at step 400: 0.3125\n",
      "Training loss (for one batch) at step 600: 0.2784\n",
      "Training epoch: 105, training rmse: 0.459385, vali rmse:0.541342\n",
      "\n",
      "Start of epoch 105\n",
      "Training loss (for one batch) at step 0: 0.2859\n",
      "Training loss (for one batch) at step 200: 0.3148\n",
      "Training loss (for one batch) at step 400: 0.3121\n",
      "Training loss (for one batch) at step 600: 0.2778\n",
      "Training epoch: 106, training rmse: 0.458771, vali rmse:0.540980\n",
      "\n",
      "Start of epoch 106\n",
      "Training loss (for one batch) at step 0: 0.2854\n",
      "Training loss (for one batch) at step 200: 0.3142\n",
      "Training loss (for one batch) at step 400: 0.3117\n",
      "Training loss (for one batch) at step 600: 0.2771\n",
      "Training epoch: 107, training rmse: 0.458173, vali rmse:0.540628\n",
      "\n",
      "Start of epoch 107\n",
      "Training loss (for one batch) at step 0: 0.2850\n",
      "Training loss (for one batch) at step 200: 0.3135\n",
      "Training loss (for one batch) at step 400: 0.3112\n",
      "Training loss (for one batch) at step 600: 0.2765\n",
      "Training epoch: 108, training rmse: 0.457588, vali rmse:0.540288\n",
      "\n",
      "Start of epoch 108\n",
      "Training loss (for one batch) at step 0: 0.2845\n",
      "Training loss (for one batch) at step 200: 0.3129\n",
      "Training loss (for one batch) at step 400: 0.3108\n",
      "Training loss (for one batch) at step 600: 0.2759\n",
      "Training epoch: 109, training rmse: 0.457018, vali rmse:0.539957\n",
      "\n",
      "Start of epoch 109\n",
      "Training loss (for one batch) at step 0: 0.2841\n",
      "Training loss (for one batch) at step 200: 0.3123\n",
      "Training loss (for one batch) at step 400: 0.3104\n",
      "Training loss (for one batch) at step 600: 0.2753\n",
      "Training epoch: 110, training rmse: 0.456461, vali rmse:0.539637\n",
      "\n",
      "Start of epoch 110\n",
      "Training loss (for one batch) at step 0: 0.2837\n",
      "Training loss (for one batch) at step 200: 0.3117\n",
      "Training loss (for one batch) at step 400: 0.3100\n",
      "Training loss (for one batch) at step 600: 0.2747\n",
      "Training epoch: 111, training rmse: 0.455917, vali rmse:0.539327\n",
      "\n",
      "Start of epoch 111\n",
      "Training loss (for one batch) at step 0: 0.2833\n",
      "Training loss (for one batch) at step 200: 0.3111\n",
      "Training loss (for one batch) at step 400: 0.3096\n",
      "Training loss (for one batch) at step 600: 0.2741\n",
      "Training epoch: 112, training rmse: 0.455386, vali rmse:0.539026\n",
      "\n",
      "Start of epoch 112\n",
      "Training loss (for one batch) at step 0: 0.2829\n",
      "Training loss (for one batch) at step 200: 0.3106\n",
      "Training loss (for one batch) at step 400: 0.3093\n",
      "Training loss (for one batch) at step 600: 0.2735\n",
      "Training epoch: 113, training rmse: 0.454867, vali rmse:0.538734\n",
      "\n",
      "Start of epoch 113\n",
      "Training loss (for one batch) at step 0: 0.2825\n",
      "Training loss (for one batch) at step 200: 0.3100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 400: 0.3089\n",
      "Training loss (for one batch) at step 600: 0.2730\n",
      "Training epoch: 114, training rmse: 0.454360, vali rmse:0.538451\n",
      "\n",
      "Start of epoch 114\n",
      "Training loss (for one batch) at step 0: 0.2821\n",
      "Training loss (for one batch) at step 200: 0.3095\n",
      "Training loss (for one batch) at step 400: 0.3085\n",
      "Training loss (for one batch) at step 600: 0.2725\n",
      "Training epoch: 115, training rmse: 0.453866, vali rmse:0.538177\n",
      "\n",
      "Start of epoch 115\n",
      "Training loss (for one batch) at step 0: 0.2817\n",
      "Training loss (for one batch) at step 200: 0.3089\n",
      "Training loss (for one batch) at step 400: 0.3082\n",
      "Training loss (for one batch) at step 600: 0.2719\n",
      "Training epoch: 116, training rmse: 0.453382, vali rmse:0.537912\n",
      "\n",
      "Start of epoch 116\n",
      "Training loss (for one batch) at step 0: 0.2813\n",
      "Training loss (for one batch) at step 200: 0.3084\n",
      "Training loss (for one batch) at step 400: 0.3079\n",
      "Training loss (for one batch) at step 600: 0.2714\n",
      "Training epoch: 117, training rmse: 0.452910, vali rmse:0.537654\n",
      "\n",
      "Start of epoch 117\n",
      "Training loss (for one batch) at step 0: 0.2810\n",
      "Training loss (for one batch) at step 200: 0.3079\n",
      "Training loss (for one batch) at step 400: 0.3075\n",
      "Training loss (for one batch) at step 600: 0.2709\n",
      "Training epoch: 118, training rmse: 0.452448, vali rmse:0.537405\n",
      "\n",
      "Start of epoch 118\n",
      "Training loss (for one batch) at step 0: 0.2806\n",
      "Training loss (for one batch) at step 200: 0.3074\n",
      "Training loss (for one batch) at step 400: 0.3072\n",
      "Training loss (for one batch) at step 600: 0.2704\n",
      "Training epoch: 119, training rmse: 0.451997, vali rmse:0.537163\n",
      "\n",
      "Start of epoch 119\n",
      "Training loss (for one batch) at step 0: 0.2803\n",
      "Training loss (for one batch) at step 200: 0.3070\n",
      "Training loss (for one batch) at step 400: 0.3069\n",
      "Training loss (for one batch) at step 600: 0.2700\n",
      "Training epoch: 120, training rmse: 0.451556, vali rmse:0.536928\n",
      "\n",
      "Start of epoch 120\n",
      "Training loss (for one batch) at step 0: 0.2800\n",
      "Training loss (for one batch) at step 200: 0.3065\n",
      "Training loss (for one batch) at step 400: 0.3066\n",
      "Training loss (for one batch) at step 600: 0.2695\n",
      "Training epoch: 121, training rmse: 0.451125, vali rmse:0.536701\n",
      "\n",
      "Start of epoch 121\n",
      "Training loss (for one batch) at step 0: 0.2796\n",
      "Training loss (for one batch) at step 200: 0.3060\n",
      "Training loss (for one batch) at step 400: 0.3063\n",
      "Training loss (for one batch) at step 600: 0.2690\n",
      "Training epoch: 122, training rmse: 0.450704, vali rmse:0.536481\n",
      "\n",
      "Start of epoch 122\n",
      "Training loss (for one batch) at step 0: 0.2793\n",
      "Training loss (for one batch) at step 200: 0.3056\n",
      "Training loss (for one batch) at step 400: 0.3060\n",
      "Training loss (for one batch) at step 600: 0.2686\n",
      "Training epoch: 123, training rmse: 0.450292, vali rmse:0.536268\n",
      "\n",
      "Start of epoch 123\n",
      "Training loss (for one batch) at step 0: 0.2790\n",
      "Training loss (for one batch) at step 200: 0.3052\n",
      "Training loss (for one batch) at step 400: 0.3057\n",
      "Training loss (for one batch) at step 600: 0.2681\n",
      "Training epoch: 124, training rmse: 0.449889, vali rmse:0.536061\n",
      "\n",
      "Start of epoch 124\n",
      "Training loss (for one batch) at step 0: 0.2787\n",
      "Training loss (for one batch) at step 200: 0.3047\n",
      "Training loss (for one batch) at step 400: 0.3054\n",
      "Training loss (for one batch) at step 600: 0.2677\n",
      "Training epoch: 125, training rmse: 0.449495, vali rmse:0.535861\n",
      "\n",
      "Start of epoch 125\n",
      "Training loss (for one batch) at step 0: 0.2784\n",
      "Training loss (for one batch) at step 200: 0.3043\n",
      "Training loss (for one batch) at step 400: 0.3051\n",
      "Training loss (for one batch) at step 600: 0.2673\n",
      "Training epoch: 126, training rmse: 0.449110, vali rmse:0.535667\n",
      "\n",
      "Start of epoch 126\n",
      "Training loss (for one batch) at step 0: 0.2781\n",
      "Training loss (for one batch) at step 200: 0.3039\n",
      "Training loss (for one batch) at step 400: 0.3048\n",
      "Training loss (for one batch) at step 600: 0.2669\n",
      "Training epoch: 127, training rmse: 0.448733, vali rmse:0.535479\n",
      "\n",
      "Start of epoch 127\n",
      "Training loss (for one batch) at step 0: 0.2779\n",
      "Training loss (for one batch) at step 200: 0.3035\n",
      "Training loss (for one batch) at step 400: 0.3046\n",
      "Training loss (for one batch) at step 600: 0.2665\n",
      "Training epoch: 128, training rmse: 0.448364, vali rmse:0.535297\n",
      "\n",
      "Start of epoch 128\n",
      "Training loss (for one batch) at step 0: 0.2776\n",
      "Training loss (for one batch) at step 200: 0.3032\n",
      "Training loss (for one batch) at step 400: 0.3043\n",
      "Training loss (for one batch) at step 600: 0.2661\n",
      "Training epoch: 129, training rmse: 0.448003, vali rmse:0.535121\n",
      "\n",
      "Start of epoch 129\n",
      "Training loss (for one batch) at step 0: 0.2773\n",
      "Training loss (for one batch) at step 200: 0.3028\n",
      "Training loss (for one batch) at step 400: 0.3041\n",
      "Training loss (for one batch) at step 600: 0.2657\n",
      "Training epoch: 130, training rmse: 0.447650, vali rmse:0.534950\n",
      "\n",
      "Start of epoch 130\n",
      "Training loss (for one batch) at step 0: 0.2771\n",
      "Training loss (for one batch) at step 200: 0.3024\n",
      "Training loss (for one batch) at step 400: 0.3038\n",
      "Training loss (for one batch) at step 600: 0.2653\n",
      "Training epoch: 131, training rmse: 0.447304, vali rmse:0.534785\n",
      "\n",
      "Start of epoch 131\n",
      "Training loss (for one batch) at step 0: 0.2768\n",
      "Training loss (for one batch) at step 200: 0.3021\n",
      "Training loss (for one batch) at step 400: 0.3036\n",
      "Training loss (for one batch) at step 600: 0.2650\n",
      "Training epoch: 132, training rmse: 0.446966, vali rmse:0.534624\n",
      "\n",
      "Start of epoch 132\n",
      "Training loss (for one batch) at step 0: 0.2766\n",
      "Training loss (for one batch) at step 200: 0.3017\n",
      "Training loss (for one batch) at step 400: 0.3033\n",
      "Training loss (for one batch) at step 600: 0.2646\n",
      "Training epoch: 133, training rmse: 0.446634, vali rmse:0.534469\n",
      "\n",
      "Start of epoch 133\n",
      "Training loss (for one batch) at step 0: 0.2763\n",
      "Training loss (for one batch) at step 200: 0.3014\n",
      "Training loss (for one batch) at step 400: 0.3031\n",
      "Training loss (for one batch) at step 600: 0.2642\n",
      "Training epoch: 134, training rmse: 0.446310, vali rmse:0.534319\n",
      "\n",
      "Start of epoch 134\n",
      "Training loss (for one batch) at step 0: 0.2761\n",
      "Training loss (for one batch) at step 200: 0.3010\n",
      "Training loss (for one batch) at step 400: 0.3029\n",
      "Training loss (for one batch) at step 600: 0.2639\n",
      "Training epoch: 135, training rmse: 0.445992, vali rmse:0.534173\n",
      "\n",
      "Start of epoch 135\n",
      "Training loss (for one batch) at step 0: 0.2758\n",
      "Training loss (for one batch) at step 200: 0.3007\n",
      "Training loss (for one batch) at step 400: 0.3026\n",
      "Training loss (for one batch) at step 600: 0.2636\n",
      "Training epoch: 136, training rmse: 0.445681, vali rmse:0.534032\n",
      "\n",
      "Start of epoch 136\n",
      "Training loss (for one batch) at step 0: 0.2756\n",
      "Training loss (for one batch) at step 200: 0.3004\n",
      "Training loss (for one batch) at step 400: 0.3024\n",
      "Training loss (for one batch) at step 600: 0.2632\n",
      "Training epoch: 137, training rmse: 0.445376, vali rmse:0.533896\n",
      "\n",
      "Start of epoch 137\n",
      "Training loss (for one batch) at step 0: 0.2754\n",
      "Training loss (for one batch) at step 200: 0.3001\n",
      "Training loss (for one batch) at step 400: 0.3022\n",
      "Training loss (for one batch) at step 600: 0.2629\n",
      "Training epoch: 138, training rmse: 0.445077, vali rmse:0.533764\n",
      "\n",
      "Start of epoch 138\n",
      "Training loss (for one batch) at step 0: 0.2752\n",
      "Training loss (for one batch) at step 200: 0.2998\n",
      "Training loss (for one batch) at step 400: 0.3020\n",
      "Training loss (for one batch) at step 600: 0.2626\n",
      "Training epoch: 139, training rmse: 0.444784, vali rmse:0.533636\n",
      "\n",
      "Start of epoch 139\n",
      "Training loss (for one batch) at step 0: 0.2750\n",
      "Training loss (for one batch) at step 200: 0.2995\n",
      "Training loss (for one batch) at step 400: 0.3018\n",
      "Training loss (for one batch) at step 600: 0.2623\n",
      "Training epoch: 140, training rmse: 0.444497, vali rmse:0.533512\n",
      "\n",
      "Start of epoch 140\n",
      "Training loss (for one batch) at step 0: 0.2748\n",
      "Training loss (for one batch) at step 200: 0.2992\n",
      "Training loss (for one batch) at step 400: 0.3016\n",
      "Training loss (for one batch) at step 600: 0.2620\n",
      "Training epoch: 141, training rmse: 0.444215, vali rmse:0.533392\n",
      "\n",
      "Start of epoch 141\n",
      "Training loss (for one batch) at step 0: 0.2746\n",
      "Training loss (for one batch) at step 200: 0.2989\n",
      "Training loss (for one batch) at step 400: 0.3014\n",
      "Training loss (for one batch) at step 600: 0.2617\n",
      "Training epoch: 142, training rmse: 0.443940, vali rmse:0.533276\n",
      "\n",
      "Start of epoch 142\n",
      "Training loss (for one batch) at step 0: 0.2744\n",
      "Training loss (for one batch) at step 200: 0.2986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 400: 0.3012\n",
      "Training loss (for one batch) at step 600: 0.2614\n",
      "Training epoch: 143, training rmse: 0.443669, vali rmse:0.533164\n",
      "\n",
      "Start of epoch 143\n",
      "Training loss (for one batch) at step 0: 0.2742\n",
      "Training loss (for one batch) at step 200: 0.2983\n",
      "Training loss (for one batch) at step 400: 0.3010\n",
      "Training loss (for one batch) at step 600: 0.2611\n",
      "Training epoch: 144, training rmse: 0.443404, vali rmse:0.533055\n",
      "\n",
      "Start of epoch 144\n",
      "Training loss (for one batch) at step 0: 0.2740\n",
      "Training loss (for one batch) at step 200: 0.2981\n",
      "Training loss (for one batch) at step 400: 0.3008\n",
      "Training loss (for one batch) at step 600: 0.2608\n",
      "Training epoch: 145, training rmse: 0.443143, vali rmse:0.532950\n",
      "\n",
      "Start of epoch 145\n",
      "Training loss (for one batch) at step 0: 0.2738\n",
      "Training loss (for one batch) at step 200: 0.2978\n",
      "Training loss (for one batch) at step 400: 0.3006\n",
      "Training loss (for one batch) at step 600: 0.2605\n",
      "Training epoch: 146, training rmse: 0.442888, vali rmse:0.532848\n",
      "\n",
      "Start of epoch 146\n",
      "Training loss (for one batch) at step 0: 0.2736\n",
      "Training loss (for one batch) at step 200: 0.2976\n",
      "Training loss (for one batch) at step 400: 0.3004\n",
      "Training loss (for one batch) at step 600: 0.2602\n",
      "Training epoch: 147, training rmse: 0.442637, vali rmse:0.532749\n",
      "\n",
      "Start of epoch 147\n",
      "Training loss (for one batch) at step 0: 0.2734\n",
      "Training loss (for one batch) at step 200: 0.2973\n",
      "Training loss (for one batch) at step 400: 0.3003\n",
      "Training loss (for one batch) at step 600: 0.2600\n",
      "Training epoch: 148, training rmse: 0.442391, vali rmse:0.532654\n",
      "\n",
      "Start of epoch 148\n",
      "Training loss (for one batch) at step 0: 0.2732\n",
      "Training loss (for one batch) at step 200: 0.2971\n",
      "Training loss (for one batch) at step 400: 0.3001\n",
      "Training loss (for one batch) at step 600: 0.2597\n",
      "Training epoch: 149, training rmse: 0.442149, vali rmse:0.532562\n",
      "\n",
      "Start of epoch 149\n",
      "Training loss (for one batch) at step 0: 0.2731\n",
      "Training loss (for one batch) at step 200: 0.2968\n",
      "Training loss (for one batch) at step 400: 0.2999\n",
      "Training loss (for one batch) at step 600: 0.2594\n",
      "Training epoch: 150, training rmse: 0.441911, vali rmse:0.532472\n",
      "\n",
      "Start of epoch 150\n",
      "Training loss (for one batch) at step 0: 0.2729\n",
      "Training loss (for one batch) at step 200: 0.2966\n",
      "Training loss (for one batch) at step 400: 0.2998\n",
      "Training loss (for one batch) at step 600: 0.2592\n",
      "Training epoch: 151, training rmse: 0.441678, vali rmse:0.532386\n",
      "\n",
      "Start of epoch 151\n",
      "Training loss (for one batch) at step 0: 0.2727\n",
      "Training loss (for one batch) at step 200: 0.2964\n",
      "Training loss (for one batch) at step 400: 0.2996\n",
      "Training loss (for one batch) at step 600: 0.2589\n",
      "Training epoch: 152, training rmse: 0.441449, vali rmse:0.532303\n",
      "\n",
      "Start of epoch 152\n",
      "Training loss (for one batch) at step 0: 0.2726\n",
      "Training loss (for one batch) at step 200: 0.2961\n",
      "Training loss (for one batch) at step 400: 0.2994\n",
      "Training loss (for one batch) at step 600: 0.2587\n",
      "Training epoch: 153, training rmse: 0.441224, vali rmse:0.532222\n",
      "\n",
      "Start of epoch 153\n",
      "Training loss (for one batch) at step 0: 0.2724\n",
      "Training loss (for one batch) at step 200: 0.2959\n",
      "Training loss (for one batch) at step 400: 0.2993\n",
      "Training loss (for one batch) at step 600: 0.2585\n",
      "Training epoch: 154, training rmse: 0.441003, vali rmse:0.532144\n",
      "\n",
      "Start of epoch 154\n",
      "Training loss (for one batch) at step 0: 0.2722\n",
      "Training loss (for one batch) at step 200: 0.2957\n",
      "Training loss (for one batch) at step 400: 0.2991\n",
      "Training loss (for one batch) at step 600: 0.2582\n",
      "Training epoch: 155, training rmse: 0.440786, vali rmse:0.532068\n",
      "\n",
      "Start of epoch 155\n",
      "Training loss (for one batch) at step 0: 0.2721\n",
      "Training loss (for one batch) at step 200: 0.2955\n",
      "Training loss (for one batch) at step 400: 0.2990\n",
      "Training loss (for one batch) at step 600: 0.2580\n",
      "Training epoch: 156, training rmse: 0.440572, vali rmse:0.531995\n",
      "\n",
      "Start of epoch 156\n",
      "Training loss (for one batch) at step 0: 0.2719\n",
      "Training loss (for one batch) at step 200: 0.2953\n",
      "Training loss (for one batch) at step 400: 0.2988\n",
      "Training loss (for one batch) at step 600: 0.2578\n",
      "Training epoch: 157, training rmse: 0.440362, vali rmse:0.531924\n",
      "\n",
      "Start of epoch 157\n",
      "Training loss (for one batch) at step 0: 0.2718\n",
      "Training loss (for one batch) at step 200: 0.2951\n",
      "Training loss (for one batch) at step 400: 0.2987\n",
      "Training loss (for one batch) at step 600: 0.2575\n",
      "Training epoch: 158, training rmse: 0.440155, vali rmse:0.531856\n",
      "\n",
      "Start of epoch 158\n",
      "Training loss (for one batch) at step 0: 0.2716\n",
      "Training loss (for one batch) at step 200: 0.2949\n",
      "Training loss (for one batch) at step 400: 0.2985\n",
      "Training loss (for one batch) at step 600: 0.2573\n",
      "Training epoch: 159, training rmse: 0.439952, vali rmse:0.531790\n",
      "\n",
      "Start of epoch 159\n",
      "Training loss (for one batch) at step 0: 0.2715\n",
      "Training loss (for one batch) at step 200: 0.2947\n",
      "Training loss (for one batch) at step 400: 0.2984\n",
      "Training loss (for one batch) at step 600: 0.2571\n",
      "Training epoch: 160, training rmse: 0.439752, vali rmse:0.531726\n",
      "\n",
      "Start of epoch 160\n",
      "Training loss (for one batch) at step 0: 0.2714\n",
      "Training loss (for one batch) at step 200: 0.2945\n",
      "Training loss (for one batch) at step 400: 0.2982\n",
      "Training loss (for one batch) at step 600: 0.2569\n",
      "Training epoch: 161, training rmse: 0.439555, vali rmse:0.531664\n",
      "\n",
      "Start of epoch 161\n",
      "Training loss (for one batch) at step 0: 0.2712\n",
      "Training loss (for one batch) at step 200: 0.2943\n",
      "Training loss (for one batch) at step 400: 0.2981\n",
      "Training loss (for one batch) at step 600: 0.2567\n",
      "Training epoch: 162, training rmse: 0.439361, vali rmse:0.531604\n",
      "\n",
      "Start of epoch 162\n",
      "Training loss (for one batch) at step 0: 0.2711\n",
      "Training loss (for one batch) at step 200: 0.2941\n",
      "Training loss (for one batch) at step 400: 0.2979\n",
      "Training loss (for one batch) at step 600: 0.2565\n",
      "Training epoch: 163, training rmse: 0.439170, vali rmse:0.531547\n",
      "\n",
      "Start of epoch 163\n",
      "Training loss (for one batch) at step 0: 0.2709\n",
      "Training loss (for one batch) at step 200: 0.2939\n",
      "Training loss (for one batch) at step 400: 0.2978\n",
      "Training loss (for one batch) at step 600: 0.2563\n",
      "Training epoch: 164, training rmse: 0.438982, vali rmse:0.531491\n",
      "\n",
      "Start of epoch 164\n",
      "Training loss (for one batch) at step 0: 0.2708\n",
      "Training loss (for one batch) at step 200: 0.2938\n",
      "Training loss (for one batch) at step 400: 0.2977\n",
      "Training loss (for one batch) at step 600: 0.2561\n",
      "Training epoch: 165, training rmse: 0.438797, vali rmse:0.531437\n",
      "\n",
      "Start of epoch 165\n",
      "Training loss (for one batch) at step 0: 0.2707\n",
      "Training loss (for one batch) at step 200: 0.2936\n",
      "Training loss (for one batch) at step 400: 0.2975\n",
      "Training loss (for one batch) at step 600: 0.2559\n",
      "Training epoch: 166, training rmse: 0.438615, vali rmse:0.531385\n",
      "\n",
      "Start of epoch 166\n",
      "Training loss (for one batch) at step 0: 0.2706\n",
      "Training loss (for one batch) at step 200: 0.2934\n",
      "Training loss (for one batch) at step 400: 0.2974\n",
      "Training loss (for one batch) at step 600: 0.2557\n",
      "Training epoch: 167, training rmse: 0.438435, vali rmse:0.531335\n",
      "\n",
      "Start of epoch 167\n",
      "Training loss (for one batch) at step 0: 0.2704\n",
      "Training loss (for one batch) at step 200: 0.2932\n",
      "Training loss (for one batch) at step 400: 0.2973\n",
      "Training loss (for one batch) at step 600: 0.2555\n",
      "Training epoch: 168, training rmse: 0.438258, vali rmse:0.531286\n",
      "\n",
      "Start of epoch 168\n",
      "Training loss (for one batch) at step 0: 0.2703\n",
      "Training loss (for one batch) at step 200: 0.2931\n",
      "Training loss (for one batch) at step 400: 0.2972\n",
      "Training loss (for one batch) at step 600: 0.2553\n",
      "Training epoch: 169, training rmse: 0.438083, vali rmse:0.531239\n",
      "\n",
      "Start of epoch 169\n",
      "Training loss (for one batch) at step 0: 0.2702\n",
      "Training loss (for one batch) at step 200: 0.2929\n",
      "Training loss (for one batch) at step 400: 0.2970\n",
      "Training loss (for one batch) at step 600: 0.2551\n",
      "Training epoch: 170, training rmse: 0.437911, vali rmse:0.531194\n",
      "\n",
      "Start of epoch 170\n",
      "Training loss (for one batch) at step 0: 0.2701\n",
      "Training loss (for one batch) at step 200: 0.2928\n",
      "Training loss (for one batch) at step 400: 0.2969\n",
      "Training loss (for one batch) at step 600: 0.2549\n",
      "Training epoch: 171, training rmse: 0.437741, vali rmse:0.531150\n",
      "\n",
      "Start of epoch 171\n",
      "Training loss (for one batch) at step 0: 0.2699\n",
      "Training loss (for one batch) at step 200: 0.2926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 400: 0.2968\n",
      "Training loss (for one batch) at step 600: 0.2547\n",
      "Training epoch: 172, training rmse: 0.437573, vali rmse:0.531108\n",
      "\n",
      "Start of epoch 172\n",
      "Training loss (for one batch) at step 0: 0.2698\n",
      "Training loss (for one batch) at step 200: 0.2924\n",
      "Training loss (for one batch) at step 400: 0.2967\n",
      "Training loss (for one batch) at step 600: 0.2546\n",
      "Training epoch: 173, training rmse: 0.437407, vali rmse:0.531067\n",
      "\n",
      "Start of epoch 173\n",
      "Training loss (for one batch) at step 0: 0.2697\n",
      "Training loss (for one batch) at step 200: 0.2923\n",
      "Training loss (for one batch) at step 400: 0.2965\n",
      "Training loss (for one batch) at step 600: 0.2544\n",
      "Training epoch: 174, training rmse: 0.437244, vali rmse:0.531028\n",
      "\n",
      "Start of epoch 174\n",
      "Training loss (for one batch) at step 0: 0.2696\n",
      "Training loss (for one batch) at step 200: 0.2921\n",
      "Training loss (for one batch) at step 400: 0.2964\n",
      "Training loss (for one batch) at step 600: 0.2542\n",
      "Training epoch: 175, training rmse: 0.437083, vali rmse:0.530990\n",
      "\n",
      "Start of epoch 175\n",
      "Training loss (for one batch) at step 0: 0.2695\n",
      "Training loss (for one batch) at step 200: 0.2920\n",
      "Training loss (for one batch) at step 400: 0.2963\n",
      "Training loss (for one batch) at step 600: 0.2541\n",
      "Training epoch: 176, training rmse: 0.436923, vali rmse:0.530953\n",
      "\n",
      "Start of epoch 176\n",
      "Training loss (for one batch) at step 0: 0.2694\n",
      "Training loss (for one batch) at step 200: 0.2918\n",
      "Training loss (for one batch) at step 400: 0.2962\n",
      "Training loss (for one batch) at step 600: 0.2539\n",
      "Training epoch: 177, training rmse: 0.436766, vali rmse:0.530918\n",
      "\n",
      "Start of epoch 177\n",
      "Training loss (for one batch) at step 0: 0.2693\n",
      "Training loss (for one batch) at step 200: 0.2917\n",
      "Training loss (for one batch) at step 400: 0.2961\n",
      "Training loss (for one batch) at step 600: 0.2537\n",
      "Training epoch: 178, training rmse: 0.436611, vali rmse:0.530884\n",
      "\n",
      "Start of epoch 178\n",
      "Training loss (for one batch) at step 0: 0.2692\n",
      "Training loss (for one batch) at step 200: 0.2916\n",
      "Training loss (for one batch) at step 400: 0.2960\n",
      "Training loss (for one batch) at step 600: 0.2536\n",
      "Training epoch: 179, training rmse: 0.436457, vali rmse:0.530851\n",
      "\n",
      "Start of epoch 179\n",
      "Training loss (for one batch) at step 0: 0.2690\n",
      "Training loss (for one batch) at step 200: 0.2914\n",
      "Training loss (for one batch) at step 400: 0.2959\n",
      "Training loss (for one batch) at step 600: 0.2534\n",
      "Training epoch: 180, training rmse: 0.436305, vali rmse:0.530819\n",
      "\n",
      "Start of epoch 180\n",
      "Training loss (for one batch) at step 0: 0.2689\n",
      "Training loss (for one batch) at step 200: 0.2913\n",
      "Training loss (for one batch) at step 400: 0.2958\n",
      "Training loss (for one batch) at step 600: 0.2532\n",
      "Training epoch: 181, training rmse: 0.436155, vali rmse:0.530788\n",
      "\n",
      "Start of epoch 181\n",
      "Training loss (for one batch) at step 0: 0.2688\n",
      "Training loss (for one batch) at step 200: 0.2912\n",
      "Training loss (for one batch) at step 400: 0.2956\n",
      "Training loss (for one batch) at step 600: 0.2531\n",
      "Training epoch: 182, training rmse: 0.436006, vali rmse:0.530759\n",
      "\n",
      "Start of epoch 182\n",
      "Training loss (for one batch) at step 0: 0.2687\n",
      "Training loss (for one batch) at step 200: 0.2910\n",
      "Training loss (for one batch) at step 400: 0.2955\n",
      "Training loss (for one batch) at step 600: 0.2529\n",
      "Training epoch: 183, training rmse: 0.435860, vali rmse:0.530730\n",
      "\n",
      "Start of epoch 183\n",
      "Training loss (for one batch) at step 0: 0.2686\n",
      "Training loss (for one batch) at step 200: 0.2909\n",
      "Training loss (for one batch) at step 400: 0.2954\n",
      "Training loss (for one batch) at step 600: 0.2528\n",
      "Training epoch: 184, training rmse: 0.435714, vali rmse:0.530703\n",
      "\n",
      "Start of epoch 184\n",
      "Training loss (for one batch) at step 0: 0.2685\n",
      "Training loss (for one batch) at step 200: 0.2908\n",
      "Training loss (for one batch) at step 400: 0.2953\n",
      "Training loss (for one batch) at step 600: 0.2526\n",
      "Training epoch: 185, training rmse: 0.435570, vali rmse:0.530676\n",
      "\n",
      "Start of epoch 185\n",
      "Training loss (for one batch) at step 0: 0.2684\n",
      "Training loss (for one batch) at step 200: 0.2907\n",
      "Training loss (for one batch) at step 400: 0.2952\n",
      "Training loss (for one batch) at step 600: 0.2525\n",
      "Training epoch: 186, training rmse: 0.435428, vali rmse:0.530651\n",
      "\n",
      "Start of epoch 186\n",
      "Training loss (for one batch) at step 0: 0.2683\n",
      "Training loss (for one batch) at step 200: 0.2905\n",
      "Training loss (for one batch) at step 400: 0.2951\n",
      "Training loss (for one batch) at step 600: 0.2523\n",
      "Training epoch: 187, training rmse: 0.435287, vali rmse:0.530626\n",
      "\n",
      "Start of epoch 187\n",
      "Training loss (for one batch) at step 0: 0.2682\n",
      "Training loss (for one batch) at step 200: 0.2904\n",
      "Training loss (for one batch) at step 400: 0.2950\n",
      "Training loss (for one batch) at step 600: 0.2522\n",
      "Training epoch: 188, training rmse: 0.435147, vali rmse:0.530602\n",
      "\n",
      "Start of epoch 188\n",
      "Training loss (for one batch) at step 0: 0.2681\n",
      "Training loss (for one batch) at step 200: 0.2903\n",
      "Training loss (for one batch) at step 400: 0.2949\n",
      "Training loss (for one batch) at step 600: 0.2521\n",
      "Training epoch: 189, training rmse: 0.435009, vali rmse:0.530580\n",
      "\n",
      "Start of epoch 189\n",
      "Training loss (for one batch) at step 0: 0.2680\n",
      "Training loss (for one batch) at step 200: 0.2902\n",
      "Training loss (for one batch) at step 400: 0.2948\n",
      "Training loss (for one batch) at step 600: 0.2519\n",
      "Training epoch: 190, training rmse: 0.434872, vali rmse:0.530558\n",
      "\n",
      "Start of epoch 190\n",
      "Training loss (for one batch) at step 0: 0.2679\n",
      "Training loss (for one batch) at step 200: 0.2901\n",
      "Training loss (for one batch) at step 400: 0.2947\n",
      "Training loss (for one batch) at step 600: 0.2518\n",
      "Training epoch: 191, training rmse: 0.434736, vali rmse:0.530536\n",
      "\n",
      "Start of epoch 191\n",
      "Training loss (for one batch) at step 0: 0.2679\n",
      "Training loss (for one batch) at step 200: 0.2899\n",
      "Training loss (for one batch) at step 400: 0.2946\n",
      "Training loss (for one batch) at step 600: 0.2516\n",
      "Training epoch: 192, training rmse: 0.434602, vali rmse:0.530516\n",
      "\n",
      "Start of epoch 192\n",
      "Training loss (for one batch) at step 0: 0.2678\n",
      "Training loss (for one batch) at step 200: 0.2898\n",
      "Training loss (for one batch) at step 400: 0.2945\n",
      "Training loss (for one batch) at step 600: 0.2515\n",
      "Training epoch: 193, training rmse: 0.434468, vali rmse:0.530497\n",
      "\n",
      "Start of epoch 193\n",
      "Training loss (for one batch) at step 0: 0.2677\n",
      "Training loss (for one batch) at step 200: 0.2897\n",
      "Training loss (for one batch) at step 400: 0.2944\n",
      "Training loss (for one batch) at step 600: 0.2514\n",
      "Training epoch: 194, training rmse: 0.434336, vali rmse:0.530478\n",
      "\n",
      "Start of epoch 194\n",
      "Training loss (for one batch) at step 0: 0.2676\n",
      "Training loss (for one batch) at step 200: 0.2896\n",
      "Training loss (for one batch) at step 400: 0.2943\n",
      "Training loss (for one batch) at step 600: 0.2512\n",
      "Training epoch: 195, training rmse: 0.434205, vali rmse:0.530460\n",
      "\n",
      "Start of epoch 195\n",
      "Training loss (for one batch) at step 0: 0.2675\n",
      "Training loss (for one batch) at step 200: 0.2895\n",
      "Training loss (for one batch) at step 400: 0.2942\n",
      "Training loss (for one batch) at step 600: 0.2511\n",
      "Training epoch: 196, training rmse: 0.434075, vali rmse:0.530442\n",
      "\n",
      "Start of epoch 196\n",
      "Training loss (for one batch) at step 0: 0.2674\n",
      "Training loss (for one batch) at step 200: 0.2894\n",
      "Training loss (for one batch) at step 400: 0.2941\n",
      "Training loss (for one batch) at step 600: 0.2510\n",
      "Training epoch: 197, training rmse: 0.433945, vali rmse:0.530425\n",
      "\n",
      "Start of epoch 197\n",
      "Training loss (for one batch) at step 0: 0.2673\n",
      "Training loss (for one batch) at step 200: 0.2893\n",
      "Training loss (for one batch) at step 400: 0.2940\n",
      "Training loss (for one batch) at step 600: 0.2508\n",
      "Training epoch: 198, training rmse: 0.433817, vali rmse:0.530409\n",
      "\n",
      "Start of epoch 198\n",
      "Training loss (for one batch) at step 0: 0.2672\n",
      "Training loss (for one batch) at step 200: 0.2892\n",
      "Training loss (for one batch) at step 400: 0.2940\n",
      "Training loss (for one batch) at step 600: 0.2507\n",
      "Training epoch: 199, training rmse: 0.433690, vali rmse:0.530394\n",
      "\n",
      "Start of epoch 199\n",
      "Training loss (for one batch) at step 0: 0.2671\n",
      "Training loss (for one batch) at step 200: 0.2891\n",
      "Training loss (for one batch) at step 400: 0.2939\n",
      "Training loss (for one batch) at step 600: 0.2506\n",
      "Training epoch: 200, training rmse: 0.433563, vali rmse:0.530379\n",
      "\n",
      "Start of epoch 200\n",
      "Training loss (for one batch) at step 0: 0.2670\n",
      "Training loss (for one batch) at step 200: 0.2890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 400: 0.2938\n",
      "Training loss (for one batch) at step 600: 0.2505\n",
      "Training epoch: 201, training rmse: 0.433438, vali rmse:0.530365\n",
      "\n",
      "Start of epoch 201\n",
      "Training loss (for one batch) at step 0: 0.2670\n",
      "Training loss (for one batch) at step 200: 0.2889\n",
      "Training loss (for one batch) at step 400: 0.2937\n",
      "Training loss (for one batch) at step 600: 0.2503\n",
      "Training epoch: 202, training rmse: 0.433313, vali rmse:0.530351\n",
      "\n",
      "Start of epoch 202\n",
      "Training loss (for one batch) at step 0: 0.2669\n",
      "Training loss (for one batch) at step 200: 0.2888\n",
      "Training loss (for one batch) at step 400: 0.2936\n",
      "Training loss (for one batch) at step 600: 0.2502\n",
      "Training epoch: 203, training rmse: 0.433189, vali rmse:0.530338\n",
      "\n",
      "Start of epoch 203\n",
      "Training loss (for one batch) at step 0: 0.2668\n",
      "Training loss (for one batch) at step 200: 0.2887\n",
      "Training loss (for one batch) at step 400: 0.2935\n",
      "Training loss (for one batch) at step 600: 0.2501\n",
      "Training epoch: 204, training rmse: 0.433066, vali rmse:0.530325\n",
      "\n",
      "Start of epoch 204\n",
      "Training loss (for one batch) at step 0: 0.2667\n",
      "Training loss (for one batch) at step 200: 0.2886\n",
      "Training loss (for one batch) at step 400: 0.2934\n",
      "Training loss (for one batch) at step 600: 0.2500\n",
      "Training epoch: 205, training rmse: 0.432943, vali rmse:0.530313\n",
      "\n",
      "Start of epoch 205\n",
      "Training loss (for one batch) at step 0: 0.2666\n",
      "Training loss (for one batch) at step 200: 0.2885\n",
      "Training loss (for one batch) at step 400: 0.2933\n",
      "Training loss (for one batch) at step 600: 0.2499\n",
      "Training epoch: 206, training rmse: 0.432821, vali rmse:0.530301\n",
      "\n",
      "Start of epoch 206\n",
      "Training loss (for one batch) at step 0: 0.2665\n",
      "Training loss (for one batch) at step 200: 0.2884\n",
      "Training loss (for one batch) at step 400: 0.2932\n",
      "Training loss (for one batch) at step 600: 0.2497\n",
      "Training epoch: 207, training rmse: 0.432700, vali rmse:0.530290\n",
      "\n",
      "Start of epoch 207\n",
      "Training loss (for one batch) at step 0: 0.2665\n",
      "Training loss (for one batch) at step 200: 0.2883\n",
      "Training loss (for one batch) at step 400: 0.2931\n",
      "Training loss (for one batch) at step 600: 0.2496\n",
      "Training epoch: 208, training rmse: 0.432580, vali rmse:0.530279\n",
      "\n",
      "Start of epoch 208\n",
      "Training loss (for one batch) at step 0: 0.2664\n",
      "Training loss (for one batch) at step 200: 0.2882\n",
      "Training loss (for one batch) at step 400: 0.2930\n",
      "Training loss (for one batch) at step 600: 0.2495\n",
      "Training epoch: 209, training rmse: 0.432460, vali rmse:0.530269\n",
      "\n",
      "Start of epoch 209\n",
      "Training loss (for one batch) at step 0: 0.2663\n",
      "Training loss (for one batch) at step 200: 0.2881\n",
      "Training loss (for one batch) at step 400: 0.2930\n",
      "Training loss (for one batch) at step 600: 0.2494\n",
      "Training epoch: 210, training rmse: 0.432340, vali rmse:0.530259\n",
      "\n",
      "Start of epoch 210\n",
      "Training loss (for one batch) at step 0: 0.2662\n",
      "Training loss (for one batch) at step 200: 0.2880\n",
      "Training loss (for one batch) at step 400: 0.2929\n",
      "Training loss (for one batch) at step 600: 0.2493\n",
      "Training epoch: 211, training rmse: 0.432222, vali rmse:0.530250\n",
      "\n",
      "Start of epoch 211\n",
      "Training loss (for one batch) at step 0: 0.2661\n",
      "Training loss (for one batch) at step 200: 0.2879\n",
      "Training loss (for one batch) at step 400: 0.2928\n",
      "Training loss (for one batch) at step 600: 0.2492\n",
      "Training epoch: 212, training rmse: 0.432103, vali rmse:0.530241\n",
      "\n",
      "Start of epoch 212\n",
      "Training loss (for one batch) at step 0: 0.2660\n",
      "Training loss (for one batch) at step 200: 0.2878\n",
      "Training loss (for one batch) at step 400: 0.2927\n",
      "Training loss (for one batch) at step 600: 0.2490\n",
      "Training epoch: 213, training rmse: 0.431986, vali rmse:0.530233\n",
      "\n",
      "Start of epoch 213\n",
      "Training loss (for one batch) at step 0: 0.2660\n",
      "Training loss (for one batch) at step 200: 0.2877\n",
      "Training loss (for one batch) at step 400: 0.2926\n",
      "Training loss (for one batch) at step 600: 0.2489\n",
      "Training epoch: 214, training rmse: 0.431868, vali rmse:0.530224\n",
      "\n",
      "Start of epoch 214\n",
      "Training loss (for one batch) at step 0: 0.2659\n",
      "Training loss (for one batch) at step 200: 0.2876\n",
      "Training loss (for one batch) at step 400: 0.2925\n",
      "Training loss (for one batch) at step 600: 0.2488\n",
      "Training epoch: 215, training rmse: 0.431752, vali rmse:0.530217\n",
      "\n",
      "Start of epoch 215\n",
      "Training loss (for one batch) at step 0: 0.2658\n",
      "Training loss (for one batch) at step 200: 0.2875\n",
      "Training loss (for one batch) at step 400: 0.2924\n",
      "Training loss (for one batch) at step 600: 0.2487\n",
      "Training epoch: 216, training rmse: 0.431635, vali rmse:0.530209\n",
      "\n",
      "Start of epoch 216\n",
      "Training loss (for one batch) at step 0: 0.2657\n",
      "Training loss (for one batch) at step 200: 0.2874\n",
      "Training loss (for one batch) at step 400: 0.2924\n",
      "Training loss (for one batch) at step 600: 0.2486\n",
      "Training epoch: 217, training rmse: 0.431519, vali rmse:0.530202\n",
      "\n",
      "Start of epoch 217\n",
      "Training loss (for one batch) at step 0: 0.2656\n",
      "Training loss (for one batch) at step 200: 0.2873\n",
      "Training loss (for one batch) at step 400: 0.2923\n",
      "Training loss (for one batch) at step 600: 0.2485\n",
      "Training epoch: 218, training rmse: 0.431404, vali rmse:0.530195\n",
      "\n",
      "Start of epoch 218\n",
      "Training loss (for one batch) at step 0: 0.2656\n",
      "Training loss (for one batch) at step 200: 0.2873\n",
      "Training loss (for one batch) at step 400: 0.2922\n",
      "Training loss (for one batch) at step 600: 0.2484\n",
      "Training epoch: 219, training rmse: 0.431289, vali rmse:0.530189\n",
      "\n",
      "Start of epoch 219\n",
      "Training loss (for one batch) at step 0: 0.2655\n",
      "Training loss (for one batch) at step 200: 0.2872\n",
      "Training loss (for one batch) at step 400: 0.2921\n",
      "Training loss (for one batch) at step 600: 0.2483\n",
      "Training epoch: 220, training rmse: 0.431174, vali rmse:0.530182\n",
      "\n",
      "Start of epoch 220\n",
      "Training loss (for one batch) at step 0: 0.2654\n",
      "Training loss (for one batch) at step 200: 0.2871\n",
      "Training loss (for one batch) at step 400: 0.2920\n",
      "Training loss (for one batch) at step 600: 0.2482\n",
      "Training epoch: 221, training rmse: 0.431059, vali rmse:0.530177\n",
      "\n",
      "Start of epoch 221\n",
      "Training loss (for one batch) at step 0: 0.2653\n",
      "Training loss (for one batch) at step 200: 0.2870\n",
      "Training loss (for one batch) at step 400: 0.2919\n",
      "Training loss (for one batch) at step 600: 0.2481\n",
      "Training epoch: 222, training rmse: 0.430945, vali rmse:0.530171\n",
      "\n",
      "Start of epoch 222\n",
      "Training loss (for one batch) at step 0: 0.2652\n",
      "Training loss (for one batch) at step 200: 0.2869\n",
      "Training loss (for one batch) at step 400: 0.2918\n",
      "Training loss (for one batch) at step 600: 0.2479\n",
      "Training epoch: 223, training rmse: 0.430831, vali rmse:0.530166\n",
      "\n",
      "Start of epoch 223\n",
      "Training loss (for one batch) at step 0: 0.2652\n",
      "Training loss (for one batch) at step 200: 0.2868\n",
      "Training loss (for one batch) at step 400: 0.2918\n",
      "Training loss (for one batch) at step 600: 0.2478\n",
      "Training epoch: 224, training rmse: 0.430717, vali rmse:0.530161\n",
      "\n",
      "Start of epoch 224\n",
      "Training loss (for one batch) at step 0: 0.2651\n",
      "Training loss (for one batch) at step 200: 0.2867\n",
      "Training loss (for one batch) at step 400: 0.2917\n",
      "Training loss (for one batch) at step 600: 0.2477\n",
      "Training epoch: 225, training rmse: 0.430604, vali rmse:0.530156\n",
      "\n",
      "Start of epoch 225\n",
      "Training loss (for one batch) at step 0: 0.2650\n",
      "Training loss (for one batch) at step 200: 0.2867\n",
      "Training loss (for one batch) at step 400: 0.2916\n",
      "Training loss (for one batch) at step 600: 0.2476\n",
      "Training epoch: 226, training rmse: 0.430491, vali rmse:0.530151\n",
      "\n",
      "Start of epoch 226\n",
      "Training loss (for one batch) at step 0: 0.2649\n",
      "Training loss (for one batch) at step 200: 0.2866\n",
      "Training loss (for one batch) at step 400: 0.2915\n",
      "Training loss (for one batch) at step 600: 0.2475\n",
      "Training epoch: 227, training rmse: 0.430378, vali rmse:0.530147\n",
      "\n",
      "Start of epoch 227\n",
      "Training loss (for one batch) at step 0: 0.2649\n",
      "Training loss (for one batch) at step 200: 0.2865\n",
      "Training loss (for one batch) at step 400: 0.2914\n",
      "Training loss (for one batch) at step 600: 0.2474\n",
      "Training epoch: 228, training rmse: 0.430265, vali rmse:0.530143\n",
      "\n",
      "Start of epoch 228\n",
      "Training loss (for one batch) at step 0: 0.2648\n",
      "Training loss (for one batch) at step 200: 0.2864\n",
      "Training loss (for one batch) at step 400: 0.2913\n",
      "Training loss (for one batch) at step 600: 0.2473\n",
      "Training epoch: 229, training rmse: 0.430153, vali rmse:0.530139\n",
      "\n",
      "Start of epoch 229\n",
      "Training loss (for one batch) at step 0: 0.2647\n",
      "Training loss (for one batch) at step 200: 0.2863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 400: 0.2912\n",
      "Training loss (for one batch) at step 600: 0.2472\n",
      "Training epoch: 230, training rmse: 0.430040, vali rmse:0.530135\n",
      "\n",
      "Start of epoch 230\n",
      "Training loss (for one batch) at step 0: 0.2646\n",
      "Training loss (for one batch) at step 200: 0.2862\n",
      "Training loss (for one batch) at step 400: 0.2912\n",
      "Training loss (for one batch) at step 600: 0.2471\n",
      "Training epoch: 231, training rmse: 0.429928, vali rmse:0.530132\n",
      "\n",
      "Start of epoch 231\n",
      "Training loss (for one batch) at step 0: 0.2645\n",
      "Training loss (for one batch) at step 200: 0.2862\n",
      "Training loss (for one batch) at step 400: 0.2911\n",
      "Training loss (for one batch) at step 600: 0.2470\n",
      "Training epoch: 232, training rmse: 0.429815, vali rmse:0.530129\n",
      "\n",
      "Start of epoch 232\n",
      "Training loss (for one batch) at step 0: 0.2645\n",
      "Training loss (for one batch) at step 200: 0.2861\n",
      "Training loss (for one batch) at step 400: 0.2910\n",
      "Training loss (for one batch) at step 600: 0.2469\n",
      "Training epoch: 233, training rmse: 0.429703, vali rmse:0.530126\n",
      "\n",
      "Start of epoch 233\n",
      "Training loss (for one batch) at step 0: 0.2644\n",
      "Training loss (for one batch) at step 200: 0.2860\n",
      "Training loss (for one batch) at step 400: 0.2909\n",
      "Training loss (for one batch) at step 600: 0.2468\n",
      "Training epoch: 234, training rmse: 0.429591, vali rmse:0.530123\n",
      "\n",
      "Start of epoch 234\n",
      "Training loss (for one batch) at step 0: 0.2643\n",
      "Training loss (for one batch) at step 200: 0.2859\n",
      "Training loss (for one batch) at step 400: 0.2908\n",
      "Training loss (for one batch) at step 600: 0.2467\n",
      "Training epoch: 235, training rmse: 0.429479, vali rmse:0.530120\n",
      "\n",
      "Start of epoch 235\n",
      "Training loss (for one batch) at step 0: 0.2642\n",
      "Training loss (for one batch) at step 200: 0.2858\n",
      "Training loss (for one batch) at step 400: 0.2907\n",
      "Training loss (for one batch) at step 600: 0.2466\n",
      "Training epoch: 236, training rmse: 0.429368, vali rmse:0.530118\n",
      "\n",
      "Start of epoch 236\n",
      "Training loss (for one batch) at step 0: 0.2642\n",
      "Training loss (for one batch) at step 200: 0.2858\n",
      "Training loss (for one batch) at step 400: 0.2907\n",
      "Training loss (for one batch) at step 600: 0.2465\n",
      "Training epoch: 237, training rmse: 0.429256, vali rmse:0.530115\n",
      "\n",
      "Start of epoch 237\n",
      "Training loss (for one batch) at step 0: 0.2641\n",
      "Training loss (for one batch) at step 200: 0.2857\n",
      "Training loss (for one batch) at step 400: 0.2906\n",
      "Training loss (for one batch) at step 600: 0.2464\n",
      "Training epoch: 238, training rmse: 0.429144, vali rmse:0.530113\n",
      "\n",
      "Start of epoch 238\n",
      "Training loss (for one batch) at step 0: 0.2640\n",
      "Training loss (for one batch) at step 200: 0.2856\n",
      "Training loss (for one batch) at step 400: 0.2905\n",
      "Training loss (for one batch) at step 600: 0.2463\n",
      "Training epoch: 239, training rmse: 0.429032, vali rmse:0.530111\n",
      "\n",
      "Start of epoch 239\n",
      "Training loss (for one batch) at step 0: 0.2639\n",
      "Training loss (for one batch) at step 200: 0.2855\n",
      "Training loss (for one batch) at step 400: 0.2904\n",
      "Training loss (for one batch) at step 600: 0.2462\n",
      "Training epoch: 240, training rmse: 0.428920, vali rmse:0.530109\n",
      "\n",
      "Start of epoch 240\n",
      "Training loss (for one batch) at step 0: 0.2638\n",
      "Training loss (for one batch) at step 200: 0.2854\n",
      "Training loss (for one batch) at step 400: 0.2903\n",
      "Training loss (for one batch) at step 600: 0.2461\n",
      "Training epoch: 241, training rmse: 0.428808, vali rmse:0.530108\n",
      "\n",
      "Start of epoch 241\n",
      "Training loss (for one batch) at step 0: 0.2638\n",
      "Training loss (for one batch) at step 200: 0.2854\n",
      "Training loss (for one batch) at step 400: 0.2902\n",
      "Training loss (for one batch) at step 600: 0.2460\n",
      "Training epoch: 242, training rmse: 0.428697, vali rmse:0.530106\n",
      "\n",
      "Start of epoch 242\n",
      "Training loss (for one batch) at step 0: 0.2637\n",
      "Training loss (for one batch) at step 200: 0.2853\n",
      "Training loss (for one batch) at step 400: 0.2901\n",
      "Training loss (for one batch) at step 600: 0.2459\n",
      "Training epoch: 243, training rmse: 0.428585, vali rmse:0.530105\n",
      "\n",
      "Start of epoch 243\n",
      "Training loss (for one batch) at step 0: 0.2636\n",
      "Training loss (for one batch) at step 200: 0.2852\n",
      "Training loss (for one batch) at step 400: 0.2901\n",
      "Training loss (for one batch) at step 600: 0.2458\n",
      "Training epoch: 244, training rmse: 0.428473, vali rmse:0.530103\n",
      "\n",
      "Start of epoch 244\n",
      "Training loss (for one batch) at step 0: 0.2635\n",
      "Training loss (for one batch) at step 200: 0.2851\n",
      "Training loss (for one batch) at step 400: 0.2900\n",
      "Training loss (for one batch) at step 600: 0.2457\n",
      "Training epoch: 245, training rmse: 0.428361, vali rmse:0.530102\n",
      "\n",
      "Start of epoch 245\n",
      "Training loss (for one batch) at step 0: 0.2634\n",
      "Training loss (for one batch) at step 200: 0.2850\n",
      "Training loss (for one batch) at step 400: 0.2899\n",
      "Training loss (for one batch) at step 600: 0.2456\n",
      "Training epoch: 246, training rmse: 0.428249, vali rmse:0.530101\n",
      "\n",
      "Start of epoch 246\n",
      "Training loss (for one batch) at step 0: 0.2634\n",
      "Training loss (for one batch) at step 200: 0.2850\n",
      "Training loss (for one batch) at step 400: 0.2898\n",
      "Training loss (for one batch) at step 600: 0.2455\n",
      "Training epoch: 247, training rmse: 0.428136, vali rmse:0.530100\n",
      "\n",
      "Start of epoch 247\n",
      "Training loss (for one batch) at step 0: 0.2633\n",
      "Training loss (for one batch) at step 200: 0.2849\n",
      "Training loss (for one batch) at step 400: 0.2897\n",
      "Training loss (for one batch) at step 600: 0.2454\n",
      "Training epoch: 248, training rmse: 0.428024, vali rmse:0.530099\n",
      "\n",
      "Start of epoch 248\n",
      "Training loss (for one batch) at step 0: 0.2632\n",
      "Training loss (for one batch) at step 200: 0.2848\n",
      "Training loss (for one batch) at step 400: 0.2896\n",
      "Training loss (for one batch) at step 600: 0.2453\n",
      "Training epoch: 249, training rmse: 0.427912, vali rmse:0.530099\n",
      "\n",
      "Start of epoch 249\n",
      "Training loss (for one batch) at step 0: 0.2631\n",
      "Training loss (for one batch) at step 200: 0.2847\n",
      "Training loss (for one batch) at step 400: 0.2895\n",
      "Training loss (for one batch) at step 600: 0.2452\n",
      "Training epoch: 250, training rmse: 0.427799, vali rmse:0.530098\n",
      "\n",
      "Start of epoch 250\n",
      "Training loss (for one batch) at step 0: 0.2631\n",
      "Training loss (for one batch) at step 200: 0.2846\n",
      "Training loss (for one batch) at step 400: 0.2895\n",
      "Training loss (for one batch) at step 600: 0.2451\n",
      "Training epoch: 251, training rmse: 0.427686, vali rmse:0.530097\n",
      "\n",
      "Start of epoch 251\n",
      "Training loss (for one batch) at step 0: 0.2630\n",
      "Training loss (for one batch) at step 200: 0.2846\n",
      "Training loss (for one batch) at step 400: 0.2894\n",
      "Training loss (for one batch) at step 600: 0.2450\n",
      "Training epoch: 252, training rmse: 0.427573, vali rmse:0.530097\n",
      "\n",
      "Start of epoch 252\n",
      "Training loss (for one batch) at step 0: 0.2629\n",
      "Training loss (for one batch) at step 200: 0.2845\n",
      "Training loss (for one batch) at step 400: 0.2893\n",
      "Training loss (for one batch) at step 600: 0.2449\n",
      "Training epoch: 253, training rmse: 0.427460, vali rmse:0.530097\n",
      "\n",
      "Start of epoch 253\n",
      "Training loss (for one batch) at step 0: 0.2628\n",
      "Training loss (for one batch) at step 200: 0.2844\n",
      "Training loss (for one batch) at step 400: 0.2892\n",
      "Training loss (for one batch) at step 600: 0.2448\n",
      "Training epoch: 254, training rmse: 0.427347, vali rmse:0.530096\n",
      "\n",
      "Start of epoch 254\n",
      "Training loss (for one batch) at step 0: 0.2627\n",
      "Training loss (for one batch) at step 200: 0.2843\n",
      "Training loss (for one batch) at step 400: 0.2891\n",
      "Training loss (for one batch) at step 600: 0.2447\n",
      "Training epoch: 255, training rmse: 0.427234, vali rmse:0.530096\n",
      "\n",
      "Start of epoch 255\n",
      "Training loss (for one batch) at step 0: 0.2627\n",
      "Training loss (for one batch) at step 200: 0.2843\n",
      "Training loss (for one batch) at step 400: 0.2890\n",
      "Training loss (for one batch) at step 600: 0.2446\n",
      "Training epoch: 256, training rmse: 0.427120, vali rmse:0.530096\n",
      "\n",
      "Start of epoch 256\n",
      "Training loss (for one batch) at step 0: 0.2626\n",
      "Training loss (for one batch) at step 200: 0.2842\n",
      "Training loss (for one batch) at step 400: 0.2889\n",
      "Training loss (for one batch) at step 600: 0.2445\n",
      "Training epoch: 257, training rmse: 0.427006, vali rmse:0.530096\n",
      "\n",
      "Start of epoch 257\n",
      "Training loss (for one batch) at step 0: 0.2625\n",
      "Training loss (for one batch) at step 200: 0.2841\n",
      "Training loss (for one batch) at step 400: 0.2888\n",
      "Training loss (for one batch) at step 600: 0.2444\n",
      "Training epoch: 258, training rmse: 0.426892, vali rmse:0.530096\n",
      "===== Testing Model =====\n",
      "Test rmse: 0.525973\n"
     ]
    }
   ],
   "source": [
    "print('===== TRAINING PMF =====')\n",
    "## initializing hyperparameters\n",
    "batch_size = 64\n",
    "epoches = 1000\n",
    "seed = 1\n",
    "weight_decay = 0.1\n",
    "emb_dim = 100\n",
    "ratio = 0.8\n",
    "test_ratio = 0.5\n",
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "\n",
    "## loading dataset\n",
    "users = pickle.load(open('dataset_RL/user_id_to_num.pkl', 'rb'))\n",
    "items = pickle.load(open('dataset_RL/item_id_to_num.pkl', 'rb'))\n",
    "# data = np.load('dataset_RL/data_RL_25000.npy')\n",
    "train_data = np.load('dataset_RL/train_data_RL_75000.npy')\n",
    "test_data = np.load('dataset_RL/test_data_RL_75000.npy')\n",
    "\n",
    "print(\"===== Dataset has been loaded =====\")\n",
    "\n",
    "## As the paper implemented, normalizing the rating\n",
    "## that will act as reward\n",
    "train_data[:, 2] = 0.5 * (train_data[:, 2] - 3)\n",
    "test_data[:, 2] = 0.5 * (test_data[:, 2] - 3)\n",
    "\n",
    "## Shuffle data\n",
    "np.random.shuffle(train_data)\n",
    "np.random.shuffle(test_data)\n",
    "\n",
    "## Splitting data data\n",
    "# train_data = data[:int(ratio * data.shape[0])]\n",
    "# vali_data = data[int(ratio * data.shape[0]):int((ratio + (1 - ratio) / 2) * data.shape[0])]\n",
    "# test_data = data[int((ratio + ( 1 - ratio ) / 2) * data.shape[0]):]\n",
    "vali_data = test_data[:int(test_ratio * test_data.shape[0])]\n",
    "test_data = test_data[int(test_ratio * test_data.shape[0]):]\n",
    "\n",
    " \n",
    "## Extract number of users and items\n",
    "NUM_USERS = len(users)\n",
    "NUM_ITEMS = len(items)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(batch_size)\n",
    "print(\"===== Preprocess the data has been finished =====\")\n",
    "\n",
    "model = PMF(NUM_USERS, NUM_ITEMS, emb_dim)\n",
    "model(1, 1)\n",
    "\n",
    "print(\"===== Model Instantiated =====\")\n",
    "model.summary()\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def train(train_dataset, len_dataset):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for step, elem in enumerate(train_dataset):\n",
    "        row = elem[:, 0] ## users as a row\n",
    "        col = elem[:, 1] ## items as a column\n",
    "        val = elem[:, 2] ## normalized ratings as value\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            ## Perlu diperiksa kembali\n",
    "            row = tf.Variable(row, trainable=False)\n",
    "            col = tf.Variable(col, trainable=False)\n",
    "            val = tf.Variable(val, trainable=False)\n",
    "            \n",
    "            ## run the forward pass\n",
    "            logits = model(row, col, training=True)\n",
    "            \n",
    "            ## compute the loss value\n",
    "            loss_value = loss_fn(val, logits)\n",
    "            \n",
    "        ## using gradien tape to automatically retrieves\n",
    "        ## the gradients of the trainable variables with respect to loss\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        \n",
    "        ## run one step of gradient descent by updating\n",
    "        ## the value of variable loss\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        epoch_loss += loss_value\n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "    \n",
    "    return float(loss_value)\n",
    "    \n",
    "print(\"===== Training Model =====\")\n",
    "## this list is used for visualization\n",
    "train_loss_list = []\n",
    "train_rmse_list = []\n",
    "vali_rmse_list = []\n",
    "len_train_data = len(train_data)\n",
    "\n",
    "last_vali_rmse = None\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    ## train epoch losses\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    \n",
    "    train_epoch_loss = train(train_dataset, len_train_data)\n",
    "    \n",
    "    ## test \n",
    "    train_loss_list.append(train_epoch_loss)\n",
    "    \n",
    "    ## creating index for predicting\n",
    "    vali_row = tf.Variable(tf.convert_to_tensor(vali_data[:, 0]), trainable=False)\n",
    "    vali_col = tf.Variable(tf.convert_to_tensor(vali_data[:, 1]), trainable=False)\n",
    "    \n",
    "    ## predicting the value\n",
    "    vali_preds = model(vali_row, vali_col, training=False)\n",
    "    \n",
    "    ## calculating rmse\n",
    "    train_rmse = np.sqrt(train_epoch_loss)\n",
    "    vali_rmse = np.sqrt(np.square(np.subtract(vali_data[:, 2], vali_preds)).mean())\n",
    "    \n",
    "    train_rmse_list.append(train_rmse)\n",
    "    vali_rmse_list.append(vali_rmse)\n",
    "    \n",
    "    print('Training epoch:{: d}, training rmse:{: .6f}, vali rmse:{:.6f}'.format(epoch+1, train_rmse, vali_rmse))\n",
    "    \n",
    "    if last_vali_rmse and last_vali_rmse < vali_rmse:\n",
    "        break\n",
    "    else:\n",
    "        last_vali_rmse = vali_rmse\n",
    "\n",
    "print(\"===== Testing Model =====\")\n",
    "test_row = tf.Variable(tf.convert_to_tensor(test_data[:, 0]), trainable=False)\n",
    "test_col = tf.Variable(tf.convert_to_tensor(test_data[:, 1]), trainable=False)\n",
    "\n",
    "preds = model(test_row, test_col, training=False)\n",
    "\n",
    "test_rmse = np.sqrt(np.square(np.subtract(test_data[:, 2], preds)).mean())\n",
    "print('Test rmse: {:f}'.format(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9133456b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Testing Model =====\n",
      "Test rmse: 0.505202\n"
     ]
    }
   ],
   "source": [
    "print(\"===== Testing Model =====\")\n",
    "test_row = tf.convert_to_tensor(test_data[:, 0])\n",
    "test_col = tf.convert_to_tensor(test_data[:, 1])\n",
    "\n",
    "preds = model(test_row, test_col, training=False)\n",
    "\n",
    "test_rmse = np.sqrt(np.square(np.subtract(test_data[:, 2], preds)).mean())\n",
    "print('Test rmse: {:f}'.format(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "133f7d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('trained/pmf_weights/pmf_oversampling_undersampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7552b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x23cfb6369a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('trained/pmf_weights/pmf_150')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43c88825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create plots\n",
    "plt.figure(1)\n",
    "plt.plot(range(1, len(train_rmse_list)+1), train_rmse_list, color='r', label='train rmse')\n",
    "plt.plot(range(1, len(vali_rmse_list)+1), vali_rmse_list, color='b', label='test rmse')\n",
    "plt.legend()\n",
    "plt.annotate(r'train=%f' % (train_rmse_list[-1]), xy=(len(train_rmse_list), train_rmse_list[-1]),\n",
    "             xycoords='data', xytext=(-30, 30), textcoords='offset points', fontsize=10,\n",
    "             arrowprops=dict(arrowstyle='->', connectionstyle='arc3, rad=.2'))\n",
    "plt.annotate(r'vali=%f' % (vali_rmse_list[-1]), xy=(len(vali_rmse_list), vali_rmse_list[-1]),\n",
    "             xycoords='data', xytext=(-30, 30), textcoords='offset points', fontsize=10,\n",
    "             arrowprops=dict(arrowstyle='->', connectionstyle='arc3, rad=.2'))\n",
    "plt.xlim([1, len(train_rmse_list)+10])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE Curve in Training Process')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
