{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a928ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c459a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMF(tf.keras.Model):\n",
    "    def __init__(self, n_users, n_items, n_dim):\n",
    "        super(PMF, self).__init__()\n",
    "        ## initializing attributes from parameters        \n",
    "        self.w_u_i_init = tf.keras.initializers.RandomUniform(minval=-.01, maxval=.01, seed=1)\n",
    "        \n",
    "        ## initializing Embedding layers\n",
    "        self.user_embedding = tf.keras.layers.Embedding(n_users,\n",
    "                                                        n_dim,\n",
    "                                                        embeddings_initializer='uniform',\n",
    "                                                        embeddings_regularizer=tf.keras.regularizers.L2(0.1))\n",
    "        \n",
    "        self.item_embedding = tf.keras.layers.Embedding(n_items,\n",
    "                                                        n_dim,\n",
    "                                                        embeddings_initializer='uniform',\n",
    "                                                        embeddings_regularizer=tf.keras.regularizers.L2(0.1))\n",
    "        \n",
    "        self.ub = tf.keras.layers.Embedding(n_users, 1, embeddings_initializer=self.w_u_i_init, embeddings_regularizer=tf.keras.regularizers.L2(0.1))\n",
    "        self.ib = tf.keras.layers.Embedding(n_items, 1, embeddings_initializer=self.w_u_i_init, embeddings_regularizer=tf.keras.regularizers.L2(0.1))\n",
    "        \n",
    "    def call(self, user_index, item_index):\n",
    "        user_h1 = self.user_embedding(user_index)\n",
    "        item_h1 = self.item_embedding(item_index)\n",
    "        \n",
    "        r_h = tf.math.reduce_sum(user_h1 * item_h1) + tf.squeeze(self.ub(user_index)) + tf.squeeze(self.ib(item_index))\n",
    "        return r_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41ed32de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== TRAINING PMF =====\n",
      "===== Dataset has been loaded =====\n",
      "===== Preprocess the data has been finished =====\n",
      "===== Model Instantiated =====\n",
      "===== Training Model =====\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 0.4496\n",
      "Training loss (for one batch) at step 200: 0.3219\n",
      "Training loss (for one batch) at step 400: 0.3509\n",
      "Training loss (for one batch) at step 600: 0.3678\n",
      "Training loss (for one batch) at step 800: 0.3799\n",
      "Training loss (for one batch) at step 1000: 0.3626\n",
      "Training loss (for one batch) at step 1200: 0.4037\n",
      "Training epoch: 1, training rmse: 0.077033, vali rmse:0.607329\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.3879\n",
      "Training loss (for one batch) at step 200: 0.3043\n",
      "Training loss (for one batch) at step 400: 0.2921\n",
      "Training loss (for one batch) at step 600: 0.3101\n",
      "Training loss (for one batch) at step 800: 0.3336\n",
      "Training loss (for one batch) at step 1000: 0.3172\n",
      "Training loss (for one batch) at step 1200: 0.3275\n",
      "Training epoch: 2, training rmse: 0.071946, vali rmse:2.678053\n",
      "===== Testing Model =====\n",
      "Test rmse: 2.527262\n"
     ]
    }
   ],
   "source": [
    "print('===== TRAINING PMF =====')\n",
    "## initializing hyperparameters\n",
    "batch_size = 64\n",
    "epoches = 150\n",
    "seed = 1\n",
    "weight_decay = 0.1\n",
    "emb_dim = 100\n",
    "ratio = 0.8\n",
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "\n",
    "## loading dataset\n",
    "users = pickle.load(open('dataset/user_id_to_num_mov.pkl', 'rb'))\n",
    "items = pickle.load(open('dataset/movie_id_to_num_mov.pkl', 'rb'))\n",
    "data = np.load('dataset/data.npy')\n",
    "\n",
    "print(\"===== Dataset has been loaded =====\")\n",
    "\n",
    "## As the paper implemented, normalizing the rating\n",
    "## that will act as reward\n",
    "data[:,2] = 0.5 * (data[:, 2] - 3)\n",
    "\n",
    "## Shuffle data\n",
    "rndg = np.random.default_rng(seed=42)\n",
    "rndg.shuffle(data)\n",
    " \n",
    "## Splitting data data\n",
    "train_data = data[:int(ratio * data.shape[0])]\n",
    "vali_data = data[int(ratio * data.shape[0]):int((ratio + (1 - ratio) / 2) * data.shape[0])]\n",
    "test_data = data[int((ratio + ( 1 - ratio ) / 2) * data.shape[0]):]\n",
    " \n",
    "## Extract number of users and items\n",
    "NUM_USERS = len(users)\n",
    "NUM_ITEMS = len(items)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(batch_size)\n",
    "print(\"===== Preprocess the data has been finished =====\")\n",
    "\n",
    "model = PMF(NUM_USERS, NUM_ITEMS, emb_dim)\n",
    "\n",
    "print(\"===== Model Instantiated =====\")\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def train(train_dataset, len_dataset):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for step, elem in enumerate(train_dataset):\n",
    "        row = elem[:, 0] ## users as a row\n",
    "        col = elem[:, 1] ## items as a column\n",
    "        val = elem[:, 2] ## normalized ratings as value\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            ## Perlu diperiksa kembali\n",
    "            row = tf.Variable(row, trainable=False)\n",
    "            col = tf.Variable(col, trainable=False)\n",
    "            val = tf.Variable(val, trainable=False)\n",
    "            \n",
    "            ## run the forward pass\n",
    "            logits = model(row, col, training=True)\n",
    "            \n",
    "            ## compute the loss value\n",
    "            loss_value = loss_fn(val, logits)\n",
    "            \n",
    "        ## using gradien tape to automatically retrieves\n",
    "        ## the gradients of the trainable variables with respect to loss\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        \n",
    "        ## run one step of gradient descent by updating\n",
    "        ## the value of variable loss\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        epoch_loss += loss_value\n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "    \n",
    "    epoch_loss /= len_dataset\n",
    "    return float(epoch_loss)\n",
    "    \n",
    "print(\"===== Training Model =====\")\n",
    "## this list is used for visualization\n",
    "train_loss_list = []\n",
    "train_rmse_list = []\n",
    "vali_rmse_list = []\n",
    "len_train_data = len(train_data)\n",
    "\n",
    "last_vali_rmse = None\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    ## train epoch losses\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    \n",
    "    train_epoch_loss = train(train_dataset, len_train_data)\n",
    "    \n",
    "    ## test \n",
    "    train_loss_list.append(train_epoch_loss)\n",
    "    \n",
    "    ## creating index for predicting\n",
    "    vali_row = tf.Variable(tf.convert_to_tensor(vali_data[:, 0]), trainable=False)\n",
    "    vali_col = tf.Variable(tf.convert_to_tensor(vali_data[:, 1]), trainable=False)\n",
    "    \n",
    "    ## predicting the value\n",
    "    vali_preds = model(vali_row, vali_col, training=False)\n",
    "    \n",
    "    ## calculating rmse\n",
    "    train_rmse = np.sqrt(train_epoch_loss)\n",
    "    vali_rmse = np.sqrt(np.mean(np.square(vali_preds-vali_data[:, 2])))\n",
    "    \n",
    "    train_rmse_list.append(train_rmse)\n",
    "    vali_rmse_list.append(vali_rmse)\n",
    "    \n",
    "    print('Training epoch:{: d}, training rmse:{: .6f}, vali rmse:{:.6f}'.format(epoch+1, train_rmse, vali_rmse))\n",
    "    \n",
    "    if last_vali_rmse and last_vali_rmse < vali_rmse:\n",
    "        break\n",
    "    else:\n",
    "        last_vali_rmse = vali_rmse\n",
    "\n",
    "print(\"===== Testing Model =====\")\n",
    "test_row = tf.Variable(tf.convert_to_tensor(test_data[:, 0]), trainable=False)\n",
    "test_col = tf.Variable(tf.convert_to_tensor(test_data[:, 1]), trainable=False)\n",
    "\n",
    "preds = model(test_row, test_col, training=False)\n",
    "\n",
    "test_rmse = np.sqrt(np.mean(np.square(preds-test_data[:, 2])))\n",
    "print('Test rmse: {:f}'.format(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c88825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create plots\n",
    "plt.figure(1)\n",
    "plt.plot(range(1, len(train_rmse_list)+1), train_rmse_list, color='r', label='train rmse')\n",
    "plt.plot(range(1, len(vali_rmse_list)+1), vali_rmse_list, color='b', label='test rmse')\n",
    "plt.legend()\n",
    "plt.annotate(r'train=%f' % (train_rmse_list[-1]), xy=(len(train_rmse_list), train_rmse_list[-1]),\n",
    "             xycoords='data', xytext=(-30, 30), textcoords='offset points', fontsize=10,\n",
    "             arrowprops=dict(arrowstyle='->', connectionstyle='arc3, rad=.2'))\n",
    "plt.annotate(r'vali=%f' % (vali_rmse_list[-1]), xy=(len(vali_rmse_list), vali_rmse_list[-1]),\n",
    "             xycoords='data', xytext=(-30, 30), textcoords='offset points', fontsize=10,\n",
    "             arrowprops=dict(arrowstyle='->', connectionstyle='arc3, rad=.2'))\n",
    "plt.xlim([1, len(train_rmse_list)+10])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE Curve in Training Process')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(784,), name=\"digits\")\n",
    "x1 = tf.keras.layers.Dense(64, activation=\"relu\")(inputs)\n",
    "x2 = tf.keras.layers.Dense(64, activation=\"relu\")(x1)\n",
    "outputs = tf.keras.layers.Dense(10, name=\"predictions\")(x2)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
