{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a928ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c459a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMF(tf.keras.Model):\n",
    "    def __init__(self, n_users, n_items, n_dim):\n",
    "        super(PMF, self).__init__()\n",
    "        ## initializing attributes from parameters        \n",
    "        self.w_u_i_init = tf.keras.initializers.RandomUniform(minval=-1., maxval=1., seed=1)\n",
    "        \n",
    "        ## initializing user embedding layer\n",
    "        ## the output shape should be n_users * n_dim\n",
    "        self.user_embedding = tf.keras.layers.Embedding(n_users,\n",
    "                                                        n_dim,\n",
    "                                                        embeddings_initializer='uniform',\n",
    "                                                        embeddings_regularizer=tf.keras.regularizers.L2(0.1))\n",
    "        ## initializing user embedding layer\n",
    "        ## the output shape should be n_items * n_dim\n",
    "        self.item_embedding = tf.keras.layers.Embedding(n_items,\n",
    "                                                        n_dim,\n",
    "                                                        embeddings_initializer='uniform',\n",
    "                                                        embeddings_regularizer=tf.keras.regularizers.L2(0.1))\n",
    "        \n",
    "        ## users embedding\n",
    "        self.ub = tf.keras.layers.Embedding(n_users, \n",
    "                                            1, \n",
    "                                            embeddings_initializer=self.w_u_i_init, \n",
    "                                            embeddings_regularizer=tf.keras.regularizers.L2(0.1))\n",
    "        \n",
    "        ## items embedding\n",
    "        self.ib = tf.keras.layers.Embedding(n_items, \n",
    "                                            1, \n",
    "                                            embeddings_initializer=self.w_u_i_init, \n",
    "                                            embeddings_regularizer=tf.keras.regularizers.L2(0.1))\n",
    "        \n",
    "    def call(self, user_index, item_index):\n",
    "        ## get the user and item embedding value\n",
    "        user_h1 = self.user_embedding(user_index)\n",
    "        item_h1 = self.item_embedding(item_index)\n",
    "        ## should be checked again\n",
    "        r_h = tf.math.reduce_sum(user_h1 * item_h1) + tf.squeeze(self.ub(user_index)) + tf.squeeze(self.ib(item_index))\n",
    "        return r_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41ed32de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== TRAINING PMF =====\n",
      "===== Dataset has been loaded =====\n",
      "===== Preprocess the data has been finished =====\n",
      "===== Model Instantiated =====\n",
      "Model: \"pmf_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_20 (Embedding)    multiple                  94300     \n",
      "                                                                 \n",
      " embedding_21 (Embedding)    multiple                  168200    \n",
      "                                                                 \n",
      " embedding_22 (Embedding)    multiple                  943       \n",
      "                                                                 \n",
      " embedding_23 (Embedding)    multiple                  1682      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 265,125\n",
      "Trainable params: 265,125\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "===== Training Model =====\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 0.8968\n",
      "Training loss (for one batch) at step 200: 0.9419\n",
      "Training loss (for one batch) at step 400: 0.8707\n",
      "Training loss (for one batch) at step 600: 0.7434\n",
      "Training loss (for one batch) at step 800: 0.5181\n",
      "Training loss (for one batch) at step 1000: 0.7373\n",
      "Training loss (for one batch) at step 1200: 0.5456\n",
      "Training epoch: 1, training rmse: 0.110600, vali rmse:33.236296\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 2.6069\n",
      "Training loss (for one batch) at step 200: 0.5630\n",
      "Training loss (for one batch) at step 400: 0.5987\n",
      "Training loss (for one batch) at step 600: 0.7536\n",
      "Training loss (for one batch) at step 800: 0.3490\n",
      "Training loss (for one batch) at step 1000: 0.5344\n",
      "Training loss (for one batch) at step 1200: 0.4195\n",
      "Training epoch: 2, training rmse: 0.102730, vali rmse:30.429709\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 3.6396\n",
      "Training loss (for one batch) at step 200: 1.0424\n",
      "Training loss (for one batch) at step 400: 0.3541\n",
      "Training loss (for one batch) at step 600: 0.4030\n",
      "Training loss (for one batch) at step 800: 0.8703\n",
      "Training loss (for one batch) at step 1000: 0.4108\n",
      "Training loss (for one batch) at step 1200: 0.5190\n",
      "Training epoch: 3, training rmse: 0.122436, vali rmse:10.004445\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 3.0256\n",
      "Training loss (for one batch) at step 200: 3.0020\n",
      "Training loss (for one batch) at step 400: 0.2924\n",
      "Training loss (for one batch) at step 600: 0.2939\n",
      "Training loss (for one batch) at step 800: 1.4791\n",
      "Training loss (for one batch) at step 1000: 0.4580\n",
      "Training loss (for one batch) at step 1200: 0.5551\n",
      "Training epoch: 4, training rmse: 0.142457, vali rmse:10.797392\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 3.7929\n",
      "Training loss (for one batch) at step 200: 2.2897\n",
      "Training loss (for one batch) at step 400: 0.3144\n",
      "Training loss (for one batch) at step 600: 0.3147\n",
      "Training loss (for one batch) at step 800: 0.9094\n",
      "Training loss (for one batch) at step 1000: 0.3417\n",
      "Training loss (for one batch) at step 1200: 0.5359\n",
      "Training epoch: 5, training rmse: 0.125423, vali rmse:0.648491\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 1.1633\n",
      "Training loss (for one batch) at step 200: 1.4823\n",
      "Training loss (for one batch) at step 400: 0.2359\n",
      "Training loss (for one batch) at step 600: 0.2464\n",
      "Training loss (for one batch) at step 800: 0.2626\n",
      "Training loss (for one batch) at step 1000: 0.3189\n",
      "Training loss (for one batch) at step 1200: 0.2129\n",
      "Training epoch: 6, training rmse: 0.083105, vali rmse:3.011723\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 0.3078\n",
      "Training loss (for one batch) at step 200: 0.4458\n",
      "Training loss (for one batch) at step 400: 0.2232\n",
      "Training loss (for one batch) at step 600: 0.2140\n",
      "Training loss (for one batch) at step 800: 0.2053\n",
      "Training loss (for one batch) at step 1000: 0.2947\n",
      "Training loss (for one batch) at step 1200: 0.1714\n",
      "Training epoch: 7, training rmse: 0.064346, vali rmse:1.389218\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 0.1774\n",
      "Training loss (for one batch) at step 200: 0.2415\n",
      "Training loss (for one batch) at step 400: 0.2115\n",
      "Training loss (for one batch) at step 600: 0.2067\n",
      "Training loss (for one batch) at step 800: 0.1995\n",
      "Training loss (for one batch) at step 1000: 0.2820\n",
      "Training loss (for one batch) at step 1200: 0.1675\n",
      "Training epoch: 8, training rmse: 0.060842, vali rmse:2.423830\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 0.1721\n",
      "Training loss (for one batch) at step 200: 0.2027\n",
      "Training loss (for one batch) at step 400: 0.2066\n",
      "Training loss (for one batch) at step 600: 0.1999\n",
      "Training loss (for one batch) at step 800: 0.2048\n",
      "Training loss (for one batch) at step 1000: 0.3005\n",
      "Training loss (for one batch) at step 1200: 0.1909\n",
      "Training epoch: 9, training rmse: 0.061408, vali rmse:1.329495\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 0.2256\n",
      "Training loss (for one batch) at step 200: 0.2468\n",
      "Training loss (for one batch) at step 400: 0.2050\n",
      "Training loss (for one batch) at step 600: 0.2824\n",
      "Training loss (for one batch) at step 800: 0.2144\n",
      "Training loss (for one batch) at step 1000: 0.2710\n",
      "Training loss (for one batch) at step 1200: 0.3400\n",
      "Training epoch: 10, training rmse: 0.065069, vali rmse:3.196871\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 0.3076\n",
      "Training loss (for one batch) at step 200: 0.1665\n",
      "Training loss (for one batch) at step 400: 0.2285\n",
      "Training loss (for one batch) at step 600: 0.3683\n",
      "Training loss (for one batch) at step 800: 0.2187\n",
      "Training loss (for one batch) at step 1000: 0.5140\n",
      "Training loss (for one batch) at step 1200: 1.2469\n",
      "Training epoch: 11, training rmse: 0.092080, vali rmse:4.838441\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 1.4324\n",
      "Training loss (for one batch) at step 200: 0.6308\n",
      "Training loss (for one batch) at step 400: 0.5682\n",
      "Training loss (for one batch) at step 600: 0.9564\n",
      "Training loss (for one batch) at step 800: 0.4447\n",
      "Training loss (for one batch) at step 1000: 1.6980\n",
      "Training loss (for one batch) at step 1200: 4.8477\n",
      "Training epoch: 12, training rmse: 0.159971, vali rmse:2.686286\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 2.6829\n",
      "Training loss (for one batch) at step 200: 0.7024\n",
      "Training loss (for one batch) at step 400: 1.1419\n",
      "Training loss (for one batch) at step 600: 1.8236\n",
      "Training loss (for one batch) at step 800: 0.5772\n",
      "Training loss (for one batch) at step 1000: 1.4977\n",
      "Training loss (for one batch) at step 1200: 1.3521\n",
      "Training epoch: 13, training rmse: 0.188115, vali rmse:0.754789\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 2.2245\n",
      "Training loss (for one batch) at step 200: 0.4862\n",
      "Training loss (for one batch) at step 400: 0.3974\n",
      "Training loss (for one batch) at step 600: 0.4382\n",
      "Training loss (for one batch) at step 800: 0.2795\n",
      "Training loss (for one batch) at step 1000: 0.5134\n",
      "Training loss (for one batch) at step 1200: 0.1695\n",
      "Training epoch: 14, training rmse: 0.110578, vali rmse:0.683744\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 0.3884\n",
      "Training loss (for one batch) at step 200: 0.2230\n",
      "Training loss (for one batch) at step 400: 0.2077\n",
      "Training loss (for one batch) at step 600: 0.1976\n",
      "Training loss (for one batch) at step 800: 0.2007\n",
      "Training loss (for one batch) at step 1000: 0.2756\n",
      "Training loss (for one batch) at step 1200: 0.1610\n",
      "Training epoch: 15, training rmse: 0.060500, vali rmse:0.483736\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 0.1832\n",
      "Training loss (for one batch) at step 200: 0.1617\n",
      "Training loss (for one batch) at step 400: 0.2042\n",
      "Training loss (for one batch) at step 600: 0.1913\n",
      "Training loss (for one batch) at step 800: 0.1989\n",
      "Training loss (for one batch) at step 1000: 0.2648\n",
      "Training loss (for one batch) at step 1200: 0.1612\n",
      "Training epoch: 16, training rmse: 0.057615, vali rmse:0.503532\n",
      "===== Testing Model =====\n",
      "Test rmse: 0.676321\n"
     ]
    }
   ],
   "source": [
    "print('===== TRAINING PMF =====')\n",
    "## initializing hyperparameters\n",
    "batch_size = 64\n",
    "epoches = 150\n",
    "seed = 1\n",
    "weight_decay = 0.1\n",
    "emb_dim = 100\n",
    "ratio = 0.8\n",
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "\n",
    "## loading dataset\n",
    "users = pickle.load(open('dataset/user_id_to_num_mov.pkl', 'rb'))\n",
    "items = pickle.load(open('dataset/movie_id_to_num_mov.pkl', 'rb'))\n",
    "data = np.load('dataset/data.npy')\n",
    "\n",
    "print(\"===== Dataset has been loaded =====\")\n",
    "\n",
    "## As the paper implemented, normalizing the rating\n",
    "## that will act as reward\n",
    "data[:, 2] = 0.5 * (data[:, 2] - 3)\n",
    "\n",
    "## Shuffle data\n",
    "np.random.shuffle(data)\n",
    " \n",
    "## Splitting data data\n",
    "train_data = data[:int(ratio * data.shape[0])]\n",
    "vali_data = data[int(ratio * data.shape[0]):int((ratio + (1 - ratio) / 2) * data.shape[0])]\n",
    "test_data = data[int((ratio + ( 1 - ratio ) / 2) * data.shape[0]):]\n",
    " \n",
    "## Extract number of users and items\n",
    "NUM_USERS = len(users)\n",
    "NUM_ITEMS = len(items)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(batch_size)\n",
    "print(\"===== Preprocess the data has been finished =====\")\n",
    "\n",
    "model = PMF(NUM_USERS, NUM_ITEMS, emb_dim)\n",
    "model(1, 1)\n",
    "\n",
    "print(\"===== Model Instantiated =====\")\n",
    "model.summary()\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def train(train_dataset, len_dataset):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for step, elem in enumerate(train_dataset):\n",
    "        row = elem[:, 0] ## users as a row\n",
    "        col = elem[:, 1] ## items as a column\n",
    "        val = elem[:, 2] ## normalized ratings as value\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            ## Perlu diperiksa kembali\n",
    "            row = tf.Variable(row, trainable=False)\n",
    "            col = tf.Variable(col, trainable=False)\n",
    "            val = tf.Variable(val, trainable=False)\n",
    "            \n",
    "            ## run the forward pass\n",
    "            logits = model(row, col, training=True)\n",
    "            \n",
    "            ## compute the loss value\n",
    "            loss_value = loss_fn(val, logits)\n",
    "            \n",
    "        ## using gradien tape to automatically retrieves\n",
    "        ## the gradients of the trainable variables with respect to loss\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        \n",
    "        ## run one step of gradient descent by updating\n",
    "        ## the value of variable loss\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        epoch_loss += loss_value\n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "    \n",
    "    return float(loss_value)\n",
    "    \n",
    "print(\"===== Training Model =====\")\n",
    "## this list is used for visualization\n",
    "train_loss_list = []\n",
    "train_rmse_list = []\n",
    "vali_rmse_list = []\n",
    "len_train_data = len(train_data)\n",
    "\n",
    "last_vali_rmse = None\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    ## train epoch losses\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    \n",
    "    train_epoch_loss = train(train_dataset, len_train_data)\n",
    "    \n",
    "    ## test \n",
    "    train_loss_list.append(train_epoch_loss)\n",
    "    \n",
    "    ## creating index for predicting\n",
    "    vali_row = tf.Variable(tf.convert_to_tensor(vali_data[:, 0]), trainable=False)\n",
    "    vali_col = tf.Variable(tf.convert_to_tensor(vali_data[:, 1]), trainable=False)\n",
    "    \n",
    "    ## predicting the value\n",
    "    vali_preds = model(vali_row, vali_col, training=False)\n",
    "    \n",
    "    ## calculating rmse\n",
    "    train_rmse = np.sqrt(train_epoch_loss)\n",
    "    vali_rmse = np.sqrt(np.square(np.subtract(vali_data[:, 2], vali_preds)).mean())\n",
    "    \n",
    "    train_rmse_list.append(train_rmse)\n",
    "    vali_rmse_list.append(vali_rmse)\n",
    "    \n",
    "    print('Training epoch:{: d}, training rmse:{: .6f}, vali rmse:{:.6f}'.format(epoch+1, train_rmse, vali_rmse))\n",
    "    \n",
    "    if last_vali_rmse and last_vali_rmse < 0.5:\n",
    "        break\n",
    "    else:\n",
    "        last_vali_rmse = vali_rmse\n",
    "\n",
    "print(\"===== Testing Model =====\")\n",
    "test_row = tf.Variable(tf.convert_to_tensor(test_data[:, 0]), trainable=False)\n",
    "test_col = tf.Variable(tf.convert_to_tensor(test_data[:, 1]), trainable=False)\n",
    "\n",
    "preds = model(test_row, test_col, training=False)\n",
    "\n",
    "test_rmse = np.sqrt(np.square(np.subtract(test_data[:, 2], vali_preds)).mean())\n",
    "print('Test rmse: {:f}'.format(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "133f7d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('trained/adam/pmf_150_adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed6bad21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([324., 113.,   1.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d30d19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = train_data[0, 0]\n",
    "x2 = train_data[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4fda2697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51334107"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(324., 113., training=False).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43c88825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create plots\n",
    "plt.figure(1)\n",
    "plt.plot(range(1, len(train_rmse_list)+1), train_rmse_list, color='r', label='train rmse')\n",
    "plt.plot(range(1, len(vali_rmse_list)+1), vali_rmse_list, color='b', label='test rmse')\n",
    "plt.legend()\n",
    "plt.annotate(r'train=%f' % (train_rmse_list[-1]), xy=(len(train_rmse_list), train_rmse_list[-1]),\n",
    "             xycoords='data', xytext=(-30, 30), textcoords='offset points', fontsize=10,\n",
    "             arrowprops=dict(arrowstyle='->', connectionstyle='arc3, rad=.2'))\n",
    "plt.annotate(r'vali=%f' % (vali_rmse_list[-1]), xy=(len(vali_rmse_list), vali_rmse_list[-1]),\n",
    "             xycoords='data', xytext=(-30, 30), textcoords='offset points', fontsize=10,\n",
    "             arrowprops=dict(arrowstyle='->', connectionstyle='arc3, rad=.2'))\n",
    "plt.xlim([1, len(train_rmse_list)+10])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE Curve in Training Process')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
