{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a928ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c459a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMF(tf.keras.Model):\n",
    "    def __init__(self, n_users, n_items, n_dim):\n",
    "        super(PMF, self).__init__()\n",
    "        ## initializing attributes from parameters        \n",
    "        self.w_u_i_init = tf.keras.initializers.RandomUniform(minval=-1., maxval=1., seed=1)\n",
    "        \n",
    "        ## initializing user embedding layer\n",
    "        ## the output shape should be n_users * n_dim\n",
    "        self.user_embedding = tf.keras.layers.Embedding(n_users,\n",
    "                                                        n_dim,\n",
    "                                                        embeddings_initializer='uniform',\n",
    "                                                        embeddings_regularizer=tf.keras.regularizers.L2(0.1))\n",
    "        ## initializing user embedding layer\n",
    "        ## the output shape should be n_items * n_dim\n",
    "        self.item_embedding = tf.keras.layers.Embedding(n_items,\n",
    "                                                        n_dim,\n",
    "                                                        embeddings_initializer='uniform',\n",
    "                                                        embeddings_regularizer=tf.keras.regularizers.L2(0.1))\n",
    "        \n",
    "        ## users embedding\n",
    "        self.ub = tf.keras.layers.Embedding(n_users, \n",
    "                                            1, \n",
    "                                            embeddings_initializer=self.w_u_i_init, \n",
    "                                            embeddings_regularizer=tf.keras.regularizers.L2(0.1))\n",
    "        \n",
    "        ## items embedding\n",
    "        self.ib = tf.keras.layers.Embedding(n_items, \n",
    "                                            1, \n",
    "                                            embeddings_initializer=self.w_u_i_init, \n",
    "                                            embeddings_regularizer=tf.keras.regularizers.L2(0.1))\n",
    "        \n",
    "    def call(self, user_index, item_index):\n",
    "        ## get the user and item embedding value\n",
    "        user_h1 = self.user_embedding(user_index)\n",
    "        item_h1 = self.item_embedding(item_index)\n",
    "        ## should be checked again\n",
    "        r_h = tf.math.reduce_sum(user_h1 * item_h1, axis=1 if len(user_h1.shape) > 1 else 0)\n",
    "        r_h += tf.squeeze(self.ub(user_index))\n",
    "        r_h += tf.squeeze(self.ib(item_index))\n",
    "        return r_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41ed32de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== TRAINING PMF =====\n",
      "===== Dataset has been loaded =====\n",
      "===== Preprocess the data has been finished =====\n",
      "===== Model Instantiated =====\n",
      "Model: \"pmf_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_12 (Embedding)    multiple                  55000     \n",
      "                                                                 \n",
      " embedding_13 (Embedding)    multiple                  12900     \n",
      "                                                                 \n",
      " embedding_14 (Embedding)    multiple                  550       \n",
      "                                                                 \n",
      " embedding_15 (Embedding)    multiple                  129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 68,579\n",
      "Trainable params: 68,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "===== Training Model =====\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 0.9299\n",
      "Training epoch: 1, training rmse: 0.877210, vali rmse:0.932944\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.9175\n",
      "Training epoch: 2, training rmse: 0.865162, vali rmse:0.925503\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 0.9014\n",
      "Training epoch: 3, training rmse: 0.852867, vali rmse:0.918051\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 0.8854\n",
      "Training epoch: 4, training rmse: 0.840689, vali rmse:0.910723\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 0.8699\n",
      "Training epoch: 5, training rmse: 0.828721, vali rmse:0.903551\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 0.8547\n",
      "Training epoch: 6, training rmse: 0.816988, vali rmse:0.896545\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 0.8400\n",
      "Training epoch: 7, training rmse: 0.805493, vali rmse:0.889703\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 0.8257\n",
      "Training epoch: 8, training rmse: 0.794235, vali rmse:0.883023\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 0.8119\n",
      "Training epoch: 9, training rmse: 0.783210, vali rmse:0.876500\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 0.7985\n",
      "Training epoch: 10, training rmse: 0.772414, vali rmse:0.870131\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 0.7855\n",
      "Training epoch: 11, training rmse: 0.761840, vali rmse:0.863910\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 0.7728\n",
      "Training epoch: 12, training rmse: 0.751487, vali rmse:0.857835\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 0.7606\n",
      "Training epoch: 13, training rmse: 0.741347, vali rmse:0.851900\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 0.7487\n",
      "Training epoch: 14, training rmse: 0.731419, vali rmse:0.846103\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 0.7371\n",
      "Training epoch: 15, training rmse: 0.721695, vali rmse:0.840438\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 0.7258\n",
      "Training epoch: 16, training rmse: 0.712174, vali rmse:0.834903\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 0.7149\n",
      "Training epoch: 17, training rmse: 0.702850, vali rmse:0.829494\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 0.7043\n",
      "Training epoch: 18, training rmse: 0.693720, vali rmse:0.824207\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 0.6940\n",
      "Training epoch: 19, training rmse: 0.684779, vali rmse:0.819039\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 0.6840\n",
      "Training epoch: 20, training rmse: 0.676023, vali rmse:0.813986\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 0.6743\n",
      "Training epoch: 21, training rmse: 0.667449, vali rmse:0.809046\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 0.6648\n",
      "Training epoch: 22, training rmse: 0.659053, vali rmse:0.804215\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 0.6556\n",
      "Training epoch: 23, training rmse: 0.650831, vali rmse:0.799490\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 0.6466\n",
      "Training epoch: 24, training rmse: 0.642780, vali rmse:0.794867\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 0.6379\n",
      "Training epoch: 25, training rmse: 0.634895, vali rmse:0.790346\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 0.6295\n",
      "Training epoch: 26, training rmse: 0.627175, vali rmse:0.785921\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 0.6212\n",
      "Training epoch: 27, training rmse: 0.619614, vali rmse:0.781591\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 0.6132\n",
      "Training epoch: 28, training rmse: 0.612211, vali rmse:0.777353\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 0.6054\n",
      "Training epoch: 29, training rmse: 0.604960, vali rmse:0.773205\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 0.5977\n",
      "Training epoch: 30, training rmse: 0.597861, vali rmse:0.769143\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 0.5903\n",
      "Training epoch: 31, training rmse: 0.590908, vali rmse:0.765166\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 0.5831\n",
      "Training epoch: 32, training rmse: 0.584100, vali rmse:0.761271\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 0.5761\n",
      "Training epoch: 33, training rmse: 0.577432, vali rmse:0.757456\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 0.5692\n",
      "Training epoch: 34, training rmse: 0.570903, vali rmse:0.753718\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 0.5625\n",
      "Training epoch: 35, training rmse: 0.564510, vali rmse:0.750057\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 0.5560\n",
      "Training epoch: 36, training rmse: 0.558248, vali rmse:0.746468\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 0.5496\n",
      "Training epoch: 37, training rmse: 0.552116, vali rmse:0.742951\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 0.5434\n",
      "Training epoch: 38, training rmse: 0.546112, vali rmse:0.739503\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 0.5374\n",
      "Training epoch: 39, training rmse: 0.540231, vali rmse:0.736122\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 0.5315\n",
      "Training epoch: 40, training rmse: 0.534472, vali rmse:0.732807\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 0.5257\n",
      "Training epoch: 41, training rmse: 0.528832, vali rmse:0.729556\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 0.5201\n",
      "Training epoch: 42, training rmse: 0.523309, vali rmse:0.726367\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 0.5146\n",
      "Training epoch: 43, training rmse: 0.517899, vali rmse:0.723239\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 0.5093\n",
      "Training epoch: 44, training rmse: 0.512601, vali rmse:0.720169\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 0.5041\n",
      "Training epoch: 45, training rmse: 0.507413, vali rmse:0.717156\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 0.4990\n",
      "Training epoch: 46, training rmse: 0.502331, vali rmse:0.714199\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 0.4940\n",
      "Training epoch: 47, training rmse: 0.497354, vali rmse:0.711295\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 0.4891\n",
      "Training epoch: 48, training rmse: 0.492480, vali rmse:0.708444\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 0.4843\n",
      "Training epoch: 49, training rmse: 0.487705, vali rmse:0.705645\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 0.4797\n",
      "Training epoch: 50, training rmse: 0.483029, vali rmse:0.702895\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 0.4751\n",
      "Training epoch: 51, training rmse: 0.478448, vali rmse:0.700194\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 0.4707\n",
      "Training epoch: 52, training rmse: 0.473962, vali rmse:0.697539\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 0.4663\n",
      "Training epoch: 53, training rmse: 0.469567, vali rmse:0.694931\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 0.4620\n",
      "Training epoch: 54, training rmse: 0.465262, vali rmse:0.692367\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 0.4579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 55, training rmse: 0.461045, vali rmse:0.689847\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 0.4538\n",
      "Training epoch: 56, training rmse: 0.456914, vali rmse:0.687369\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 0.4498\n",
      "Training epoch: 57, training rmse: 0.452867, vali rmse:0.684933\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 0.4459\n",
      "Training epoch: 58, training rmse: 0.448903, vali rmse:0.682536\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 0.4421\n",
      "Training epoch: 59, training rmse: 0.445019, vali rmse:0.680179\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 0.4383\n",
      "Training epoch: 60, training rmse: 0.441214, vali rmse:0.677860\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 0.4346\n",
      "Training epoch: 61, training rmse: 0.437486, vali rmse:0.675578\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 0.4310\n",
      "Training epoch: 62, training rmse: 0.433833, vali rmse:0.673332\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 0.4275\n",
      "Training epoch: 63, training rmse: 0.430254, vali rmse:0.671121\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 0.4241\n",
      "Training epoch: 64, training rmse: 0.426747, vali rmse:0.668944\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 0.4207\n",
      "Training epoch: 65, training rmse: 0.423311, vali rmse:0.666801\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 0.4174\n",
      "Training epoch: 66, training rmse: 0.419943, vali rmse:0.664691\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 0.4141\n",
      "Training epoch: 67, training rmse: 0.416644, vali rmse:0.662612\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 0.4109\n",
      "Training epoch: 68, training rmse: 0.413410, vali rmse:0.660565\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 0.4078\n",
      "Training epoch: 69, training rmse: 0.410240, vali rmse:0.658547\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 0.4047\n",
      "Training epoch: 70, training rmse: 0.407134, vali rmse:0.656560\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 0.4017\n",
      "Training epoch: 71, training rmse: 0.404089, vali rmse:0.654601\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 0.3987\n",
      "Training epoch: 72, training rmse: 0.401105, vali rmse:0.652669\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 0.3958\n",
      "Training epoch: 73, training rmse: 0.398179, vali rmse:0.650766\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 0.3930\n",
      "Training epoch: 74, training rmse: 0.395312, vali rmse:0.648889\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 0.3902\n",
      "Training epoch: 75, training rmse: 0.392500, vali rmse:0.647038\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 0.3874\n",
      "Training epoch: 76, training rmse: 0.389744, vali rmse:0.645212\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 0.3847\n",
      "Training epoch: 77, training rmse: 0.387042, vali rmse:0.643412\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 0.3821\n",
      "Training epoch: 78, training rmse: 0.384393, vali rmse:0.641635\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 0.3795\n",
      "Training epoch: 79, training rmse: 0.381795, vali rmse:0.639883\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 0.3769\n",
      "Training epoch: 80, training rmse: 0.379247, vali rmse:0.638153\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 0.3744\n",
      "Training epoch: 81, training rmse: 0.376749, vali rmse:0.636446\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 0.3719\n",
      "Training epoch: 82, training rmse: 0.374299, vali rmse:0.634761\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 0.3695\n",
      "Training epoch: 83, training rmse: 0.371896, vali rmse:0.633098\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 0.3671\n",
      "Training epoch: 84, training rmse: 0.369539, vali rmse:0.631455\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 0.3648\n",
      "Training epoch: 85, training rmse: 0.367227, vali rmse:0.629834\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 0.3624\n",
      "Training epoch: 86, training rmse: 0.364959, vali rmse:0.628232\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 0.3602\n",
      "Training epoch: 87, training rmse: 0.362734, vali rmse:0.626650\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 0.3579\n",
      "Training epoch: 88, training rmse: 0.360551, vali rmse:0.625088\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 0.3557\n",
      "Training epoch: 89, training rmse: 0.358409, vali rmse:0.623544\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 0.3536\n",
      "Training epoch: 90, training rmse: 0.356307, vali rmse:0.622018\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 0.3514\n",
      "Training epoch: 91, training rmse: 0.354245, vali rmse:0.620511\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 0.3493\n",
      "Training epoch: 92, training rmse: 0.352221, vali rmse:0.619021\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 0.3473\n",
      "Training epoch: 93, training rmse: 0.350234, vali rmse:0.617549\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 0.3452\n",
      "Training epoch: 94, training rmse: 0.348284, vali rmse:0.616093\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 0.3432\n",
      "Training epoch: 95, training rmse: 0.346371, vali rmse:0.614655\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 0.3413\n",
      "Training epoch: 96, training rmse: 0.344492, vali rmse:0.613232\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 0.3393\n",
      "Training epoch: 97, training rmse: 0.342647, vali rmse:0.611825\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 0.3374\n",
      "Training epoch: 98, training rmse: 0.340836, vali rmse:0.610434\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 0.3355\n",
      "Training epoch: 99, training rmse: 0.339058, vali rmse:0.609057\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 0.3337\n",
      "Training epoch: 100, training rmse: 0.337312, vali rmse:0.607696\n",
      "\n",
      "Start of epoch 100\n",
      "Training loss (for one batch) at step 0: 0.3318\n",
      "Training epoch: 101, training rmse: 0.335597, vali rmse:0.606350\n",
      "\n",
      "Start of epoch 101\n",
      "Training loss (for one batch) at step 0: 0.3300\n",
      "Training epoch: 102, training rmse: 0.333913, vali rmse:0.605017\n",
      "\n",
      "Start of epoch 102\n",
      "Training loss (for one batch) at step 0: 0.3283\n",
      "Training epoch: 103, training rmse: 0.332259, vali rmse:0.603699\n",
      "\n",
      "Start of epoch 103\n",
      "Training loss (for one batch) at step 0: 0.3265\n",
      "Training epoch: 104, training rmse: 0.330634, vali rmse:0.602395\n",
      "\n",
      "Start of epoch 104\n",
      "Training loss (for one batch) at step 0: 0.3248\n",
      "Training epoch: 105, training rmse: 0.329038, vali rmse:0.601104\n",
      "\n",
      "Start of epoch 105\n",
      "Training loss (for one batch) at step 0: 0.3231\n",
      "Training epoch: 106, training rmse: 0.327470, vali rmse:0.599827\n",
      "\n",
      "Start of epoch 106\n",
      "Training loss (for one batch) at step 0: 0.3214\n",
      "Training epoch: 107, training rmse: 0.325929, vali rmse:0.598562\n",
      "\n",
      "Start of epoch 107\n",
      "Training loss (for one batch) at step 0: 0.3197\n",
      "Training epoch: 108, training rmse: 0.324415, vali rmse:0.597310\n",
      "\n",
      "Start of epoch 108\n",
      "Training loss (for one batch) at step 0: 0.3181\n",
      "Training epoch: 109, training rmse: 0.322927, vali rmse:0.596071\n",
      "\n",
      "Start of epoch 109\n",
      "Training loss (for one batch) at step 0: 0.3165\n",
      "Training epoch: 110, training rmse: 0.321464, vali rmse:0.594844\n",
      "\n",
      "Start of epoch 110\n",
      "Training loss (for one batch) at step 0: 0.3149\n",
      "Training epoch: 111, training rmse: 0.320027, vali rmse:0.593629\n",
      "\n",
      "Start of epoch 111\n",
      "Training loss (for one batch) at step 0: 0.3133\n",
      "Training epoch: 112, training rmse: 0.318614, vali rmse:0.592426\n",
      "\n",
      "Start of epoch 112\n",
      "Training loss (for one batch) at step 0: 0.3118\n",
      "Training epoch: 113, training rmse: 0.317225, vali rmse:0.591235\n",
      "\n",
      "Start of epoch 113\n",
      "Training loss (for one batch) at step 0: 0.3102\n",
      "Training epoch: 114, training rmse: 0.315860, vali rmse:0.590055\n",
      "\n",
      "Start of epoch 114\n",
      "Training loss (for one batch) at step 0: 0.3087\n",
      "Training epoch: 115, training rmse: 0.314517, vali rmse:0.588886\n",
      "\n",
      "Start of epoch 115\n",
      "Training loss (for one batch) at step 0: 0.3072\n",
      "Training epoch: 116, training rmse: 0.313197, vali rmse:0.587729\n",
      "\n",
      "Start of epoch 116\n",
      "Training loss (for one batch) at step 0: 0.3058\n",
      "Training epoch: 117, training rmse: 0.311899, vali rmse:0.586582\n",
      "\n",
      "Start of epoch 117\n",
      "Training loss (for one batch) at step 0: 0.3043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 118, training rmse: 0.310622, vali rmse:0.585446\n",
      "\n",
      "Start of epoch 118\n",
      "Training loss (for one batch) at step 0: 0.3029\n",
      "Training epoch: 119, training rmse: 0.309366, vali rmse:0.584320\n",
      "\n",
      "Start of epoch 119\n",
      "Training loss (for one batch) at step 0: 0.3015\n",
      "Training epoch: 120, training rmse: 0.308131, vali rmse:0.583205\n",
      "\n",
      "Start of epoch 120\n",
      "Training loss (for one batch) at step 0: 0.3001\n",
      "Training epoch: 121, training rmse: 0.306915, vali rmse:0.582100\n",
      "\n",
      "Start of epoch 121\n",
      "Training loss (for one batch) at step 0: 0.2987\n",
      "Training epoch: 122, training rmse: 0.305720, vali rmse:0.581005\n",
      "\n",
      "Start of epoch 122\n",
      "Training loss (for one batch) at step 0: 0.2973\n",
      "Training epoch: 123, training rmse: 0.304543, vali rmse:0.579920\n",
      "\n",
      "Start of epoch 123\n",
      "Training loss (for one batch) at step 0: 0.2960\n",
      "Training epoch: 124, training rmse: 0.303386, vali rmse:0.578844\n",
      "\n",
      "Start of epoch 124\n",
      "Training loss (for one batch) at step 0: 0.2946\n",
      "Training epoch: 125, training rmse: 0.302247, vali rmse:0.577778\n",
      "\n",
      "Start of epoch 125\n",
      "Training loss (for one batch) at step 0: 0.2933\n",
      "Training epoch: 126, training rmse: 0.301126, vali rmse:0.576721\n",
      "\n",
      "Start of epoch 126\n",
      "Training loss (for one batch) at step 0: 0.2920\n",
      "Training epoch: 127, training rmse: 0.300022, vali rmse:0.575674\n",
      "\n",
      "Start of epoch 127\n",
      "Training loss (for one batch) at step 0: 0.2907\n",
      "Training epoch: 128, training rmse: 0.298936, vali rmse:0.574636\n",
      "\n",
      "Start of epoch 128\n",
      "Training loss (for one batch) at step 0: 0.2895\n",
      "Training epoch: 129, training rmse: 0.297866, vali rmse:0.573606\n",
      "\n",
      "Start of epoch 129\n",
      "Training loss (for one batch) at step 0: 0.2882\n",
      "Training epoch: 130, training rmse: 0.296813, vali rmse:0.572585\n",
      "\n",
      "Start of epoch 130\n",
      "Training loss (for one batch) at step 0: 0.2870\n",
      "Training epoch: 131, training rmse: 0.295777, vali rmse:0.571573\n",
      "\n",
      "Start of epoch 131\n",
      "Training loss (for one batch) at step 0: 0.2858\n",
      "Training epoch: 132, training rmse: 0.294756, vali rmse:0.570570\n",
      "\n",
      "Start of epoch 132\n",
      "Training loss (for one batch) at step 0: 0.2845\n",
      "Training epoch: 133, training rmse: 0.293750, vali rmse:0.569575\n",
      "\n",
      "Start of epoch 133\n",
      "Training loss (for one batch) at step 0: 0.2833\n",
      "Training epoch: 134, training rmse: 0.292760, vali rmse:0.568588\n",
      "\n",
      "Start of epoch 134\n",
      "Training loss (for one batch) at step 0: 0.2822\n",
      "Training epoch: 135, training rmse: 0.291785, vali rmse:0.567610\n",
      "\n",
      "Start of epoch 135\n",
      "Training loss (for one batch) at step 0: 0.2810\n",
      "Training epoch: 136, training rmse: 0.290824, vali rmse:0.566639\n",
      "\n",
      "Start of epoch 136\n",
      "Training loss (for one batch) at step 0: 0.2798\n",
      "Training epoch: 137, training rmse: 0.289877, vali rmse:0.565677\n",
      "\n",
      "Start of epoch 137\n",
      "Training loss (for one batch) at step 0: 0.2787\n",
      "Training epoch: 138, training rmse: 0.288945, vali rmse:0.564722\n",
      "\n",
      "Start of epoch 138\n",
      "Training loss (for one batch) at step 0: 0.2776\n",
      "Training epoch: 139, training rmse: 0.288026, vali rmse:0.563775\n",
      "\n",
      "Start of epoch 139\n",
      "Training loss (for one batch) at step 0: 0.2764\n",
      "Training epoch: 140, training rmse: 0.287120, vali rmse:0.562836\n",
      "\n",
      "Start of epoch 140\n",
      "Training loss (for one batch) at step 0: 0.2753\n",
      "Training epoch: 141, training rmse: 0.286228, vali rmse:0.561904\n",
      "\n",
      "Start of epoch 141\n",
      "Training loss (for one batch) at step 0: 0.2742\n",
      "Training epoch: 142, training rmse: 0.285349, vali rmse:0.560980\n",
      "\n",
      "Start of epoch 142\n",
      "Training loss (for one batch) at step 0: 0.2732\n",
      "Training epoch: 143, training rmse: 0.284482, vali rmse:0.560063\n",
      "\n",
      "Start of epoch 143\n",
      "Training loss (for one batch) at step 0: 0.2721\n",
      "Training epoch: 144, training rmse: 0.283627, vali rmse:0.559153\n",
      "\n",
      "Start of epoch 144\n",
      "Training loss (for one batch) at step 0: 0.2710\n",
      "Training epoch: 145, training rmse: 0.282785, vali rmse:0.558250\n",
      "\n",
      "Start of epoch 145\n",
      "Training loss (for one batch) at step 0: 0.2700\n",
      "Training epoch: 146, training rmse: 0.281954, vali rmse:0.557355\n",
      "\n",
      "Start of epoch 146\n",
      "Training loss (for one batch) at step 0: 0.2689\n",
      "Training epoch: 147, training rmse: 0.281135, vali rmse:0.556466\n",
      "\n",
      "Start of epoch 147\n",
      "Training loss (for one batch) at step 0: 0.2679\n",
      "Training epoch: 148, training rmse: 0.280327, vali rmse:0.555584\n",
      "\n",
      "Start of epoch 148\n",
      "Training loss (for one batch) at step 0: 0.2669\n",
      "Training epoch: 149, training rmse: 0.279531, vali rmse:0.554710\n",
      "\n",
      "Start of epoch 149\n",
      "Training loss (for one batch) at step 0: 0.2659\n",
      "Training epoch: 150, training rmse: 0.278745, vali rmse:0.553841\n",
      "\n",
      "Start of epoch 150\n",
      "Training loss (for one batch) at step 0: 0.2649\n",
      "Training epoch: 151, training rmse: 0.277970, vali rmse:0.552980\n",
      "\n",
      "Start of epoch 151\n",
      "Training loss (for one batch) at step 0: 0.2639\n",
      "Training epoch: 152, training rmse: 0.277206, vali rmse:0.552125\n",
      "\n",
      "Start of epoch 152\n",
      "Training loss (for one batch) at step 0: 0.2630\n",
      "Training epoch: 153, training rmse: 0.276452, vali rmse:0.551276\n",
      "\n",
      "Start of epoch 153\n",
      "Training loss (for one batch) at step 0: 0.2620\n",
      "Training epoch: 154, training rmse: 0.275708, vali rmse:0.550434\n",
      "\n",
      "Start of epoch 154\n",
      "Training loss (for one batch) at step 0: 0.2610\n",
      "Training epoch: 155, training rmse: 0.274974, vali rmse:0.549598\n",
      "\n",
      "Start of epoch 155\n",
      "Training loss (for one batch) at step 0: 0.2601\n",
      "Training epoch: 156, training rmse: 0.274250, vali rmse:0.548768\n",
      "\n",
      "Start of epoch 156\n",
      "Training loss (for one batch) at step 0: 0.2592\n",
      "Training epoch: 157, training rmse: 0.273535, vali rmse:0.547945\n",
      "\n",
      "Start of epoch 157\n",
      "Training loss (for one batch) at step 0: 0.2582\n",
      "Training epoch: 158, training rmse: 0.272830, vali rmse:0.547127\n",
      "\n",
      "Start of epoch 158\n",
      "Training loss (for one batch) at step 0: 0.2573\n",
      "Training epoch: 159, training rmse: 0.272134, vali rmse:0.546316\n",
      "\n",
      "Start of epoch 159\n",
      "Training loss (for one batch) at step 0: 0.2564\n",
      "Training epoch: 160, training rmse: 0.271447, vali rmse:0.545511\n",
      "\n",
      "Start of epoch 160\n",
      "Training loss (for one batch) at step 0: 0.2555\n",
      "Training epoch: 161, training rmse: 0.270768, vali rmse:0.544711\n",
      "\n",
      "Start of epoch 161\n",
      "Training loss (for one batch) at step 0: 0.2546\n",
      "Training epoch: 162, training rmse: 0.270099, vali rmse:0.543917\n",
      "\n",
      "Start of epoch 162\n",
      "Training loss (for one batch) at step 0: 0.2537\n",
      "Training epoch: 163, training rmse: 0.269438, vali rmse:0.543129\n",
      "\n",
      "Start of epoch 163\n",
      "Training loss (for one batch) at step 0: 0.2529\n",
      "Training epoch: 164, training rmse: 0.268785, vali rmse:0.542347\n",
      "\n",
      "Start of epoch 164\n",
      "Training loss (for one batch) at step 0: 0.2520\n",
      "Training epoch: 165, training rmse: 0.268140, vali rmse:0.541571\n",
      "\n",
      "Start of epoch 165\n",
      "Training loss (for one batch) at step 0: 0.2512\n",
      "Training epoch: 166, training rmse: 0.267504, vali rmse:0.540800\n",
      "\n",
      "Start of epoch 166\n",
      "Training loss (for one batch) at step 0: 0.2503\n",
      "Training epoch: 167, training rmse: 0.266875, vali rmse:0.540034\n",
      "\n",
      "Start of epoch 167\n",
      "Training loss (for one batch) at step 0: 0.2495\n",
      "Training epoch: 168, training rmse: 0.266254, vali rmse:0.539274\n",
      "\n",
      "Start of epoch 168\n",
      "Training loss (for one batch) at step 0: 0.2486\n",
      "Training epoch: 169, training rmse: 0.265641, vali rmse:0.538520\n",
      "\n",
      "Start of epoch 169\n",
      "Training loss (for one batch) at step 0: 0.2478\n",
      "Training epoch: 170, training rmse: 0.265035, vali rmse:0.537771\n",
      "\n",
      "Start of epoch 170\n",
      "Training loss (for one batch) at step 0: 0.2470\n",
      "Training epoch: 171, training rmse: 0.264437, vali rmse:0.537027\n",
      "\n",
      "Start of epoch 171\n",
      "Training loss (for one batch) at step 0: 0.2462\n",
      "Training epoch: 172, training rmse: 0.263846, vali rmse:0.536288\n",
      "\n",
      "Start of epoch 172\n",
      "Training loss (for one batch) at step 0: 0.2454\n",
      "Training epoch: 173, training rmse: 0.263262, vali rmse:0.535555\n",
      "\n",
      "Start of epoch 173\n",
      "Training loss (for one batch) at step 0: 0.2446\n",
      "Training epoch: 174, training rmse: 0.262685, vali rmse:0.534827\n",
      "\n",
      "Start of epoch 174\n",
      "Training loss (for one batch) at step 0: 0.2438\n",
      "Training epoch: 175, training rmse: 0.262114, vali rmse:0.534104\n",
      "\n",
      "Start of epoch 175\n",
      "Training loss (for one batch) at step 0: 0.2431\n",
      "Training epoch: 176, training rmse: 0.261551, vali rmse:0.533386\n",
      "\n",
      "Start of epoch 176\n",
      "Training loss (for one batch) at step 0: 0.2423\n",
      "Training epoch: 177, training rmse: 0.260994, vali rmse:0.532673\n",
      "\n",
      "Start of epoch 177\n",
      "Training loss (for one batch) at step 0: 0.2415\n",
      "Training epoch: 178, training rmse: 0.260443, vali rmse:0.531964\n",
      "\n",
      "Start of epoch 178\n",
      "Training loss (for one batch) at step 0: 0.2408\n",
      "Training epoch: 179, training rmse: 0.259899, vali rmse:0.531261\n",
      "\n",
      "Start of epoch 179\n",
      "Training loss (for one batch) at step 0: 0.2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 180, training rmse: 0.259362, vali rmse:0.530563\n",
      "\n",
      "Start of epoch 180\n",
      "Training loss (for one batch) at step 0: 0.2393\n",
      "Training epoch: 181, training rmse: 0.258830, vali rmse:0.529869\n",
      "\n",
      "Start of epoch 181\n",
      "Training loss (for one batch) at step 0: 0.2385\n",
      "Training epoch: 182, training rmse: 0.258304, vali rmse:0.529181\n",
      "\n",
      "Start of epoch 182\n",
      "Training loss (for one batch) at step 0: 0.2378\n",
      "Training epoch: 183, training rmse: 0.257785, vali rmse:0.528497\n",
      "\n",
      "Start of epoch 183\n",
      "Training loss (for one batch) at step 0: 0.2371\n",
      "Training epoch: 184, training rmse: 0.257271, vali rmse:0.527817\n",
      "\n",
      "Start of epoch 184\n",
      "Training loss (for one batch) at step 0: 0.2364\n",
      "Training epoch: 185, training rmse: 0.256763, vali rmse:0.527143\n",
      "\n",
      "Start of epoch 185\n",
      "Training loss (for one batch) at step 0: 0.2357\n",
      "Training epoch: 186, training rmse: 0.256261, vali rmse:0.526473\n",
      "\n",
      "Start of epoch 186\n",
      "Training loss (for one batch) at step 0: 0.2350\n",
      "Training epoch: 187, training rmse: 0.255764, vali rmse:0.525807\n",
      "\n",
      "Start of epoch 187\n",
      "Training loss (for one batch) at step 0: 0.2343\n",
      "Training epoch: 188, training rmse: 0.255273, vali rmse:0.525146\n",
      "\n",
      "Start of epoch 188\n",
      "Training loss (for one batch) at step 0: 0.2336\n",
      "Training epoch: 189, training rmse: 0.254787, vali rmse:0.524490\n",
      "\n",
      "Start of epoch 189\n",
      "Training loss (for one batch) at step 0: 0.2329\n",
      "Training epoch: 190, training rmse: 0.254306, vali rmse:0.523838\n",
      "\n",
      "Start of epoch 190\n",
      "Training loss (for one batch) at step 0: 0.2322\n",
      "Training epoch: 191, training rmse: 0.253831, vali rmse:0.523190\n",
      "\n",
      "Start of epoch 191\n",
      "Training loss (for one batch) at step 0: 0.2316\n",
      "Training epoch: 192, training rmse: 0.253361, vali rmse:0.522547\n",
      "\n",
      "Start of epoch 192\n",
      "Training loss (for one batch) at step 0: 0.2309\n",
      "Training epoch: 193, training rmse: 0.252896, vali rmse:0.521908\n",
      "\n",
      "Start of epoch 193\n",
      "Training loss (for one batch) at step 0: 0.2302\n",
      "Training epoch: 194, training rmse: 0.252435, vali rmse:0.521273\n",
      "\n",
      "Start of epoch 194\n",
      "Training loss (for one batch) at step 0: 0.2296\n",
      "Training epoch: 195, training rmse: 0.251980, vali rmse:0.520643\n",
      "\n",
      "Start of epoch 195\n",
      "Training loss (for one batch) at step 0: 0.2289\n",
      "Training epoch: 196, training rmse: 0.251530, vali rmse:0.520017\n",
      "\n",
      "Start of epoch 196\n",
      "Training loss (for one batch) at step 0: 0.2283\n",
      "Training epoch: 197, training rmse: 0.251084, vali rmse:0.519395\n",
      "\n",
      "Start of epoch 197\n",
      "Training loss (for one batch) at step 0: 0.2277\n",
      "Training epoch: 198, training rmse: 0.250643, vali rmse:0.518777\n",
      "\n",
      "Start of epoch 198\n",
      "Training loss (for one batch) at step 0: 0.2270\n",
      "Training epoch: 199, training rmse: 0.250206, vali rmse:0.518163\n",
      "\n",
      "Start of epoch 199\n",
      "Training loss (for one batch) at step 0: 0.2264\n",
      "Training epoch: 200, training rmse: 0.249774, vali rmse:0.517554\n",
      "\n",
      "Start of epoch 200\n",
      "Training loss (for one batch) at step 0: 0.2258\n",
      "Training epoch: 201, training rmse: 0.249347, vali rmse:0.516948\n",
      "\n",
      "Start of epoch 201\n",
      "Training loss (for one batch) at step 0: 0.2252\n",
      "Training epoch: 202, training rmse: 0.248924, vali rmse:0.516347\n",
      "\n",
      "Start of epoch 202\n",
      "Training loss (for one batch) at step 0: 0.2246\n",
      "Training epoch: 203, training rmse: 0.248505, vali rmse:0.515749\n",
      "\n",
      "Start of epoch 203\n",
      "Training loss (for one batch) at step 0: 0.2240\n",
      "Training epoch: 204, training rmse: 0.248090, vali rmse:0.515156\n",
      "\n",
      "Start of epoch 204\n",
      "Training loss (for one batch) at step 0: 0.2234\n",
      "Training epoch: 205, training rmse: 0.247680, vali rmse:0.514566\n",
      "\n",
      "Start of epoch 205\n",
      "Training loss (for one batch) at step 0: 0.2228\n",
      "Training epoch: 206, training rmse: 0.247274, vali rmse:0.513980\n",
      "\n",
      "Start of epoch 206\n",
      "Training loss (for one batch) at step 0: 0.2222\n",
      "Training epoch: 207, training rmse: 0.246871, vali rmse:0.513398\n",
      "\n",
      "Start of epoch 207\n",
      "Training loss (for one batch) at step 0: 0.2216\n",
      "Training epoch: 208, training rmse: 0.246473, vali rmse:0.512820\n",
      "\n",
      "Start of epoch 208\n",
      "Training loss (for one batch) at step 0: 0.2210\n",
      "Training epoch: 209, training rmse: 0.246079, vali rmse:0.512246\n",
      "\n",
      "Start of epoch 209\n",
      "Training loss (for one batch) at step 0: 0.2205\n",
      "Training epoch: 210, training rmse: 0.245689, vali rmse:0.511676\n",
      "\n",
      "Start of epoch 210\n",
      "Training loss (for one batch) at step 0: 0.2199\n",
      "Training epoch: 211, training rmse: 0.245302, vali rmse:0.511109\n",
      "\n",
      "Start of epoch 211\n",
      "Training loss (for one batch) at step 0: 0.2193\n",
      "Training epoch: 212, training rmse: 0.244919, vali rmse:0.510546\n",
      "\n",
      "Start of epoch 212\n",
      "Training loss (for one batch) at step 0: 0.2188\n",
      "Training epoch: 213, training rmse: 0.244540, vali rmse:0.509987\n",
      "\n",
      "Start of epoch 213\n",
      "Training loss (for one batch) at step 0: 0.2182\n",
      "Training epoch: 214, training rmse: 0.244165, vali rmse:0.509431\n",
      "\n",
      "Start of epoch 214\n",
      "Training loss (for one batch) at step 0: 0.2177\n",
      "Training epoch: 215, training rmse: 0.243793, vali rmse:0.508879\n",
      "\n",
      "Start of epoch 215\n",
      "Training loss (for one batch) at step 0: 0.2171\n",
      "Training epoch: 216, training rmse: 0.243425, vali rmse:0.508331\n",
      "\n",
      "Start of epoch 216\n",
      "Training loss (for one batch) at step 0: 0.2166\n",
      "Training epoch: 217, training rmse: 0.243060, vali rmse:0.507786\n",
      "\n",
      "Start of epoch 217\n",
      "Training loss (for one batch) at step 0: 0.2161\n",
      "Training epoch: 218, training rmse: 0.242699, vali rmse:0.507244\n",
      "\n",
      "Start of epoch 218\n",
      "Training loss (for one batch) at step 0: 0.2155\n",
      "Training epoch: 219, training rmse: 0.242341, vali rmse:0.506707\n",
      "\n",
      "Start of epoch 219\n",
      "Training loss (for one batch) at step 0: 0.2150\n",
      "Training epoch: 220, training rmse: 0.241987, vali rmse:0.506172\n",
      "\n",
      "Start of epoch 220\n",
      "Training loss (for one batch) at step 0: 0.2145\n",
      "Training epoch: 221, training rmse: 0.241636, vali rmse:0.505642\n",
      "\n",
      "Start of epoch 221\n",
      "Training loss (for one batch) at step 0: 0.2140\n",
      "Training epoch: 222, training rmse: 0.241288, vali rmse:0.505114\n",
      "\n",
      "Start of epoch 222\n",
      "Training loss (for one batch) at step 0: 0.2134\n",
      "Training epoch: 223, training rmse: 0.240943, vali rmse:0.504590\n",
      "\n",
      "Start of epoch 223\n",
      "Training loss (for one batch) at step 0: 0.2129\n",
      "Training epoch: 224, training rmse: 0.240602, vali rmse:0.504070\n",
      "\n",
      "Start of epoch 224\n",
      "Training loss (for one batch) at step 0: 0.2124\n",
      "Training epoch: 225, training rmse: 0.240263, vali rmse:0.503553\n",
      "\n",
      "Start of epoch 225\n",
      "Training loss (for one batch) at step 0: 0.2119\n",
      "Training epoch: 226, training rmse: 0.239928, vali rmse:0.503039\n",
      "\n",
      "Start of epoch 226\n",
      "Training loss (for one batch) at step 0: 0.2114\n",
      "Training epoch: 227, training rmse: 0.239596, vali rmse:0.502529\n",
      "\n",
      "Start of epoch 227\n",
      "Training loss (for one batch) at step 0: 0.2109\n",
      "Training epoch: 228, training rmse: 0.239267, vali rmse:0.502022\n",
      "\n",
      "Start of epoch 228\n",
      "Training loss (for one batch) at step 0: 0.2105\n",
      "Training epoch: 229, training rmse: 0.238941, vali rmse:0.501518\n",
      "\n",
      "Start of epoch 229\n",
      "Training loss (for one batch) at step 0: 0.2100\n",
      "Training epoch: 230, training rmse: 0.238617, vali rmse:0.501017\n",
      "\n",
      "Start of epoch 230\n",
      "Training loss (for one batch) at step 0: 0.2095\n",
      "Training epoch: 231, training rmse: 0.238297, vali rmse:0.500520\n",
      "\n",
      "Start of epoch 231\n",
      "Training loss (for one batch) at step 0: 0.2090\n",
      "Training epoch: 232, training rmse: 0.237979, vali rmse:0.500026\n",
      "\n",
      "Start of epoch 232\n",
      "Training loss (for one batch) at step 0: 0.2085\n",
      "Training epoch: 233, training rmse: 0.237665, vali rmse:0.499535\n",
      "\n",
      "Start of epoch 233\n",
      "Training loss (for one batch) at step 0: 0.2081\n",
      "Training epoch: 234, training rmse: 0.237353, vali rmse:0.499047\n",
      "\n",
      "Start of epoch 234\n",
      "Training loss (for one batch) at step 0: 0.2076\n",
      "Training epoch: 235, training rmse: 0.237044, vali rmse:0.498562\n",
      "\n",
      "Start of epoch 235\n",
      "Training loss (for one batch) at step 0: 0.2072\n",
      "Training epoch: 236, training rmse: 0.236737, vali rmse:0.498081\n",
      "\n",
      "Start of epoch 236\n",
      "Training loss (for one batch) at step 0: 0.2067\n",
      "Training epoch: 237, training rmse: 0.236433, vali rmse:0.497602\n",
      "\n",
      "Start of epoch 237\n",
      "Training loss (for one batch) at step 0: 0.2062\n",
      "Training epoch: 238, training rmse: 0.236132, vali rmse:0.497127\n",
      "\n",
      "Start of epoch 238\n",
      "Training loss (for one batch) at step 0: 0.2058\n",
      "Training epoch: 239, training rmse: 0.235833, vali rmse:0.496655\n",
      "\n",
      "Start of epoch 239\n",
      "Training loss (for one batch) at step 0: 0.2053\n",
      "Training epoch: 240, training rmse: 0.235537, vali rmse:0.496185\n",
      "\n",
      "Start of epoch 240\n",
      "Training loss (for one batch) at step 0: 0.2049\n",
      "Training epoch: 241, training rmse: 0.235244, vali rmse:0.495719\n",
      "\n",
      "Start of epoch 241\n",
      "Training loss (for one batch) at step 0: 0.2045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 242, training rmse: 0.234953, vali rmse:0.495256\n",
      "\n",
      "Start of epoch 242\n",
      "Training loss (for one batch) at step 0: 0.2040\n",
      "Training epoch: 243, training rmse: 0.234664, vali rmse:0.494795\n",
      "\n",
      "Start of epoch 243\n",
      "Training loss (for one batch) at step 0: 0.2036\n",
      "Training epoch: 244, training rmse: 0.234378, vali rmse:0.494338\n",
      "\n",
      "Start of epoch 244\n",
      "Training loss (for one batch) at step 0: 0.2032\n",
      "Training epoch: 245, training rmse: 0.234094, vali rmse:0.493884\n",
      "\n",
      "Start of epoch 245\n",
      "Training loss (for one batch) at step 0: 0.2027\n",
      "Training epoch: 246, training rmse: 0.233813, vali rmse:0.493432\n",
      "\n",
      "Start of epoch 246\n",
      "Training loss (for one batch) at step 0: 0.2023\n",
      "Training epoch: 247, training rmse: 0.233534, vali rmse:0.492984\n",
      "\n",
      "Start of epoch 247\n",
      "Training loss (for one batch) at step 0: 0.2019\n",
      "Training epoch: 248, training rmse: 0.233257, vali rmse:0.492538\n",
      "\n",
      "Start of epoch 248\n",
      "Training loss (for one batch) at step 0: 0.2015\n",
      "Training epoch: 249, training rmse: 0.232983, vali rmse:0.492095\n",
      "\n",
      "Start of epoch 249\n",
      "Training loss (for one batch) at step 0: 0.2011\n",
      "Training epoch: 250, training rmse: 0.232711, vali rmse:0.491655\n",
      "\n",
      "Start of epoch 250\n",
      "Training loss (for one batch) at step 0: 0.2007\n",
      "Training epoch: 251, training rmse: 0.232441, vali rmse:0.491218\n",
      "\n",
      "Start of epoch 251\n",
      "Training loss (for one batch) at step 0: 0.2003\n",
      "Training epoch: 252, training rmse: 0.232173, vali rmse:0.490783\n",
      "\n",
      "Start of epoch 252\n",
      "Training loss (for one batch) at step 0: 0.1999\n",
      "Training epoch: 253, training rmse: 0.231908, vali rmse:0.490352\n",
      "\n",
      "Start of epoch 253\n",
      "Training loss (for one batch) at step 0: 0.1995\n",
      "Training epoch: 254, training rmse: 0.231644, vali rmse:0.489923\n",
      "\n",
      "Start of epoch 254\n",
      "Training loss (for one batch) at step 0: 0.1991\n",
      "Training epoch: 255, training rmse: 0.231383, vali rmse:0.489497\n",
      "\n",
      "Start of epoch 255\n",
      "Training loss (for one batch) at step 0: 0.1987\n",
      "Training epoch: 256, training rmse: 0.231124, vali rmse:0.489073\n",
      "\n",
      "Start of epoch 256\n",
      "Training loss (for one batch) at step 0: 0.1983\n",
      "Training epoch: 257, training rmse: 0.230867, vali rmse:0.488652\n",
      "\n",
      "Start of epoch 257\n",
      "Training loss (for one batch) at step 0: 0.1979\n",
      "Training epoch: 258, training rmse: 0.230612, vali rmse:0.488234\n",
      "\n",
      "Start of epoch 258\n",
      "Training loss (for one batch) at step 0: 0.1975\n",
      "Training epoch: 259, training rmse: 0.230358, vali rmse:0.487819\n",
      "\n",
      "Start of epoch 259\n",
      "Training loss (for one batch) at step 0: 0.1971\n",
      "Training epoch: 260, training rmse: 0.230107, vali rmse:0.487406\n",
      "\n",
      "Start of epoch 260\n",
      "Training loss (for one batch) at step 0: 0.1968\n",
      "Training epoch: 261, training rmse: 0.229858, vali rmse:0.486996\n",
      "\n",
      "Start of epoch 261\n",
      "Training loss (for one batch) at step 0: 0.1964\n",
      "Training epoch: 262, training rmse: 0.229611, vali rmse:0.486589\n",
      "\n",
      "Start of epoch 262\n",
      "Training loss (for one batch) at step 0: 0.1960\n",
      "Training epoch: 263, training rmse: 0.229366, vali rmse:0.486184\n",
      "\n",
      "Start of epoch 263\n",
      "Training loss (for one batch) at step 0: 0.1957\n",
      "Training epoch: 264, training rmse: 0.229123, vali rmse:0.485782\n",
      "\n",
      "Start of epoch 264\n",
      "Training loss (for one batch) at step 0: 0.1953\n",
      "Training epoch: 265, training rmse: 0.228881, vali rmse:0.485382\n",
      "\n",
      "Start of epoch 265\n",
      "Training loss (for one batch) at step 0: 0.1949\n",
      "Training epoch: 266, training rmse: 0.228641, vali rmse:0.484985\n",
      "\n",
      "Start of epoch 266\n",
      "Training loss (for one batch) at step 0: 0.1946\n",
      "Training epoch: 267, training rmse: 0.228404, vali rmse:0.484590\n",
      "\n",
      "Start of epoch 267\n",
      "Training loss (for one batch) at step 0: 0.1942\n",
      "Training epoch: 268, training rmse: 0.228168, vali rmse:0.484198\n",
      "\n",
      "Start of epoch 268\n",
      "Training loss (for one batch) at step 0: 0.1939\n",
      "Training epoch: 269, training rmse: 0.227933, vali rmse:0.483808\n",
      "\n",
      "Start of epoch 269\n",
      "Training loss (for one batch) at step 0: 0.1935\n",
      "Training epoch: 270, training rmse: 0.227701, vali rmse:0.483421\n",
      "\n",
      "Start of epoch 270\n",
      "Training loss (for one batch) at step 0: 0.1932\n",
      "Training epoch: 271, training rmse: 0.227470, vali rmse:0.483036\n",
      "\n",
      "Start of epoch 271\n",
      "Training loss (for one batch) at step 0: 0.1928\n",
      "Training epoch: 272, training rmse: 0.227241, vali rmse:0.482654\n",
      "\n",
      "Start of epoch 272\n",
      "Training loss (for one batch) at step 0: 0.1925\n",
      "Training epoch: 273, training rmse: 0.227014, vali rmse:0.482274\n",
      "\n",
      "Start of epoch 273\n",
      "Training loss (for one batch) at step 0: 0.1921\n",
      "Training epoch: 274, training rmse: 0.226788, vali rmse:0.481897\n",
      "\n",
      "Start of epoch 274\n",
      "Training loss (for one batch) at step 0: 0.1918\n",
      "Training epoch: 275, training rmse: 0.226565, vali rmse:0.481522\n",
      "\n",
      "Start of epoch 275\n",
      "Training loss (for one batch) at step 0: 0.1915\n",
      "Training epoch: 276, training rmse: 0.226342, vali rmse:0.481149\n",
      "\n",
      "Start of epoch 276\n",
      "Training loss (for one batch) at step 0: 0.1911\n",
      "Training epoch: 277, training rmse: 0.226122, vali rmse:0.480779\n",
      "\n",
      "Start of epoch 277\n",
      "Training loss (for one batch) at step 0: 0.1908\n",
      "Training epoch: 278, training rmse: 0.225903, vali rmse:0.480411\n",
      "\n",
      "Start of epoch 278\n",
      "Training loss (for one batch) at step 0: 0.1905\n",
      "Training epoch: 279, training rmse: 0.225685, vali rmse:0.480046\n",
      "\n",
      "Start of epoch 279\n",
      "Training loss (for one batch) at step 0: 0.1901\n",
      "Training epoch: 280, training rmse: 0.225469, vali rmse:0.479683\n",
      "\n",
      "Start of epoch 280\n",
      "Training loss (for one batch) at step 0: 0.1898\n",
      "Training epoch: 281, training rmse: 0.225255, vali rmse:0.479322\n",
      "\n",
      "Start of epoch 281\n",
      "Training loss (for one batch) at step 0: 0.1895\n",
      "Training epoch: 282, training rmse: 0.225042, vali rmse:0.478963\n",
      "\n",
      "Start of epoch 282\n",
      "Training loss (for one batch) at step 0: 0.1892\n",
      "Training epoch: 283, training rmse: 0.224831, vali rmse:0.478607\n",
      "\n",
      "Start of epoch 283\n",
      "Training loss (for one batch) at step 0: 0.1889\n",
      "Training epoch: 284, training rmse: 0.224621, vali rmse:0.478253\n",
      "\n",
      "Start of epoch 284\n",
      "Training loss (for one batch) at step 0: 0.1886\n",
      "Training epoch: 285, training rmse: 0.224413, vali rmse:0.477901\n",
      "\n",
      "Start of epoch 285\n",
      "Training loss (for one batch) at step 0: 0.1883\n",
      "Training epoch: 286, training rmse: 0.224206, vali rmse:0.477551\n",
      "\n",
      "Start of epoch 286\n",
      "Training loss (for one batch) at step 0: 0.1879\n",
      "Training epoch: 287, training rmse: 0.224000, vali rmse:0.477204\n",
      "\n",
      "Start of epoch 287\n",
      "Training loss (for one batch) at step 0: 0.1876\n",
      "Training epoch: 288, training rmse: 0.223796, vali rmse:0.476859\n",
      "\n",
      "Start of epoch 288\n",
      "Training loss (for one batch) at step 0: 0.1873\n",
      "Training epoch: 289, training rmse: 0.223594, vali rmse:0.476516\n",
      "\n",
      "Start of epoch 289\n",
      "Training loss (for one batch) at step 0: 0.1870\n",
      "Training epoch: 290, training rmse: 0.223392, vali rmse:0.476175\n",
      "\n",
      "Start of epoch 290\n",
      "Training loss (for one batch) at step 0: 0.1867\n",
      "Training epoch: 291, training rmse: 0.223193, vali rmse:0.475836\n",
      "\n",
      "Start of epoch 291\n",
      "Training loss (for one batch) at step 0: 0.1864\n",
      "Training epoch: 292, training rmse: 0.222994, vali rmse:0.475500\n",
      "\n",
      "Start of epoch 292\n",
      "Training loss (for one batch) at step 0: 0.1861\n",
      "Training epoch: 293, training rmse: 0.222797, vali rmse:0.475165\n",
      "\n",
      "Start of epoch 293\n",
      "Training loss (for one batch) at step 0: 0.1859\n",
      "Training epoch: 294, training rmse: 0.222601, vali rmse:0.474833\n",
      "\n",
      "Start of epoch 294\n",
      "Training loss (for one batch) at step 0: 0.1856\n",
      "Training epoch: 295, training rmse: 0.222407, vali rmse:0.474503\n",
      "\n",
      "Start of epoch 295\n",
      "Training loss (for one batch) at step 0: 0.1853\n",
      "Training epoch: 296, training rmse: 0.222214, vali rmse:0.474175\n",
      "\n",
      "Start of epoch 296\n",
      "Training loss (for one batch) at step 0: 0.1850\n",
      "Training epoch: 297, training rmse: 0.222022, vali rmse:0.473849\n",
      "\n",
      "Start of epoch 297\n",
      "Training loss (for one batch) at step 0: 0.1847\n",
      "Training epoch: 298, training rmse: 0.221831, vali rmse:0.473525\n",
      "\n",
      "Start of epoch 298\n",
      "Training loss (for one batch) at step 0: 0.1844\n",
      "Training epoch: 299, training rmse: 0.221642, vali rmse:0.473203\n",
      "\n",
      "Start of epoch 299\n",
      "Training loss (for one batch) at step 0: 0.1842\n",
      "Training epoch: 300, training rmse: 0.221454, vali rmse:0.472884\n",
      "\n",
      "Start of epoch 300\n",
      "Training loss (for one batch) at step 0: 0.1839\n",
      "Training epoch: 301, training rmse: 0.221267, vali rmse:0.472566\n",
      "\n",
      "Start of epoch 301\n",
      "Training loss (for one batch) at step 0: 0.1836\n",
      "Training epoch: 302, training rmse: 0.221081, vali rmse:0.472250\n",
      "\n",
      "Start of epoch 302\n",
      "Training loss (for one batch) at step 0: 0.1833\n",
      "Training epoch: 303, training rmse: 0.220896, vali rmse:0.471937\n",
      "\n",
      "Start of epoch 303\n",
      "Training loss (for one batch) at step 0: 0.1831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 304, training rmse: 0.220713, vali rmse:0.471625\n",
      "\n",
      "Start of epoch 304\n",
      "Training loss (for one batch) at step 0: 0.1828\n",
      "Training epoch: 305, training rmse: 0.220531, vali rmse:0.471315\n",
      "\n",
      "Start of epoch 305\n",
      "Training loss (for one batch) at step 0: 0.1825\n",
      "Training epoch: 306, training rmse: 0.220350, vali rmse:0.471007\n",
      "\n",
      "Start of epoch 306\n",
      "Training loss (for one batch) at step 0: 0.1823\n",
      "Training epoch: 307, training rmse: 0.220170, vali rmse:0.470701\n",
      "\n",
      "Start of epoch 307\n",
      "Training loss (for one batch) at step 0: 0.1820\n",
      "Training epoch: 308, training rmse: 0.219992, vali rmse:0.470398\n",
      "\n",
      "Start of epoch 308\n",
      "Training loss (for one batch) at step 0: 0.1817\n",
      "Training epoch: 309, training rmse: 0.219814, vali rmse:0.470096\n",
      "\n",
      "Start of epoch 309\n",
      "Training loss (for one batch) at step 0: 0.1815\n",
      "Training epoch: 310, training rmse: 0.219637, vali rmse:0.469796\n",
      "\n",
      "Start of epoch 310\n",
      "Training loss (for one batch) at step 0: 0.1812\n",
      "Training epoch: 311, training rmse: 0.219462, vali rmse:0.469497\n",
      "\n",
      "Start of epoch 311\n",
      "Training loss (for one batch) at step 0: 0.1810\n",
      "Training epoch: 312, training rmse: 0.219288, vali rmse:0.469201\n",
      "\n",
      "Start of epoch 312\n",
      "Training loss (for one batch) at step 0: 0.1807\n",
      "Training epoch: 313, training rmse: 0.219115, vali rmse:0.468907\n",
      "\n",
      "Start of epoch 313\n",
      "Training loss (for one batch) at step 0: 0.1805\n",
      "Training epoch: 314, training rmse: 0.218942, vali rmse:0.468614\n",
      "\n",
      "Start of epoch 314\n",
      "Training loss (for one batch) at step 0: 0.1802\n",
      "Training epoch: 315, training rmse: 0.218771, vali rmse:0.468324\n",
      "\n",
      "Start of epoch 315\n",
      "Training loss (for one batch) at step 0: 0.1800\n",
      "Training epoch: 316, training rmse: 0.218601, vali rmse:0.468035\n",
      "\n",
      "Start of epoch 316\n",
      "Training loss (for one batch) at step 0: 0.1797\n",
      "Training epoch: 317, training rmse: 0.218432, vali rmse:0.467748\n",
      "\n",
      "Start of epoch 317\n",
      "Training loss (for one batch) at step 0: 0.1795\n",
      "Training epoch: 318, training rmse: 0.218264, vali rmse:0.467463\n",
      "\n",
      "Start of epoch 318\n",
      "Training loss (for one batch) at step 0: 0.1792\n",
      "Training epoch: 319, training rmse: 0.218097, vali rmse:0.467179\n",
      "\n",
      "Start of epoch 319\n",
      "Training loss (for one batch) at step 0: 0.1790\n",
      "Training epoch: 320, training rmse: 0.217931, vali rmse:0.466898\n",
      "\n",
      "Start of epoch 320\n",
      "Training loss (for one batch) at step 0: 0.1788\n",
      "Training epoch: 321, training rmse: 0.217766, vali rmse:0.466618\n",
      "\n",
      "Start of epoch 321\n",
      "Training loss (for one batch) at step 0: 0.1785\n",
      "Training epoch: 322, training rmse: 0.217602, vali rmse:0.466340\n",
      "\n",
      "Start of epoch 322\n",
      "Training loss (for one batch) at step 0: 0.1783\n",
      "Training epoch: 323, training rmse: 0.217439, vali rmse:0.466064\n",
      "\n",
      "Start of epoch 323\n",
      "Training loss (for one batch) at step 0: 0.1781\n",
      "Training epoch: 324, training rmse: 0.217276, vali rmse:0.465789\n",
      "\n",
      "Start of epoch 324\n",
      "Training loss (for one batch) at step 0: 0.1778\n",
      "Training epoch: 325, training rmse: 0.217115, vali rmse:0.465516\n",
      "\n",
      "Start of epoch 325\n",
      "Training loss (for one batch) at step 0: 0.1776\n",
      "Training epoch: 326, training rmse: 0.216955, vali rmse:0.465245\n",
      "\n",
      "Start of epoch 326\n",
      "Training loss (for one batch) at step 0: 0.1774\n",
      "Training epoch: 327, training rmse: 0.216795, vali rmse:0.464976\n",
      "\n",
      "Start of epoch 327\n",
      "Training loss (for one batch) at step 0: 0.1771\n",
      "Training epoch: 328, training rmse: 0.216637, vali rmse:0.464708\n",
      "\n",
      "Start of epoch 328\n",
      "Training loss (for one batch) at step 0: 0.1769\n",
      "Training epoch: 329, training rmse: 0.216479, vali rmse:0.464442\n",
      "\n",
      "Start of epoch 329\n",
      "Training loss (for one batch) at step 0: 0.1767\n",
      "Training epoch: 330, training rmse: 0.216322, vali rmse:0.464178\n",
      "\n",
      "Start of epoch 330\n",
      "Training loss (for one batch) at step 0: 0.1765\n",
      "Training epoch: 331, training rmse: 0.216166, vali rmse:0.463916\n",
      "\n",
      "Start of epoch 331\n",
      "Training loss (for one batch) at step 0: 0.1763\n",
      "Training epoch: 332, training rmse: 0.216011, vali rmse:0.463655\n",
      "\n",
      "Start of epoch 332\n",
      "Training loss (for one batch) at step 0: 0.1760\n",
      "Training epoch: 333, training rmse: 0.215857, vali rmse:0.463396\n",
      "\n",
      "Start of epoch 333\n",
      "Training loss (for one batch) at step 0: 0.1758\n",
      "Training epoch: 334, training rmse: 0.215703, vali rmse:0.463138\n",
      "\n",
      "Start of epoch 334\n",
      "Training loss (for one batch) at step 0: 0.1756\n",
      "Training epoch: 335, training rmse: 0.215551, vali rmse:0.462882\n",
      "\n",
      "Start of epoch 335\n",
      "Training loss (for one batch) at step 0: 0.1754\n",
      "Training epoch: 336, training rmse: 0.215399, vali rmse:0.462628\n",
      "\n",
      "Start of epoch 336\n",
      "Training loss (for one batch) at step 0: 0.1752\n",
      "Training epoch: 337, training rmse: 0.215248, vali rmse:0.462375\n",
      "\n",
      "Start of epoch 337\n",
      "Training loss (for one batch) at step 0: 0.1750\n",
      "Training epoch: 338, training rmse: 0.215098, vali rmse:0.462124\n",
      "\n",
      "Start of epoch 338\n",
      "Training loss (for one batch) at step 0: 0.1748\n",
      "Training epoch: 339, training rmse: 0.214949, vali rmse:0.461874\n",
      "\n",
      "Start of epoch 339\n",
      "Training loss (for one batch) at step 0: 0.1746\n",
      "Training epoch: 340, training rmse: 0.214800, vali rmse:0.461626\n",
      "\n",
      "Start of epoch 340\n",
      "Training loss (for one batch) at step 0: 0.1743\n",
      "Training epoch: 341, training rmse: 0.214652, vali rmse:0.461380\n",
      "\n",
      "Start of epoch 341\n",
      "Training loss (for one batch) at step 0: 0.1741\n",
      "Training epoch: 342, training rmse: 0.214505, vali rmse:0.461135\n",
      "\n",
      "Start of epoch 342\n",
      "Training loss (for one batch) at step 0: 0.1739\n",
      "Training epoch: 343, training rmse: 0.214359, vali rmse:0.460892\n",
      "\n",
      "Start of epoch 343\n",
      "Training loss (for one batch) at step 0: 0.1737\n",
      "Training epoch: 344, training rmse: 0.214214, vali rmse:0.460650\n",
      "\n",
      "Start of epoch 344\n",
      "Training loss (for one batch) at step 0: 0.1735\n",
      "Training epoch: 345, training rmse: 0.214069, vali rmse:0.460410\n",
      "\n",
      "Start of epoch 345\n",
      "Training loss (for one batch) at step 0: 0.1733\n",
      "Training epoch: 346, training rmse: 0.213925, vali rmse:0.460171\n",
      "\n",
      "Start of epoch 346\n",
      "Training loss (for one batch) at step 0: 0.1731\n",
      "Training epoch: 347, training rmse: 0.213782, vali rmse:0.459934\n",
      "\n",
      "Start of epoch 347\n",
      "Training loss (for one batch) at step 0: 0.1729\n",
      "Training epoch: 348, training rmse: 0.213639, vali rmse:0.459698\n",
      "\n",
      "Start of epoch 348\n",
      "Training loss (for one batch) at step 0: 0.1728\n",
      "Training epoch: 349, training rmse: 0.213497, vali rmse:0.459464\n",
      "\n",
      "Start of epoch 349\n",
      "Training loss (for one batch) at step 0: 0.1726\n",
      "Training epoch: 350, training rmse: 0.213356, vali rmse:0.459231\n",
      "\n",
      "Start of epoch 350\n",
      "Training loss (for one batch) at step 0: 0.1724\n",
      "Training epoch: 351, training rmse: 0.213216, vali rmse:0.459000\n",
      "\n",
      "Start of epoch 351\n",
      "Training loss (for one batch) at step 0: 0.1722\n",
      "Training epoch: 352, training rmse: 0.213076, vali rmse:0.458770\n",
      "\n",
      "Start of epoch 352\n",
      "Training loss (for one batch) at step 0: 0.1720\n",
      "Training epoch: 353, training rmse: 0.212937, vali rmse:0.458542\n",
      "\n",
      "Start of epoch 353\n",
      "Training loss (for one batch) at step 0: 0.1718\n",
      "Training epoch: 354, training rmse: 0.212798, vali rmse:0.458315\n",
      "\n",
      "Start of epoch 354\n",
      "Training loss (for one batch) at step 0: 0.1716\n",
      "Training epoch: 355, training rmse: 0.212660, vali rmse:0.458090\n",
      "\n",
      "Start of epoch 355\n",
      "Training loss (for one batch) at step 0: 0.1714\n",
      "Training epoch: 356, training rmse: 0.212523, vali rmse:0.457866\n",
      "\n",
      "Start of epoch 356\n",
      "Training loss (for one batch) at step 0: 0.1712\n",
      "Training epoch: 357, training rmse: 0.212387, vali rmse:0.457643\n",
      "\n",
      "Start of epoch 357\n",
      "Training loss (for one batch) at step 0: 0.1711\n",
      "Training epoch: 358, training rmse: 0.212251, vali rmse:0.457422\n",
      "\n",
      "Start of epoch 358\n",
      "Training loss (for one batch) at step 0: 0.1709\n",
      "Training epoch: 359, training rmse: 0.212116, vali rmse:0.457202\n",
      "\n",
      "Start of epoch 359\n",
      "Training loss (for one batch) at step 0: 0.1707\n",
      "Training epoch: 360, training rmse: 0.211981, vali rmse:0.456984\n",
      "\n",
      "Start of epoch 360\n",
      "Training loss (for one batch) at step 0: 0.1705\n",
      "Training epoch: 361, training rmse: 0.211847, vali rmse:0.456767\n",
      "\n",
      "Start of epoch 361\n",
      "Training loss (for one batch) at step 0: 0.1703\n",
      "Training epoch: 362, training rmse: 0.211714, vali rmse:0.456551\n",
      "\n",
      "Start of epoch 362\n",
      "Training loss (for one batch) at step 0: 0.1702\n",
      "Training epoch: 363, training rmse: 0.211581, vali rmse:0.456337\n",
      "\n",
      "Start of epoch 363\n",
      "Training loss (for one batch) at step 0: 0.1700\n",
      "Training epoch: 364, training rmse: 0.211449, vali rmse:0.456124\n",
      "\n",
      "Start of epoch 364\n",
      "Training loss (for one batch) at step 0: 0.1698\n",
      "Training epoch: 365, training rmse: 0.211317, vali rmse:0.455912\n",
      "\n",
      "Start of epoch 365\n",
      "Training loss (for one batch) at step 0: 0.1696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 366, training rmse: 0.211186, vali rmse:0.455702\n",
      "\n",
      "Start of epoch 366\n",
      "Training loss (for one batch) at step 0: 0.1695\n",
      "Training epoch: 367, training rmse: 0.211055, vali rmse:0.455493\n",
      "\n",
      "Start of epoch 367\n",
      "Training loss (for one batch) at step 0: 0.1693\n",
      "Training epoch: 368, training rmse: 0.210926, vali rmse:0.455286\n",
      "\n",
      "Start of epoch 368\n",
      "Training loss (for one batch) at step 0: 0.1691\n",
      "Training epoch: 369, training rmse: 0.210796, vali rmse:0.455079\n",
      "\n",
      "Start of epoch 369\n",
      "Training loss (for one batch) at step 0: 0.1690\n",
      "Training epoch: 370, training rmse: 0.210667, vali rmse:0.454874\n",
      "\n",
      "Start of epoch 370\n",
      "Training loss (for one batch) at step 0: 0.1688\n",
      "Training epoch: 371, training rmse: 0.210539, vali rmse:0.454671\n",
      "\n",
      "Start of epoch 371\n",
      "Training loss (for one batch) at step 0: 0.1686\n",
      "Training epoch: 372, training rmse: 0.210411, vali rmse:0.454468\n",
      "\n",
      "Start of epoch 372\n",
      "Training loss (for one batch) at step 0: 0.1685\n",
      "Training epoch: 373, training rmse: 0.210284, vali rmse:0.454267\n",
      "\n",
      "Start of epoch 373\n",
      "Training loss (for one batch) at step 0: 0.1683\n",
      "Training epoch: 374, training rmse: 0.210158, vali rmse:0.454067\n",
      "\n",
      "Start of epoch 374\n",
      "Training loss (for one batch) at step 0: 0.1681\n",
      "Training epoch: 375, training rmse: 0.210031, vali rmse:0.453869\n",
      "\n",
      "Start of epoch 375\n",
      "Training loss (for one batch) at step 0: 0.1680\n",
      "Training epoch: 376, training rmse: 0.209906, vali rmse:0.453671\n",
      "\n",
      "Start of epoch 376\n",
      "Training loss (for one batch) at step 0: 0.1678\n",
      "Training epoch: 377, training rmse: 0.209781, vali rmse:0.453475\n",
      "\n",
      "Start of epoch 377\n",
      "Training loss (for one batch) at step 0: 0.1677\n",
      "Training epoch: 378, training rmse: 0.209656, vali rmse:0.453281\n",
      "\n",
      "Start of epoch 378\n",
      "Training loss (for one batch) at step 0: 0.1675\n",
      "Training epoch: 379, training rmse: 0.209532, vali rmse:0.453087\n",
      "\n",
      "Start of epoch 379\n",
      "Training loss (for one batch) at step 0: 0.1674\n",
      "Training epoch: 380, training rmse: 0.209408, vali rmse:0.452895\n",
      "\n",
      "Start of epoch 380\n",
      "Training loss (for one batch) at step 0: 0.1672\n",
      "Training epoch: 381, training rmse: 0.209285, vali rmse:0.452703\n",
      "\n",
      "Start of epoch 381\n",
      "Training loss (for one batch) at step 0: 0.1670\n",
      "Training epoch: 382, training rmse: 0.209163, vali rmse:0.452513\n",
      "\n",
      "Start of epoch 382\n",
      "Training loss (for one batch) at step 0: 0.1669\n",
      "Training epoch: 383, training rmse: 0.209040, vali rmse:0.452325\n",
      "\n",
      "Start of epoch 383\n",
      "Training loss (for one batch) at step 0: 0.1667\n",
      "Training epoch: 384, training rmse: 0.208919, vali rmse:0.452137\n",
      "\n",
      "Start of epoch 384\n",
      "Training loss (for one batch) at step 0: 0.1666\n",
      "Training epoch: 385, training rmse: 0.208797, vali rmse:0.451951\n",
      "\n",
      "Start of epoch 385\n",
      "Training loss (for one batch) at step 0: 0.1664\n",
      "Training epoch: 386, training rmse: 0.208677, vali rmse:0.451765\n",
      "\n",
      "Start of epoch 386\n",
      "Training loss (for one batch) at step 0: 0.1663\n",
      "Training epoch: 387, training rmse: 0.208556, vali rmse:0.451581\n",
      "\n",
      "Start of epoch 387\n",
      "Training loss (for one batch) at step 0: 0.1661\n",
      "Training epoch: 388, training rmse: 0.208436, vali rmse:0.451398\n",
      "\n",
      "Start of epoch 388\n",
      "Training loss (for one batch) at step 0: 0.1660\n",
      "Training epoch: 389, training rmse: 0.208317, vali rmse:0.451217\n",
      "\n",
      "Start of epoch 389\n",
      "Training loss (for one batch) at step 0: 0.1658\n",
      "Training epoch: 390, training rmse: 0.208198, vali rmse:0.451036\n",
      "\n",
      "Start of epoch 390\n",
      "Training loss (for one batch) at step 0: 0.1657\n",
      "Training epoch: 391, training rmse: 0.208079, vali rmse:0.450857\n",
      "\n",
      "Start of epoch 391\n",
      "Training loss (for one batch) at step 0: 0.1656\n",
      "Training epoch: 392, training rmse: 0.207961, vali rmse:0.450678\n",
      "\n",
      "Start of epoch 392\n",
      "Training loss (for one batch) at step 0: 0.1654\n",
      "Training epoch: 393, training rmse: 0.207843, vali rmse:0.450501\n",
      "\n",
      "Start of epoch 393\n",
      "Training loss (for one batch) at step 0: 0.1653\n",
      "Training epoch: 394, training rmse: 0.207726, vali rmse:0.450325\n",
      "\n",
      "Start of epoch 394\n",
      "Training loss (for one batch) at step 0: 0.1651\n",
      "Training epoch: 395, training rmse: 0.207609, vali rmse:0.450150\n",
      "\n",
      "Start of epoch 395\n",
      "Training loss (for one batch) at step 0: 0.1650\n",
      "Training epoch: 396, training rmse: 0.207492, vali rmse:0.449976\n",
      "\n",
      "Start of epoch 396\n",
      "Training loss (for one batch) at step 0: 0.1648\n",
      "Training epoch: 397, training rmse: 0.207376, vali rmse:0.449803\n",
      "\n",
      "Start of epoch 397\n",
      "Training loss (for one batch) at step 0: 0.1647\n",
      "Training epoch: 398, training rmse: 0.207260, vali rmse:0.449632\n",
      "\n",
      "Start of epoch 398\n",
      "Training loss (for one batch) at step 0: 0.1646\n",
      "Training epoch: 399, training rmse: 0.207145, vali rmse:0.449461\n",
      "\n",
      "Start of epoch 399\n",
      "Training loss (for one batch) at step 0: 0.1644\n",
      "Training epoch: 400, training rmse: 0.207030, vali rmse:0.449292\n",
      "\n",
      "Start of epoch 400\n",
      "Training loss (for one batch) at step 0: 0.1643\n",
      "Training epoch: 401, training rmse: 0.206915, vali rmse:0.449123\n",
      "\n",
      "Start of epoch 401\n",
      "Training loss (for one batch) at step 0: 0.1642\n",
      "Training epoch: 402, training rmse: 0.206801, vali rmse:0.448956\n",
      "\n",
      "Start of epoch 402\n",
      "Training loss (for one batch) at step 0: 0.1640\n",
      "Training epoch: 403, training rmse: 0.206687, vali rmse:0.448789\n",
      "\n",
      "Start of epoch 403\n",
      "Training loss (for one batch) at step 0: 0.1639\n",
      "Training epoch: 404, training rmse: 0.206573, vali rmse:0.448624\n",
      "\n",
      "Start of epoch 404\n",
      "Training loss (for one batch) at step 0: 0.1638\n",
      "Training epoch: 405, training rmse: 0.206460, vali rmse:0.448460\n",
      "\n",
      "Start of epoch 405\n",
      "Training loss (for one batch) at step 0: 0.1636\n",
      "Training epoch: 406, training rmse: 0.206347, vali rmse:0.448297\n",
      "\n",
      "Start of epoch 406\n",
      "Training loss (for one batch) at step 0: 0.1635\n",
      "Training epoch: 407, training rmse: 0.206235, vali rmse:0.448134\n",
      "\n",
      "Start of epoch 407\n",
      "Training loss (for one batch) at step 0: 0.1634\n",
      "Training epoch: 408, training rmse: 0.206123, vali rmse:0.447973\n",
      "\n",
      "Start of epoch 408\n",
      "Training loss (for one batch) at step 0: 0.1632\n",
      "Training epoch: 409, training rmse: 0.206011, vali rmse:0.447813\n",
      "\n",
      "Start of epoch 409\n",
      "Training loss (for one batch) at step 0: 0.1631\n",
      "Training epoch: 410, training rmse: 0.205900, vali rmse:0.447654\n",
      "\n",
      "Start of epoch 410\n",
      "Training loss (for one batch) at step 0: 0.1630\n",
      "Training epoch: 411, training rmse: 0.205788, vali rmse:0.447496\n",
      "\n",
      "Start of epoch 411\n",
      "Training loss (for one batch) at step 0: 0.1629\n",
      "Training epoch: 412, training rmse: 0.205678, vali rmse:0.447338\n",
      "\n",
      "Start of epoch 412\n",
      "Training loss (for one batch) at step 0: 0.1627\n",
      "Training epoch: 413, training rmse: 0.205567, vali rmse:0.447182\n",
      "\n",
      "Start of epoch 413\n",
      "Training loss (for one batch) at step 0: 0.1626\n",
      "Training epoch: 414, training rmse: 0.205457, vali rmse:0.447027\n",
      "\n",
      "Start of epoch 414\n",
      "Training loss (for one batch) at step 0: 0.1625\n",
      "Training epoch: 415, training rmse: 0.205347, vali rmse:0.446873\n",
      "\n",
      "Start of epoch 415\n",
      "Training loss (for one batch) at step 0: 0.1624\n",
      "Training epoch: 416, training rmse: 0.205238, vali rmse:0.446720\n",
      "\n",
      "Start of epoch 416\n",
      "Training loss (for one batch) at step 0: 0.1622\n",
      "Training epoch: 417, training rmse: 0.205128, vali rmse:0.446567\n",
      "\n",
      "Start of epoch 417\n",
      "Training loss (for one batch) at step 0: 0.1621\n",
      "Training epoch: 418, training rmse: 0.205019, vali rmse:0.446416\n",
      "\n",
      "Start of epoch 418\n",
      "Training loss (for one batch) at step 0: 0.1620\n",
      "Training epoch: 419, training rmse: 0.204911, vali rmse:0.446266\n",
      "\n",
      "Start of epoch 419\n",
      "Training loss (for one batch) at step 0: 0.1619\n",
      "Training epoch: 420, training rmse: 0.204802, vali rmse:0.446116\n",
      "\n",
      "Start of epoch 420\n",
      "Training loss (for one batch) at step 0: 0.1618\n",
      "Training epoch: 421, training rmse: 0.204694, vali rmse:0.445968\n",
      "\n",
      "Start of epoch 421\n",
      "Training loss (for one batch) at step 0: 0.1616\n",
      "Training epoch: 422, training rmse: 0.204587, vali rmse:0.445820\n",
      "\n",
      "Start of epoch 422\n",
      "Training loss (for one batch) at step 0: 0.1615\n",
      "Training epoch: 423, training rmse: 0.204479, vali rmse:0.445673\n",
      "\n",
      "Start of epoch 423\n",
      "Training loss (for one batch) at step 0: 0.1614\n",
      "Training epoch: 424, training rmse: 0.204372, vali rmse:0.445528\n",
      "\n",
      "Start of epoch 424\n",
      "Training loss (for one batch) at step 0: 0.1613\n",
      "Training epoch: 425, training rmse: 0.204265, vali rmse:0.445383\n",
      "\n",
      "Start of epoch 425\n",
      "Training loss (for one batch) at step 0: 0.1612\n",
      "Training epoch: 426, training rmse: 0.204158, vali rmse:0.445239\n",
      "\n",
      "Start of epoch 426\n",
      "Training loss (for one batch) at step 0: 0.1611\n",
      "Training epoch: 427, training rmse: 0.204052, vali rmse:0.445096\n",
      "\n",
      "Start of epoch 427\n",
      "Training loss (for one batch) at step 0: 0.1609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 428, training rmse: 0.203946, vali rmse:0.444954\n",
      "\n",
      "Start of epoch 428\n",
      "Training loss (for one batch) at step 0: 0.1608\n",
      "Training epoch: 429, training rmse: 0.203840, vali rmse:0.444813\n",
      "\n",
      "Start of epoch 429\n",
      "Training loss (for one batch) at step 0: 0.1607\n",
      "Training epoch: 430, training rmse: 0.203734, vali rmse:0.444672\n",
      "\n",
      "Start of epoch 430\n",
      "Training loss (for one batch) at step 0: 0.1606\n",
      "Training epoch: 431, training rmse: 0.203629, vali rmse:0.444533\n",
      "\n",
      "Start of epoch 431\n",
      "Training loss (for one batch) at step 0: 0.1605\n",
      "Training epoch: 432, training rmse: 0.203524, vali rmse:0.444394\n",
      "\n",
      "Start of epoch 432\n",
      "Training loss (for one batch) at step 0: 0.1604\n",
      "Training epoch: 433, training rmse: 0.203419, vali rmse:0.444257\n",
      "\n",
      "Start of epoch 433\n",
      "Training loss (for one batch) at step 0: 0.1603\n",
      "Training epoch: 434, training rmse: 0.203314, vali rmse:0.444120\n",
      "\n",
      "Start of epoch 434\n",
      "Training loss (for one batch) at step 0: 0.1602\n",
      "Training epoch: 435, training rmse: 0.203210, vali rmse:0.443984\n",
      "\n",
      "Start of epoch 435\n",
      "Training loss (for one batch) at step 0: 0.1600\n",
      "Training epoch: 436, training rmse: 0.203106, vali rmse:0.443849\n",
      "\n",
      "Start of epoch 436\n",
      "Training loss (for one batch) at step 0: 0.1599\n",
      "Training epoch: 437, training rmse: 0.203002, vali rmse:0.443714\n",
      "\n",
      "Start of epoch 437\n",
      "Training loss (for one batch) at step 0: 0.1598\n",
      "Training epoch: 438, training rmse: 0.202898, vali rmse:0.443581\n",
      "\n",
      "Start of epoch 438\n",
      "Training loss (for one batch) at step 0: 0.1597\n",
      "Training epoch: 439, training rmse: 0.202794, vali rmse:0.443448\n",
      "\n",
      "Start of epoch 439\n",
      "Training loss (for one batch) at step 0: 0.1596\n",
      "Training epoch: 440, training rmse: 0.202691, vali rmse:0.443316\n",
      "\n",
      "Start of epoch 440\n",
      "Training loss (for one batch) at step 0: 0.1595\n",
      "Training epoch: 441, training rmse: 0.202588, vali rmse:0.443185\n",
      "\n",
      "Start of epoch 441\n",
      "Training loss (for one batch) at step 0: 0.1594\n",
      "Training epoch: 442, training rmse: 0.202485, vali rmse:0.443055\n",
      "\n",
      "Start of epoch 442\n",
      "Training loss (for one batch) at step 0: 0.1593\n",
      "Training epoch: 443, training rmse: 0.202383, vali rmse:0.442926\n",
      "\n",
      "Start of epoch 443\n",
      "Training loss (for one batch) at step 0: 0.1592\n",
      "Training epoch: 444, training rmse: 0.202280, vali rmse:0.442797\n",
      "\n",
      "Start of epoch 444\n",
      "Training loss (for one batch) at step 0: 0.1591\n",
      "Training epoch: 445, training rmse: 0.202178, vali rmse:0.442670\n",
      "\n",
      "Start of epoch 445\n",
      "Training loss (for one batch) at step 0: 0.1590\n",
      "Training epoch: 446, training rmse: 0.202076, vali rmse:0.442543\n",
      "\n",
      "Start of epoch 446\n",
      "Training loss (for one batch) at step 0: 0.1589\n",
      "Training epoch: 447, training rmse: 0.201974, vali rmse:0.442417\n",
      "\n",
      "Start of epoch 447\n",
      "Training loss (for one batch) at step 0: 0.1588\n",
      "Training epoch: 448, training rmse: 0.201872, vali rmse:0.442291\n",
      "\n",
      "Start of epoch 448\n",
      "Training loss (for one batch) at step 0: 0.1587\n",
      "Training epoch: 449, training rmse: 0.201771, vali rmse:0.442167\n",
      "\n",
      "Start of epoch 449\n",
      "Training loss (for one batch) at step 0: 0.1586\n",
      "Training epoch: 450, training rmse: 0.201670, vali rmse:0.442043\n",
      "\n",
      "Start of epoch 450\n",
      "Training loss (for one batch) at step 0: 0.1585\n",
      "Training epoch: 451, training rmse: 0.201569, vali rmse:0.441920\n",
      "\n",
      "Start of epoch 451\n",
      "Training loss (for one batch) at step 0: 0.1584\n",
      "Training epoch: 452, training rmse: 0.201468, vali rmse:0.441798\n",
      "\n",
      "Start of epoch 452\n",
      "Training loss (for one batch) at step 0: 0.1583\n",
      "Training epoch: 453, training rmse: 0.201367, vali rmse:0.441676\n",
      "\n",
      "Start of epoch 453\n",
      "Training loss (for one batch) at step 0: 0.1582\n",
      "Training epoch: 454, training rmse: 0.201267, vali rmse:0.441555\n",
      "\n",
      "Start of epoch 454\n",
      "Training loss (for one batch) at step 0: 0.1581\n",
      "Training epoch: 455, training rmse: 0.201166, vali rmse:0.441435\n",
      "\n",
      "Start of epoch 455\n",
      "Training loss (for one batch) at step 0: 0.1580\n",
      "Training epoch: 456, training rmse: 0.201066, vali rmse:0.441316\n",
      "\n",
      "Start of epoch 456\n",
      "Training loss (for one batch) at step 0: 0.1579\n",
      "Training epoch: 457, training rmse: 0.200966, vali rmse:0.441198\n",
      "\n",
      "Start of epoch 457\n",
      "Training loss (for one batch) at step 0: 0.1578\n",
      "Training epoch: 458, training rmse: 0.200866, vali rmse:0.441080\n",
      "\n",
      "Start of epoch 458\n",
      "Training loss (for one batch) at step 0: 0.1577\n",
      "Training epoch: 459, training rmse: 0.200766, vali rmse:0.440963\n",
      "\n",
      "Start of epoch 459\n",
      "Training loss (for one batch) at step 0: 0.1576\n",
      "Training epoch: 460, training rmse: 0.200667, vali rmse:0.440846\n",
      "\n",
      "Start of epoch 460\n",
      "Training loss (for one batch) at step 0: 0.1575\n",
      "Training epoch: 461, training rmse: 0.200568, vali rmse:0.440731\n",
      "\n",
      "Start of epoch 461\n",
      "Training loss (for one batch) at step 0: 0.1574\n",
      "Training epoch: 462, training rmse: 0.200468, vali rmse:0.440616\n",
      "\n",
      "Start of epoch 462\n",
      "Training loss (for one batch) at step 0: 0.1573\n",
      "Training epoch: 463, training rmse: 0.200369, vali rmse:0.440502\n",
      "\n",
      "Start of epoch 463\n",
      "Training loss (for one batch) at step 0: 0.1572\n",
      "Training epoch: 464, training rmse: 0.200270, vali rmse:0.440388\n",
      "\n",
      "Start of epoch 464\n",
      "Training loss (for one batch) at step 0: 0.1571\n",
      "Training epoch: 465, training rmse: 0.200172, vali rmse:0.440276\n",
      "\n",
      "Start of epoch 465\n",
      "Training loss (for one batch) at step 0: 0.1571\n",
      "Training epoch: 466, training rmse: 0.200073, vali rmse:0.440164\n",
      "\n",
      "Start of epoch 466\n",
      "Training loss (for one batch) at step 0: 0.1570\n",
      "Training epoch: 467, training rmse: 0.199975, vali rmse:0.440052\n",
      "\n",
      "Start of epoch 467\n",
      "Training loss (for one batch) at step 0: 0.1569\n",
      "Training epoch: 468, training rmse: 0.199876, vali rmse:0.439942\n",
      "\n",
      "Start of epoch 468\n",
      "Training loss (for one batch) at step 0: 0.1568\n",
      "Training epoch: 469, training rmse: 0.199778, vali rmse:0.439832\n",
      "\n",
      "Start of epoch 469\n",
      "Training loss (for one batch) at step 0: 0.1567\n",
      "Training epoch: 470, training rmse: 0.199680, vali rmse:0.439722\n",
      "\n",
      "Start of epoch 470\n",
      "Training loss (for one batch) at step 0: 0.1566\n",
      "Training epoch: 471, training rmse: 0.199582, vali rmse:0.439614\n",
      "\n",
      "Start of epoch 471\n",
      "Training loss (for one batch) at step 0: 0.1565\n",
      "Training epoch: 472, training rmse: 0.199484, vali rmse:0.439506\n",
      "\n",
      "Start of epoch 472\n",
      "Training loss (for one batch) at step 0: 0.1564\n",
      "Training epoch: 473, training rmse: 0.199387, vali rmse:0.439399\n",
      "\n",
      "Start of epoch 473\n",
      "Training loss (for one batch) at step 0: 0.1563\n",
      "Training epoch: 474, training rmse: 0.199289, vali rmse:0.439292\n",
      "\n",
      "Start of epoch 474\n",
      "Training loss (for one batch) at step 0: 0.1562\n",
      "Training epoch: 475, training rmse: 0.199192, vali rmse:0.439186\n",
      "\n",
      "Start of epoch 475\n",
      "Training loss (for one batch) at step 0: 0.1562\n",
      "Training epoch: 476, training rmse: 0.199094, vali rmse:0.439081\n",
      "\n",
      "Start of epoch 476\n",
      "Training loss (for one batch) at step 0: 0.1561\n",
      "Training epoch: 477, training rmse: 0.198997, vali rmse:0.438976\n",
      "\n",
      "Start of epoch 477\n",
      "Training loss (for one batch) at step 0: 0.1560\n",
      "Training epoch: 478, training rmse: 0.198900, vali rmse:0.438872\n",
      "\n",
      "Start of epoch 478\n",
      "Training loss (for one batch) at step 0: 0.1559\n",
      "Training epoch: 479, training rmse: 0.198803, vali rmse:0.438769\n",
      "\n",
      "Start of epoch 479\n",
      "Training loss (for one batch) at step 0: 0.1558\n",
      "Training epoch: 480, training rmse: 0.198706, vali rmse:0.438666\n",
      "\n",
      "Start of epoch 480\n",
      "Training loss (for one batch) at step 0: 0.1557\n",
      "Training epoch: 481, training rmse: 0.198610, vali rmse:0.438564\n",
      "\n",
      "Start of epoch 481\n",
      "Training loss (for one batch) at step 0: 0.1556\n",
      "Training epoch: 482, training rmse: 0.198513, vali rmse:0.438463\n",
      "\n",
      "Start of epoch 482\n",
      "Training loss (for one batch) at step 0: 0.1556\n",
      "Training epoch: 483, training rmse: 0.198416, vali rmse:0.438362\n",
      "\n",
      "Start of epoch 483\n",
      "Training loss (for one batch) at step 0: 0.1555\n",
      "Training epoch: 484, training rmse: 0.198320, vali rmse:0.438262\n",
      "\n",
      "Start of epoch 484\n",
      "Training loss (for one batch) at step 0: 0.1554\n",
      "Training epoch: 485, training rmse: 0.198224, vali rmse:0.438162\n",
      "\n",
      "Start of epoch 485\n",
      "Training loss (for one batch) at step 0: 0.1553\n",
      "Training epoch: 486, training rmse: 0.198128, vali rmse:0.438063\n",
      "\n",
      "Start of epoch 486\n",
      "Training loss (for one batch) at step 0: 0.1552\n",
      "Training epoch: 487, training rmse: 0.198031, vali rmse:0.437965\n",
      "\n",
      "Start of epoch 487\n",
      "Training loss (for one batch) at step 0: 0.1552\n",
      "Training epoch: 488, training rmse: 0.197935, vali rmse:0.437867\n",
      "\n",
      "Start of epoch 488\n",
      "Training loss (for one batch) at step 0: 0.1551\n",
      "Training epoch: 489, training rmse: 0.197839, vali rmse:0.437770\n",
      "\n",
      "Start of epoch 489\n",
      "Training loss (for one batch) at step 0: 0.1550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 490, training rmse: 0.197744, vali rmse:0.437673\n",
      "\n",
      "Start of epoch 490\n",
      "Training loss (for one batch) at step 0: 0.1549\n",
      "Training epoch: 491, training rmse: 0.197648, vali rmse:0.437577\n",
      "\n",
      "Start of epoch 491\n",
      "Training loss (for one batch) at step 0: 0.1548\n",
      "Training epoch: 492, training rmse: 0.197552, vali rmse:0.437482\n",
      "\n",
      "Start of epoch 492\n",
      "Training loss (for one batch) at step 0: 0.1548\n",
      "Training epoch: 493, training rmse: 0.197457, vali rmse:0.437387\n",
      "\n",
      "Start of epoch 493\n",
      "Training loss (for one batch) at step 0: 0.1547\n",
      "Training epoch: 494, training rmse: 0.197361, vali rmse:0.437293\n",
      "\n",
      "Start of epoch 494\n",
      "Training loss (for one batch) at step 0: 0.1546\n",
      "Training epoch: 495, training rmse: 0.197266, vali rmse:0.437200\n",
      "\n",
      "Start of epoch 495\n",
      "Training loss (for one batch) at step 0: 0.1545\n",
      "Training epoch: 496, training rmse: 0.197170, vali rmse:0.437107\n",
      "\n",
      "Start of epoch 496\n",
      "Training loss (for one batch) at step 0: 0.1544\n",
      "Training epoch: 497, training rmse: 0.197075, vali rmse:0.437014\n",
      "\n",
      "Start of epoch 497\n",
      "Training loss (for one batch) at step 0: 0.1544\n",
      "Training epoch: 498, training rmse: 0.196980, vali rmse:0.436922\n",
      "\n",
      "Start of epoch 498\n",
      "Training loss (for one batch) at step 0: 0.1543\n",
      "Training epoch: 499, training rmse: 0.196885, vali rmse:0.436831\n",
      "\n",
      "Start of epoch 499\n",
      "Training loss (for one batch) at step 0: 0.1542\n",
      "Training epoch: 500, training rmse: 0.196790, vali rmse:0.436740\n",
      "\n",
      "Start of epoch 500\n",
      "Training loss (for one batch) at step 0: 0.1541\n",
      "Training epoch: 501, training rmse: 0.196695, vali rmse:0.436650\n",
      "\n",
      "Start of epoch 501\n",
      "Training loss (for one batch) at step 0: 0.1541\n",
      "Training epoch: 502, training rmse: 0.196600, vali rmse:0.436560\n",
      "\n",
      "Start of epoch 502\n",
      "Training loss (for one batch) at step 0: 0.1540\n",
      "Training epoch: 503, training rmse: 0.196505, vali rmse:0.436471\n",
      "\n",
      "Start of epoch 503\n",
      "Training loss (for one batch) at step 0: 0.1539\n",
      "Training epoch: 504, training rmse: 0.196410, vali rmse:0.436383\n",
      "\n",
      "Start of epoch 504\n",
      "Training loss (for one batch) at step 0: 0.1538\n",
      "Training epoch: 505, training rmse: 0.196315, vali rmse:0.436295\n",
      "\n",
      "Start of epoch 505\n",
      "Training loss (for one batch) at step 0: 0.1537\n",
      "Training epoch: 506, training rmse: 0.196221, vali rmse:0.436207\n",
      "\n",
      "Start of epoch 506\n",
      "Training loss (for one batch) at step 0: 0.1537\n",
      "Training epoch: 507, training rmse: 0.196126, vali rmse:0.436120\n",
      "\n",
      "Start of epoch 507\n",
      "Training loss (for one batch) at step 0: 0.1536\n",
      "Training epoch: 508, training rmse: 0.196031, vali rmse:0.436034\n",
      "\n",
      "Start of epoch 508\n",
      "Training loss (for one batch) at step 0: 0.1535\n",
      "Training epoch: 509, training rmse: 0.195937, vali rmse:0.435948\n",
      "\n",
      "Start of epoch 509\n",
      "Training loss (for one batch) at step 0: 0.1535\n",
      "Training epoch: 510, training rmse: 0.195842, vali rmse:0.435862\n",
      "\n",
      "Start of epoch 510\n",
      "Training loss (for one batch) at step 0: 0.1534\n",
      "Training epoch: 511, training rmse: 0.195748, vali rmse:0.435778\n",
      "\n",
      "Start of epoch 511\n",
      "Training loss (for one batch) at step 0: 0.1533\n",
      "Training epoch: 512, training rmse: 0.195654, vali rmse:0.435693\n",
      "\n",
      "Start of epoch 512\n",
      "Training loss (for one batch) at step 0: 0.1532\n",
      "Training epoch: 513, training rmse: 0.195559, vali rmse:0.435609\n",
      "\n",
      "Start of epoch 513\n",
      "Training loss (for one batch) at step 0: 0.1532\n",
      "Training epoch: 514, training rmse: 0.195465, vali rmse:0.435526\n",
      "\n",
      "Start of epoch 514\n",
      "Training loss (for one batch) at step 0: 0.1531\n",
      "Training epoch: 515, training rmse: 0.195371, vali rmse:0.435443\n",
      "\n",
      "Start of epoch 515\n",
      "Training loss (for one batch) at step 0: 0.1530\n",
      "Training epoch: 516, training rmse: 0.195277, vali rmse:0.435361\n",
      "\n",
      "Start of epoch 516\n",
      "Training loss (for one batch) at step 0: 0.1529\n",
      "Training epoch: 517, training rmse: 0.195183, vali rmse:0.435279\n",
      "\n",
      "Start of epoch 517\n",
      "Training loss (for one batch) at step 0: 0.1529\n",
      "Training epoch: 518, training rmse: 0.195089, vali rmse:0.435198\n",
      "\n",
      "Start of epoch 518\n",
      "Training loss (for one batch) at step 0: 0.1528\n",
      "Training epoch: 519, training rmse: 0.194994, vali rmse:0.435117\n",
      "\n",
      "Start of epoch 519\n",
      "Training loss (for one batch) at step 0: 0.1527\n",
      "Training epoch: 520, training rmse: 0.194900, vali rmse:0.435037\n",
      "\n",
      "Start of epoch 520\n",
      "Training loss (for one batch) at step 0: 0.1527\n",
      "Training epoch: 521, training rmse: 0.194806, vali rmse:0.434957\n",
      "\n",
      "Start of epoch 521\n",
      "Training loss (for one batch) at step 0: 0.1526\n",
      "Training epoch: 522, training rmse: 0.194712, vali rmse:0.434878\n",
      "\n",
      "Start of epoch 522\n",
      "Training loss (for one batch) at step 0: 0.1525\n",
      "Training epoch: 523, training rmse: 0.194619, vali rmse:0.434799\n",
      "\n",
      "Start of epoch 523\n",
      "Training loss (for one batch) at step 0: 0.1525\n",
      "Training epoch: 524, training rmse: 0.194525, vali rmse:0.434721\n",
      "\n",
      "Start of epoch 524\n",
      "Training loss (for one batch) at step 0: 0.1524\n",
      "Training epoch: 525, training rmse: 0.194431, vali rmse:0.434643\n",
      "\n",
      "Start of epoch 525\n",
      "Training loss (for one batch) at step 0: 0.1523\n",
      "Training epoch: 526, training rmse: 0.194337, vali rmse:0.434565\n",
      "\n",
      "Start of epoch 526\n",
      "Training loss (for one batch) at step 0: 0.1523\n",
      "Training epoch: 527, training rmse: 0.194243, vali rmse:0.434488\n",
      "\n",
      "Start of epoch 527\n",
      "Training loss (for one batch) at step 0: 0.1522\n",
      "Training epoch: 528, training rmse: 0.194149, vali rmse:0.434412\n",
      "\n",
      "Start of epoch 528\n",
      "Training loss (for one batch) at step 0: 0.1521\n",
      "Training epoch: 529, training rmse: 0.194056, vali rmse:0.434336\n",
      "\n",
      "Start of epoch 529\n",
      "Training loss (for one batch) at step 0: 0.1520\n",
      "Training epoch: 530, training rmse: 0.193962, vali rmse:0.434260\n",
      "\n",
      "Start of epoch 530\n",
      "Training loss (for one batch) at step 0: 0.1520\n",
      "Training epoch: 531, training rmse: 0.193868, vali rmse:0.434185\n",
      "\n",
      "Start of epoch 531\n",
      "Training loss (for one batch) at step 0: 0.1519\n",
      "Training epoch: 532, training rmse: 0.193774, vali rmse:0.434111\n",
      "\n",
      "Start of epoch 532\n",
      "Training loss (for one batch) at step 0: 0.1518\n",
      "Training epoch: 533, training rmse: 0.193681, vali rmse:0.434036\n",
      "\n",
      "Start of epoch 533\n",
      "Training loss (for one batch) at step 0: 0.1518\n",
      "Training epoch: 534, training rmse: 0.193587, vali rmse:0.433963\n",
      "\n",
      "Start of epoch 534\n",
      "Training loss (for one batch) at step 0: 0.1517\n",
      "Training epoch: 535, training rmse: 0.193493, vali rmse:0.433889\n",
      "\n",
      "Start of epoch 535\n",
      "Training loss (for one batch) at step 0: 0.1517\n",
      "Training epoch: 536, training rmse: 0.193400, vali rmse:0.433817\n",
      "\n",
      "Start of epoch 536\n",
      "Training loss (for one batch) at step 0: 0.1516\n",
      "Training epoch: 537, training rmse: 0.193306, vali rmse:0.433744\n",
      "\n",
      "Start of epoch 537\n",
      "Training loss (for one batch) at step 0: 0.1515\n",
      "Training epoch: 538, training rmse: 0.193213, vali rmse:0.433672\n",
      "\n",
      "Start of epoch 538\n",
      "Training loss (for one batch) at step 0: 0.1515\n",
      "Training epoch: 539, training rmse: 0.193119, vali rmse:0.433601\n",
      "\n",
      "Start of epoch 539\n",
      "Training loss (for one batch) at step 0: 0.1514\n",
      "Training epoch: 540, training rmse: 0.193025, vali rmse:0.433530\n",
      "\n",
      "Start of epoch 540\n",
      "Training loss (for one batch) at step 0: 0.1513\n",
      "Training epoch: 541, training rmse: 0.192932, vali rmse:0.433459\n",
      "\n",
      "Start of epoch 541\n",
      "Training loss (for one batch) at step 0: 0.1513\n",
      "Training epoch: 542, training rmse: 0.192838, vali rmse:0.433389\n",
      "\n",
      "Start of epoch 542\n",
      "Training loss (for one batch) at step 0: 0.1512\n",
      "Training epoch: 543, training rmse: 0.192745, vali rmse:0.433319\n",
      "\n",
      "Start of epoch 543\n",
      "Training loss (for one batch) at step 0: 0.1511\n",
      "Training epoch: 544, training rmse: 0.192651, vali rmse:0.433249\n",
      "\n",
      "Start of epoch 544\n",
      "Training loss (for one batch) at step 0: 0.1511\n",
      "Training epoch: 545, training rmse: 0.192557, vali rmse:0.433180\n",
      "\n",
      "Start of epoch 545\n",
      "Training loss (for one batch) at step 0: 0.1510\n",
      "Training epoch: 546, training rmse: 0.192464, vali rmse:0.433112\n",
      "\n",
      "Start of epoch 546\n",
      "Training loss (for one batch) at step 0: 0.1509\n",
      "Training epoch: 547, training rmse: 0.192370, vali rmse:0.433044\n",
      "\n",
      "Start of epoch 547\n",
      "Training loss (for one batch) at step 0: 0.1509\n",
      "Training epoch: 548, training rmse: 0.192277, vali rmse:0.432976\n",
      "\n",
      "Start of epoch 548\n",
      "Training loss (for one batch) at step 0: 0.1508\n",
      "Training epoch: 549, training rmse: 0.192183, vali rmse:0.432909\n",
      "\n",
      "Start of epoch 549\n",
      "Training loss (for one batch) at step 0: 0.1508\n",
      "Training epoch: 550, training rmse: 0.192089, vali rmse:0.432842\n",
      "\n",
      "Start of epoch 550\n",
      "Training loss (for one batch) at step 0: 0.1507\n",
      "Training epoch: 551, training rmse: 0.191996, vali rmse:0.432775\n",
      "\n",
      "Start of epoch 551\n",
      "Training loss (for one batch) at step 0: 0.1506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 552, training rmse: 0.191902, vali rmse:0.432709\n",
      "\n",
      "Start of epoch 552\n",
      "Training loss (for one batch) at step 0: 0.1506\n",
      "Training epoch: 553, training rmse: 0.191809, vali rmse:0.432643\n",
      "\n",
      "Start of epoch 553\n",
      "Training loss (for one batch) at step 0: 0.1505\n",
      "Training epoch: 554, training rmse: 0.191715, vali rmse:0.432578\n",
      "\n",
      "Start of epoch 554\n",
      "Training loss (for one batch) at step 0: 0.1504\n",
      "Training epoch: 555, training rmse: 0.191621, vali rmse:0.432513\n",
      "\n",
      "Start of epoch 555\n",
      "Training loss (for one batch) at step 0: 0.1504\n",
      "Training epoch: 556, training rmse: 0.191528, vali rmse:0.432448\n",
      "\n",
      "Start of epoch 556\n",
      "Training loss (for one batch) at step 0: 0.1503\n",
      "Training epoch: 557, training rmse: 0.191434, vali rmse:0.432384\n",
      "\n",
      "Start of epoch 557\n",
      "Training loss (for one batch) at step 0: 0.1503\n",
      "Training epoch: 558, training rmse: 0.191340, vali rmse:0.432320\n",
      "\n",
      "Start of epoch 558\n",
      "Training loss (for one batch) at step 0: 0.1502\n",
      "Training epoch: 559, training rmse: 0.191247, vali rmse:0.432257\n",
      "\n",
      "Start of epoch 559\n",
      "Training loss (for one batch) at step 0: 0.1501\n",
      "Training epoch: 560, training rmse: 0.191153, vali rmse:0.432194\n",
      "\n",
      "Start of epoch 560\n",
      "Training loss (for one batch) at step 0: 0.1501\n",
      "Training epoch: 561, training rmse: 0.191059, vali rmse:0.432131\n",
      "\n",
      "Start of epoch 561\n",
      "Training loss (for one batch) at step 0: 0.1500\n",
      "Training epoch: 562, training rmse: 0.190966, vali rmse:0.432069\n",
      "\n",
      "Start of epoch 562\n",
      "Training loss (for one batch) at step 0: 0.1500\n",
      "Training epoch: 563, training rmse: 0.190872, vali rmse:0.432007\n",
      "\n",
      "Start of epoch 563\n",
      "Training loss (for one batch) at step 0: 0.1499\n",
      "Training epoch: 564, training rmse: 0.190778, vali rmse:0.431945\n",
      "\n",
      "Start of epoch 564\n",
      "Training loss (for one batch) at step 0: 0.1498\n",
      "Training epoch: 565, training rmse: 0.190684, vali rmse:0.431884\n",
      "\n",
      "Start of epoch 565\n",
      "Training loss (for one batch) at step 0: 0.1498\n",
      "Training epoch: 566, training rmse: 0.190591, vali rmse:0.431823\n",
      "\n",
      "Start of epoch 566\n",
      "Training loss (for one batch) at step 0: 0.1497\n",
      "Training epoch: 567, training rmse: 0.190497, vali rmse:0.431763\n",
      "\n",
      "Start of epoch 567\n",
      "Training loss (for one batch) at step 0: 0.1497\n",
      "Training epoch: 568, training rmse: 0.190403, vali rmse:0.431703\n",
      "\n",
      "Start of epoch 568\n",
      "Training loss (for one batch) at step 0: 0.1496\n",
      "Training epoch: 569, training rmse: 0.190309, vali rmse:0.431643\n",
      "\n",
      "Start of epoch 569\n",
      "Training loss (for one batch) at step 0: 0.1495\n",
      "Training epoch: 570, training rmse: 0.190215, vali rmse:0.431583\n",
      "\n",
      "Start of epoch 570\n",
      "Training loss (for one batch) at step 0: 0.1495\n",
      "Training epoch: 571, training rmse: 0.190121, vali rmse:0.431524\n",
      "\n",
      "Start of epoch 571\n",
      "Training loss (for one batch) at step 0: 0.1494\n",
      "Training epoch: 572, training rmse: 0.190027, vali rmse:0.431466\n",
      "\n",
      "Start of epoch 572\n",
      "Training loss (for one batch) at step 0: 0.1494\n",
      "Training epoch: 573, training rmse: 0.189933, vali rmse:0.431407\n",
      "\n",
      "Start of epoch 573\n",
      "Training loss (for one batch) at step 0: 0.1493\n",
      "Training epoch: 574, training rmse: 0.189839, vali rmse:0.431349\n",
      "\n",
      "Start of epoch 574\n",
      "Training loss (for one batch) at step 0: 0.1493\n",
      "Training epoch: 575, training rmse: 0.189745, vali rmse:0.431292\n",
      "\n",
      "Start of epoch 575\n",
      "Training loss (for one batch) at step 0: 0.1492\n",
      "Training epoch: 576, training rmse: 0.189651, vali rmse:0.431234\n",
      "\n",
      "Start of epoch 576\n",
      "Training loss (for one batch) at step 0: 0.1491\n",
      "Training epoch: 577, training rmse: 0.189557, vali rmse:0.431177\n",
      "\n",
      "Start of epoch 577\n",
      "Training loss (for one batch) at step 0: 0.1491\n",
      "Training epoch: 578, training rmse: 0.189463, vali rmse:0.431121\n",
      "\n",
      "Start of epoch 578\n",
      "Training loss (for one batch) at step 0: 0.1490\n",
      "Training epoch: 579, training rmse: 0.189369, vali rmse:0.431064\n",
      "\n",
      "Start of epoch 579\n",
      "Training loss (for one batch) at step 0: 0.1490\n",
      "Training epoch: 580, training rmse: 0.189275, vali rmse:0.431008\n",
      "\n",
      "Start of epoch 580\n",
      "Training loss (for one batch) at step 0: 0.1489\n",
      "Training epoch: 581, training rmse: 0.189180, vali rmse:0.430953\n",
      "\n",
      "Start of epoch 581\n",
      "Training loss (for one batch) at step 0: 0.1489\n",
      "Training epoch: 582, training rmse: 0.189086, vali rmse:0.430897\n",
      "\n",
      "Start of epoch 582\n",
      "Training loss (for one batch) at step 0: 0.1488\n",
      "Training epoch: 583, training rmse: 0.188992, vali rmse:0.430842\n",
      "\n",
      "Start of epoch 583\n",
      "Training loss (for one batch) at step 0: 0.1487\n",
      "Training epoch: 584, training rmse: 0.188897, vali rmse:0.430788\n",
      "\n",
      "Start of epoch 584\n",
      "Training loss (for one batch) at step 0: 0.1487\n",
      "Training epoch: 585, training rmse: 0.188803, vali rmse:0.430733\n",
      "\n",
      "Start of epoch 585\n",
      "Training loss (for one batch) at step 0: 0.1486\n",
      "Training epoch: 586, training rmse: 0.188709, vali rmse:0.430679\n",
      "\n",
      "Start of epoch 586\n",
      "Training loss (for one batch) at step 0: 0.1486\n",
      "Training epoch: 587, training rmse: 0.188614, vali rmse:0.430625\n",
      "\n",
      "Start of epoch 587\n",
      "Training loss (for one batch) at step 0: 0.1485\n",
      "Training epoch: 588, training rmse: 0.188520, vali rmse:0.430572\n",
      "\n",
      "Start of epoch 588\n",
      "Training loss (for one batch) at step 0: 0.1485\n",
      "Training epoch: 589, training rmse: 0.188425, vali rmse:0.430519\n",
      "\n",
      "Start of epoch 589\n",
      "Training loss (for one batch) at step 0: 0.1484\n",
      "Training epoch: 590, training rmse: 0.188330, vali rmse:0.430466\n",
      "\n",
      "Start of epoch 590\n",
      "Training loss (for one batch) at step 0: 0.1484\n",
      "Training epoch: 591, training rmse: 0.188236, vali rmse:0.430414\n",
      "\n",
      "Start of epoch 591\n",
      "Training loss (for one batch) at step 0: 0.1483\n",
      "Training epoch: 592, training rmse: 0.188141, vali rmse:0.430361\n",
      "\n",
      "Start of epoch 592\n",
      "Training loss (for one batch) at step 0: 0.1483\n",
      "Training epoch: 593, training rmse: 0.188046, vali rmse:0.430310\n",
      "\n",
      "Start of epoch 593\n",
      "Training loss (for one batch) at step 0: 0.1482\n",
      "Training epoch: 594, training rmse: 0.187951, vali rmse:0.430258\n",
      "\n",
      "Start of epoch 594\n",
      "Training loss (for one batch) at step 0: 0.1481\n",
      "Training epoch: 595, training rmse: 0.187857, vali rmse:0.430207\n",
      "\n",
      "Start of epoch 595\n",
      "Training loss (for one batch) at step 0: 0.1481\n",
      "Training epoch: 596, training rmse: 0.187762, vali rmse:0.430156\n",
      "\n",
      "Start of epoch 596\n",
      "Training loss (for one batch) at step 0: 0.1480\n",
      "Training epoch: 597, training rmse: 0.187667, vali rmse:0.430105\n",
      "\n",
      "Start of epoch 597\n",
      "Training loss (for one batch) at step 0: 0.1480\n",
      "Training epoch: 598, training rmse: 0.187572, vali rmse:0.430055\n",
      "\n",
      "Start of epoch 598\n",
      "Training loss (for one batch) at step 0: 0.1479\n",
      "Training epoch: 599, training rmse: 0.187477, vali rmse:0.430005\n",
      "\n",
      "Start of epoch 599\n",
      "Training loss (for one batch) at step 0: 0.1479\n",
      "Training epoch: 600, training rmse: 0.187382, vali rmse:0.429955\n",
      "\n",
      "Start of epoch 600\n",
      "Training loss (for one batch) at step 0: 0.1478\n",
      "Training epoch: 601, training rmse: 0.187286, vali rmse:0.429905\n",
      "\n",
      "Start of epoch 601\n",
      "Training loss (for one batch) at step 0: 0.1478\n",
      "Training epoch: 602, training rmse: 0.187191, vali rmse:0.429856\n",
      "\n",
      "Start of epoch 602\n",
      "Training loss (for one batch) at step 0: 0.1477\n",
      "Training epoch: 603, training rmse: 0.187096, vali rmse:0.429807\n",
      "\n",
      "Start of epoch 603\n",
      "Training loss (for one batch) at step 0: 0.1477\n",
      "Training epoch: 604, training rmse: 0.187001, vali rmse:0.429758\n",
      "\n",
      "Start of epoch 604\n",
      "Training loss (for one batch) at step 0: 0.1476\n",
      "Training epoch: 605, training rmse: 0.186905, vali rmse:0.429710\n",
      "\n",
      "Start of epoch 605\n",
      "Training loss (for one batch) at step 0: 0.1475\n",
      "Training epoch: 606, training rmse: 0.186810, vali rmse:0.429662\n",
      "\n",
      "Start of epoch 606\n",
      "Training loss (for one batch) at step 0: 0.1475\n",
      "Training epoch: 607, training rmse: 0.186714, vali rmse:0.429614\n",
      "\n",
      "Start of epoch 607\n",
      "Training loss (for one batch) at step 0: 0.1474\n",
      "Training epoch: 608, training rmse: 0.186619, vali rmse:0.429567\n",
      "\n",
      "Start of epoch 608\n",
      "Training loss (for one batch) at step 0: 0.1474\n",
      "Training epoch: 609, training rmse: 0.186523, vali rmse:0.429519\n",
      "\n",
      "Start of epoch 609\n",
      "Training loss (for one batch) at step 0: 0.1473\n",
      "Training epoch: 610, training rmse: 0.186428, vali rmse:0.429472\n",
      "\n",
      "Start of epoch 610\n",
      "Training loss (for one batch) at step 0: 0.1473\n",
      "Training epoch: 611, training rmse: 0.186332, vali rmse:0.429426\n",
      "\n",
      "Start of epoch 611\n",
      "Training loss (for one batch) at step 0: 0.1472\n",
      "Training epoch: 612, training rmse: 0.186236, vali rmse:0.429379\n",
      "\n",
      "Start of epoch 612\n",
      "Training loss (for one batch) at step 0: 0.1472\n",
      "Training epoch: 613, training rmse: 0.186140, vali rmse:0.429333\n",
      "\n",
      "Start of epoch 613\n",
      "Training loss (for one batch) at step 0: 0.1471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 614, training rmse: 0.186045, vali rmse:0.429287\n",
      "\n",
      "Start of epoch 614\n",
      "Training loss (for one batch) at step 0: 0.1471\n",
      "Training epoch: 615, training rmse: 0.185949, vali rmse:0.429241\n",
      "\n",
      "Start of epoch 615\n",
      "Training loss (for one batch) at step 0: 0.1470\n",
      "Training epoch: 616, training rmse: 0.185853, vali rmse:0.429196\n",
      "\n",
      "Start of epoch 616\n",
      "Training loss (for one batch) at step 0: 0.1470\n",
      "Training epoch: 617, training rmse: 0.185757, vali rmse:0.429151\n",
      "\n",
      "Start of epoch 617\n",
      "Training loss (for one batch) at step 0: 0.1469\n",
      "Training epoch: 618, training rmse: 0.185660, vali rmse:0.429106\n",
      "\n",
      "Start of epoch 618\n",
      "Training loss (for one batch) at step 0: 0.1469\n",
      "Training epoch: 619, training rmse: 0.185564, vali rmse:0.429061\n",
      "\n",
      "Start of epoch 619\n",
      "Training loss (for one batch) at step 0: 0.1468\n",
      "Training epoch: 620, training rmse: 0.185468, vali rmse:0.429017\n",
      "\n",
      "Start of epoch 620\n",
      "Training loss (for one batch) at step 0: 0.1468\n",
      "Training epoch: 621, training rmse: 0.185372, vali rmse:0.428973\n",
      "\n",
      "Start of epoch 621\n",
      "Training loss (for one batch) at step 0: 0.1467\n",
      "Training epoch: 622, training rmse: 0.185275, vali rmse:0.428929\n",
      "\n",
      "Start of epoch 622\n",
      "Training loss (for one batch) at step 0: 0.1467\n",
      "Training epoch: 623, training rmse: 0.185179, vali rmse:0.428885\n",
      "\n",
      "Start of epoch 623\n",
      "Training loss (for one batch) at step 0: 0.1466\n",
      "Training epoch: 624, training rmse: 0.185082, vali rmse:0.428842\n",
      "\n",
      "Start of epoch 624\n",
      "Training loss (for one batch) at step 0: 0.1466\n",
      "Training epoch: 625, training rmse: 0.184986, vali rmse:0.428799\n",
      "\n",
      "Start of epoch 625\n",
      "Training loss (for one batch) at step 0: 0.1465\n",
      "Training epoch: 626, training rmse: 0.184889, vali rmse:0.428756\n",
      "\n",
      "Start of epoch 626\n",
      "Training loss (for one batch) at step 0: 0.1465\n",
      "Training epoch: 627, training rmse: 0.184792, vali rmse:0.428713\n",
      "\n",
      "Start of epoch 627\n",
      "Training loss (for one batch) at step 0: 0.1464\n",
      "Training epoch: 628, training rmse: 0.184695, vali rmse:0.428671\n",
      "\n",
      "Start of epoch 628\n",
      "Training loss (for one batch) at step 0: 0.1464\n",
      "Training epoch: 629, training rmse: 0.184599, vali rmse:0.428629\n",
      "\n",
      "Start of epoch 629\n",
      "Training loss (for one batch) at step 0: 0.1463\n",
      "Training epoch: 630, training rmse: 0.184502, vali rmse:0.428587\n",
      "\n",
      "Start of epoch 630\n",
      "Training loss (for one batch) at step 0: 0.1463\n",
      "Training epoch: 631, training rmse: 0.184405, vali rmse:0.428545\n",
      "\n",
      "Start of epoch 631\n",
      "Training loss (for one batch) at step 0: 0.1462\n",
      "Training epoch: 632, training rmse: 0.184308, vali rmse:0.428504\n",
      "\n",
      "Start of epoch 632\n",
      "Training loss (for one batch) at step 0: 0.1461\n",
      "Training epoch: 633, training rmse: 0.184210, vali rmse:0.428462\n",
      "\n",
      "Start of epoch 633\n",
      "Training loss (for one batch) at step 0: 0.1461\n",
      "Training epoch: 634, training rmse: 0.184113, vali rmse:0.428421\n",
      "\n",
      "Start of epoch 634\n",
      "Training loss (for one batch) at step 0: 0.1460\n",
      "Training epoch: 635, training rmse: 0.184016, vali rmse:0.428381\n",
      "\n",
      "Start of epoch 635\n",
      "Training loss (for one batch) at step 0: 0.1460\n",
      "Training epoch: 636, training rmse: 0.183918, vali rmse:0.428340\n",
      "\n",
      "Start of epoch 636\n",
      "Training loss (for one batch) at step 0: 0.1459\n",
      "Training epoch: 637, training rmse: 0.183821, vali rmse:0.428300\n",
      "\n",
      "Start of epoch 637\n",
      "Training loss (for one batch) at step 0: 0.1459\n",
      "Training epoch: 638, training rmse: 0.183723, vali rmse:0.428260\n",
      "\n",
      "Start of epoch 638\n",
      "Training loss (for one batch) at step 0: 0.1458\n",
      "Training epoch: 639, training rmse: 0.183626, vali rmse:0.428220\n",
      "\n",
      "Start of epoch 639\n",
      "Training loss (for one batch) at step 0: 0.1458\n",
      "Training epoch: 640, training rmse: 0.183528, vali rmse:0.428180\n",
      "\n",
      "Start of epoch 640\n",
      "Training loss (for one batch) at step 0: 0.1457\n",
      "Training epoch: 641, training rmse: 0.183430, vali rmse:0.428141\n",
      "\n",
      "Start of epoch 641\n",
      "Training loss (for one batch) at step 0: 0.1457\n",
      "Training epoch: 642, training rmse: 0.183332, vali rmse:0.428102\n",
      "\n",
      "Start of epoch 642\n",
      "Training loss (for one batch) at step 0: 0.1456\n",
      "Training epoch: 643, training rmse: 0.183234, vali rmse:0.428063\n",
      "\n",
      "Start of epoch 643\n",
      "Training loss (for one batch) at step 0: 0.1456\n",
      "Training epoch: 644, training rmse: 0.183136, vali rmse:0.428024\n",
      "\n",
      "Start of epoch 644\n",
      "Training loss (for one batch) at step 0: 0.1455\n",
      "Training epoch: 645, training rmse: 0.183038, vali rmse:0.427985\n",
      "\n",
      "Start of epoch 645\n",
      "Training loss (for one batch) at step 0: 0.1455\n",
      "Training epoch: 646, training rmse: 0.182940, vali rmse:0.427947\n",
      "\n",
      "Start of epoch 646\n",
      "Training loss (for one batch) at step 0: 0.1454\n",
      "Training epoch: 647, training rmse: 0.182842, vali rmse:0.427909\n",
      "\n",
      "Start of epoch 647\n",
      "Training loss (for one batch) at step 0: 0.1454\n",
      "Training epoch: 648, training rmse: 0.182744, vali rmse:0.427871\n",
      "\n",
      "Start of epoch 648\n",
      "Training loss (for one batch) at step 0: 0.1453\n",
      "Training epoch: 649, training rmse: 0.182645, vali rmse:0.427833\n",
      "\n",
      "Start of epoch 649\n",
      "Training loss (for one batch) at step 0: 0.1453\n",
      "Training epoch: 650, training rmse: 0.182547, vali rmse:0.427796\n",
      "\n",
      "Start of epoch 650\n",
      "Training loss (for one batch) at step 0: 0.1452\n",
      "Training epoch: 651, training rmse: 0.182448, vali rmse:0.427759\n",
      "\n",
      "Start of epoch 651\n",
      "Training loss (for one batch) at step 0: 0.1452\n",
      "Training epoch: 652, training rmse: 0.182350, vali rmse:0.427722\n",
      "\n",
      "Start of epoch 652\n",
      "Training loss (for one batch) at step 0: 0.1451\n",
      "Training epoch: 653, training rmse: 0.182251, vali rmse:0.427685\n",
      "\n",
      "Start of epoch 653\n",
      "Training loss (for one batch) at step 0: 0.1451\n",
      "Training epoch: 654, training rmse: 0.182152, vali rmse:0.427648\n",
      "\n",
      "Start of epoch 654\n",
      "Training loss (for one batch) at step 0: 0.1451\n",
      "Training epoch: 655, training rmse: 0.182053, vali rmse:0.427612\n",
      "\n",
      "Start of epoch 655\n",
      "Training loss (for one batch) at step 0: 0.1450\n",
      "Training epoch: 656, training rmse: 0.181954, vali rmse:0.427575\n",
      "\n",
      "Start of epoch 656\n",
      "Training loss (for one batch) at step 0: 0.1450\n",
      "Training epoch: 657, training rmse: 0.181855, vali rmse:0.427539\n",
      "\n",
      "Start of epoch 657\n",
      "Training loss (for one batch) at step 0: 0.1449\n",
      "Training epoch: 658, training rmse: 0.181756, vali rmse:0.427504\n",
      "\n",
      "Start of epoch 658\n",
      "Training loss (for one batch) at step 0: 0.1449\n",
      "Training epoch: 659, training rmse: 0.181657, vali rmse:0.427468\n",
      "\n",
      "Start of epoch 659\n",
      "Training loss (for one batch) at step 0: 0.1448\n",
      "Training epoch: 660, training rmse: 0.181557, vali rmse:0.427432\n",
      "\n",
      "Start of epoch 660\n",
      "Training loss (for one batch) at step 0: 0.1448\n",
      "Training epoch: 661, training rmse: 0.181458, vali rmse:0.427397\n",
      "\n",
      "Start of epoch 661\n",
      "Training loss (for one batch) at step 0: 0.1447\n",
      "Training epoch: 662, training rmse: 0.181358, vali rmse:0.427362\n",
      "\n",
      "Start of epoch 662\n",
      "Training loss (for one batch) at step 0: 0.1447\n",
      "Training epoch: 663, training rmse: 0.181259, vali rmse:0.427327\n",
      "\n",
      "Start of epoch 663\n",
      "Training loss (for one batch) at step 0: 0.1446\n",
      "Training epoch: 664, training rmse: 0.181159, vali rmse:0.427293\n",
      "\n",
      "Start of epoch 664\n",
      "Training loss (for one batch) at step 0: 0.1446\n",
      "Training epoch: 665, training rmse: 0.181059, vali rmse:0.427258\n",
      "\n",
      "Start of epoch 665\n",
      "Training loss (for one batch) at step 0: 0.1445\n",
      "Training epoch: 666, training rmse: 0.180959, vali rmse:0.427224\n",
      "\n",
      "Start of epoch 666\n",
      "Training loss (for one batch) at step 0: 0.1445\n",
      "Training epoch: 667, training rmse: 0.180859, vali rmse:0.427190\n",
      "\n",
      "Start of epoch 667\n",
      "Training loss (for one batch) at step 0: 0.1444\n",
      "Training epoch: 668, training rmse: 0.180759, vali rmse:0.427156\n",
      "\n",
      "Start of epoch 668\n",
      "Training loss (for one batch) at step 0: 0.1444\n",
      "Training epoch: 669, training rmse: 0.180659, vali rmse:0.427122\n",
      "\n",
      "Start of epoch 669\n",
      "Training loss (for one batch) at step 0: 0.1443\n",
      "Training epoch: 670, training rmse: 0.180559, vali rmse:0.427088\n",
      "\n",
      "Start of epoch 670\n",
      "Training loss (for one batch) at step 0: 0.1443\n",
      "Training epoch: 671, training rmse: 0.180459, vali rmse:0.427055\n",
      "\n",
      "Start of epoch 671\n",
      "Training loss (for one batch) at step 0: 0.1442\n",
      "Training epoch: 672, training rmse: 0.180358, vali rmse:0.427022\n",
      "\n",
      "Start of epoch 672\n",
      "Training loss (for one batch) at step 0: 0.1442\n",
      "Training epoch: 673, training rmse: 0.180258, vali rmse:0.426989\n",
      "\n",
      "Start of epoch 673\n",
      "Training loss (for one batch) at step 0: 0.1441\n",
      "Training epoch: 674, training rmse: 0.180157, vali rmse:0.426956\n",
      "\n",
      "Start of epoch 674\n",
      "Training loss (for one batch) at step 0: 0.1441\n",
      "Training epoch: 675, training rmse: 0.180056, vali rmse:0.426923\n",
      "\n",
      "Start of epoch 675\n",
      "Training loss (for one batch) at step 0: 0.1440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 676, training rmse: 0.179956, vali rmse:0.426891\n",
      "\n",
      "Start of epoch 676\n",
      "Training loss (for one batch) at step 0: 0.1440\n",
      "Training epoch: 677, training rmse: 0.179855, vali rmse:0.426858\n",
      "\n",
      "Start of epoch 677\n",
      "Training loss (for one batch) at step 0: 0.1439\n",
      "Training epoch: 678, training rmse: 0.179754, vali rmse:0.426826\n",
      "\n",
      "Start of epoch 678\n",
      "Training loss (for one batch) at step 0: 0.1439\n",
      "Training epoch: 679, training rmse: 0.179653, vali rmse:0.426794\n",
      "\n",
      "Start of epoch 679\n",
      "Training loss (for one batch) at step 0: 0.1438\n",
      "Training epoch: 680, training rmse: 0.179552, vali rmse:0.426762\n",
      "\n",
      "Start of epoch 680\n",
      "Training loss (for one batch) at step 0: 0.1438\n",
      "Training epoch: 681, training rmse: 0.179450, vali rmse:0.426731\n",
      "\n",
      "Start of epoch 681\n",
      "Training loss (for one batch) at step 0: 0.1437\n",
      "Training epoch: 682, training rmse: 0.179349, vali rmse:0.426699\n",
      "\n",
      "Start of epoch 682\n",
      "Training loss (for one batch) at step 0: 0.1437\n",
      "Training epoch: 683, training rmse: 0.179248, vali rmse:0.426668\n",
      "\n",
      "Start of epoch 683\n",
      "Training loss (for one batch) at step 0: 0.1436\n",
      "Training epoch: 684, training rmse: 0.179146, vali rmse:0.426637\n",
      "\n",
      "Start of epoch 684\n",
      "Training loss (for one batch) at step 0: 0.1436\n",
      "Training epoch: 685, training rmse: 0.179044, vali rmse:0.426606\n",
      "\n",
      "Start of epoch 685\n",
      "Training loss (for one batch) at step 0: 0.1435\n",
      "Training epoch: 686, training rmse: 0.178943, vali rmse:0.426575\n",
      "\n",
      "Start of epoch 686\n",
      "Training loss (for one batch) at step 0: 0.1435\n",
      "Training epoch: 687, training rmse: 0.178841, vali rmse:0.426544\n",
      "\n",
      "Start of epoch 687\n",
      "Training loss (for one batch) at step 0: 0.1434\n",
      "Training epoch: 688, training rmse: 0.178739, vali rmse:0.426514\n",
      "\n",
      "Start of epoch 688\n",
      "Training loss (for one batch) at step 0: 0.1434\n",
      "Training epoch: 689, training rmse: 0.178637, vali rmse:0.426484\n",
      "\n",
      "Start of epoch 689\n",
      "Training loss (for one batch) at step 0: 0.1433\n",
      "Training epoch: 690, training rmse: 0.178535, vali rmse:0.426453\n",
      "\n",
      "Start of epoch 690\n",
      "Training loss (for one batch) at step 0: 0.1433\n",
      "Training epoch: 691, training rmse: 0.178433, vali rmse:0.426423\n",
      "\n",
      "Start of epoch 691\n",
      "Training loss (for one batch) at step 0: 0.1432\n",
      "Training epoch: 692, training rmse: 0.178330, vali rmse:0.426394\n",
      "\n",
      "Start of epoch 692\n",
      "Training loss (for one batch) at step 0: 0.1432\n",
      "Training epoch: 693, training rmse: 0.178228, vali rmse:0.426364\n",
      "\n",
      "Start of epoch 693\n",
      "Training loss (for one batch) at step 0: 0.1432\n",
      "Training epoch: 694, training rmse: 0.178125, vali rmse:0.426334\n",
      "\n",
      "Start of epoch 694\n",
      "Training loss (for one batch) at step 0: 0.1431\n",
      "Training epoch: 695, training rmse: 0.178023, vali rmse:0.426305\n",
      "\n",
      "Start of epoch 695\n",
      "Training loss (for one batch) at step 0: 0.1431\n",
      "Training epoch: 696, training rmse: 0.177920, vali rmse:0.426276\n",
      "\n",
      "Start of epoch 696\n",
      "Training loss (for one batch) at step 0: 0.1430\n",
      "Training epoch: 697, training rmse: 0.177817, vali rmse:0.426247\n",
      "\n",
      "Start of epoch 697\n",
      "Training loss (for one batch) at step 0: 0.1430\n",
      "Training epoch: 698, training rmse: 0.177714, vali rmse:0.426218\n",
      "\n",
      "Start of epoch 698\n",
      "Training loss (for one batch) at step 0: 0.1429\n",
      "Training epoch: 699, training rmse: 0.177611, vali rmse:0.426189\n",
      "\n",
      "Start of epoch 699\n",
      "Training loss (for one batch) at step 0: 0.1429\n",
      "Training epoch: 700, training rmse: 0.177508, vali rmse:0.426160\n",
      "\n",
      "Start of epoch 700\n",
      "Training loss (for one batch) at step 0: 0.1428\n",
      "Training epoch: 701, training rmse: 0.177405, vali rmse:0.426132\n",
      "\n",
      "Start of epoch 701\n",
      "Training loss (for one batch) at step 0: 0.1428\n",
      "Training epoch: 702, training rmse: 0.177302, vali rmse:0.426104\n",
      "\n",
      "Start of epoch 702\n",
      "Training loss (for one batch) at step 0: 0.1427\n",
      "Training epoch: 703, training rmse: 0.177198, vali rmse:0.426076\n",
      "\n",
      "Start of epoch 703\n",
      "Training loss (for one batch) at step 0: 0.1427\n",
      "Training epoch: 704, training rmse: 0.177095, vali rmse:0.426048\n",
      "\n",
      "Start of epoch 704\n",
      "Training loss (for one batch) at step 0: 0.1426\n",
      "Training epoch: 705, training rmse: 0.176991, vali rmse:0.426020\n",
      "\n",
      "Start of epoch 705\n",
      "Training loss (for one batch) at step 0: 0.1426\n",
      "Training epoch: 706, training rmse: 0.176887, vali rmse:0.425992\n",
      "\n",
      "Start of epoch 706\n",
      "Training loss (for one batch) at step 0: 0.1425\n",
      "Training epoch: 707, training rmse: 0.176783, vali rmse:0.425964\n",
      "\n",
      "Start of epoch 707\n",
      "Training loss (for one batch) at step 0: 0.1425\n",
      "Training epoch: 708, training rmse: 0.176679, vali rmse:0.425937\n",
      "\n",
      "Start of epoch 708\n",
      "Training loss (for one batch) at step 0: 0.1424\n",
      "Training epoch: 709, training rmse: 0.176575, vali rmse:0.425910\n",
      "\n",
      "Start of epoch 709\n",
      "Training loss (for one batch) at step 0: 0.1424\n",
      "Training epoch: 710, training rmse: 0.176471, vali rmse:0.425882\n",
      "\n",
      "Start of epoch 710\n",
      "Training loss (for one batch) at step 0: 0.1423\n",
      "Training epoch: 711, training rmse: 0.176367, vali rmse:0.425855\n",
      "\n",
      "Start of epoch 711\n",
      "Training loss (for one batch) at step 0: 0.1423\n",
      "Training epoch: 712, training rmse: 0.176263, vali rmse:0.425828\n",
      "\n",
      "Start of epoch 712\n",
      "Training loss (for one batch) at step 0: 0.1422\n",
      "Training epoch: 713, training rmse: 0.176158, vali rmse:0.425802\n",
      "\n",
      "Start of epoch 713\n",
      "Training loss (for one batch) at step 0: 0.1422\n",
      "Training epoch: 714, training rmse: 0.176054, vali rmse:0.425775\n",
      "\n",
      "Start of epoch 714\n",
      "Training loss (for one batch) at step 0: 0.1421\n",
      "Training epoch: 715, training rmse: 0.175949, vali rmse:0.425749\n",
      "\n",
      "Start of epoch 715\n",
      "Training loss (for one batch) at step 0: 0.1421\n",
      "Training epoch: 716, training rmse: 0.175844, vali rmse:0.425722\n",
      "\n",
      "Start of epoch 716\n",
      "Training loss (for one batch) at step 0: 0.1420\n",
      "Training epoch: 717, training rmse: 0.175739, vali rmse:0.425696\n",
      "\n",
      "Start of epoch 717\n",
      "Training loss (for one batch) at step 0: 0.1420\n",
      "Training epoch: 718, training rmse: 0.175634, vali rmse:0.425670\n",
      "\n",
      "Start of epoch 718\n",
      "Training loss (for one batch) at step 0: 0.1419\n",
      "Training epoch: 719, training rmse: 0.175529, vali rmse:0.425644\n",
      "\n",
      "Start of epoch 719\n",
      "Training loss (for one batch) at step 0: 0.1419\n",
      "Training epoch: 720, training rmse: 0.175424, vali rmse:0.425618\n",
      "\n",
      "Start of epoch 720\n",
      "Training loss (for one batch) at step 0: 0.1419\n",
      "Training epoch: 721, training rmse: 0.175319, vali rmse:0.425593\n",
      "\n",
      "Start of epoch 721\n",
      "Training loss (for one batch) at step 0: 0.1418\n",
      "Training epoch: 722, training rmse: 0.175213, vali rmse:0.425567\n",
      "\n",
      "Start of epoch 722\n",
      "Training loss (for one batch) at step 0: 0.1418\n",
      "Training epoch: 723, training rmse: 0.175108, vali rmse:0.425542\n",
      "\n",
      "Start of epoch 723\n",
      "Training loss (for one batch) at step 0: 0.1417\n",
      "Training epoch: 724, training rmse: 0.175002, vali rmse:0.425516\n",
      "\n",
      "Start of epoch 724\n",
      "Training loss (for one batch) at step 0: 0.1417\n",
      "Training epoch: 725, training rmse: 0.174897, vali rmse:0.425491\n",
      "\n",
      "Start of epoch 725\n",
      "Training loss (for one batch) at step 0: 0.1416\n",
      "Training epoch: 726, training rmse: 0.174791, vali rmse:0.425466\n",
      "\n",
      "Start of epoch 726\n",
      "Training loss (for one batch) at step 0: 0.1416\n",
      "Training epoch: 727, training rmse: 0.174685, vali rmse:0.425441\n",
      "\n",
      "Start of epoch 727\n",
      "Training loss (for one batch) at step 0: 0.1415\n",
      "Training epoch: 728, training rmse: 0.174579, vali rmse:0.425416\n",
      "\n",
      "Start of epoch 728\n",
      "Training loss (for one batch) at step 0: 0.1415\n",
      "Training epoch: 729, training rmse: 0.174473, vali rmse:0.425392\n",
      "\n",
      "Start of epoch 729\n",
      "Training loss (for one batch) at step 0: 0.1414\n",
      "Training epoch: 730, training rmse: 0.174366, vali rmse:0.425367\n",
      "\n",
      "Start of epoch 730\n",
      "Training loss (for one batch) at step 0: 0.1414\n",
      "Training epoch: 731, training rmse: 0.174260, vali rmse:0.425343\n",
      "\n",
      "Start of epoch 731\n",
      "Training loss (for one batch) at step 0: 0.1413\n",
      "Training epoch: 732, training rmse: 0.174153, vali rmse:0.425318\n",
      "\n",
      "Start of epoch 732\n",
      "Training loss (for one batch) at step 0: 0.1413\n",
      "Training epoch: 733, training rmse: 0.174047, vali rmse:0.425294\n",
      "\n",
      "Start of epoch 733\n",
      "Training loss (for one batch) at step 0: 0.1412\n",
      "Training epoch: 734, training rmse: 0.173940, vali rmse:0.425270\n",
      "\n",
      "Start of epoch 734\n",
      "Training loss (for one batch) at step 0: 0.1412\n",
      "Training epoch: 735, training rmse: 0.173833, vali rmse:0.425246\n",
      "\n",
      "Start of epoch 735\n",
      "Training loss (for one batch) at step 0: 0.1411\n",
      "Training epoch: 736, training rmse: 0.173726, vali rmse:0.425222\n",
      "\n",
      "Start of epoch 736\n",
      "Training loss (for one batch) at step 0: 0.1411\n",
      "Training epoch: 737, training rmse: 0.173619, vali rmse:0.425199\n",
      "\n",
      "Start of epoch 737\n",
      "Training loss (for one batch) at step 0: 0.1410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 738, training rmse: 0.173512, vali rmse:0.425175\n",
      "\n",
      "Start of epoch 738\n",
      "Training loss (for one batch) at step 0: 0.1410\n",
      "Training epoch: 739, training rmse: 0.173405, vali rmse:0.425151\n",
      "\n",
      "Start of epoch 739\n",
      "Training loss (for one batch) at step 0: 0.1409\n",
      "Training epoch: 740, training rmse: 0.173297, vali rmse:0.425128\n",
      "\n",
      "Start of epoch 740\n",
      "Training loss (for one batch) at step 0: 0.1409\n",
      "Training epoch: 741, training rmse: 0.173190, vali rmse:0.425105\n",
      "\n",
      "Start of epoch 741\n",
      "Training loss (for one batch) at step 0: 0.1408\n",
      "Training epoch: 742, training rmse: 0.173082, vali rmse:0.425082\n",
      "\n",
      "Start of epoch 742\n",
      "Training loss (for one batch) at step 0: 0.1408\n",
      "Training epoch: 743, training rmse: 0.172975, vali rmse:0.425059\n",
      "\n",
      "Start of epoch 743\n",
      "Training loss (for one batch) at step 0: 0.1407\n",
      "Training epoch: 744, training rmse: 0.172867, vali rmse:0.425036\n",
      "\n",
      "Start of epoch 744\n",
      "Training loss (for one batch) at step 0: 0.1407\n",
      "Training epoch: 745, training rmse: 0.172759, vali rmse:0.425013\n",
      "\n",
      "Start of epoch 745\n",
      "Training loss (for one batch) at step 0: 0.1406\n",
      "Training epoch: 746, training rmse: 0.172651, vali rmse:0.424990\n",
      "\n",
      "Start of epoch 746\n",
      "Training loss (for one batch) at step 0: 0.1406\n",
      "Training epoch: 747, training rmse: 0.172543, vali rmse:0.424968\n",
      "\n",
      "Start of epoch 747\n",
      "Training loss (for one batch) at step 0: 0.1405\n",
      "Training epoch: 748, training rmse: 0.172434, vali rmse:0.424945\n",
      "\n",
      "Start of epoch 748\n",
      "Training loss (for one batch) at step 0: 0.1405\n",
      "Training epoch: 749, training rmse: 0.172326, vali rmse:0.424923\n",
      "\n",
      "Start of epoch 749\n",
      "Training loss (for one batch) at step 0: 0.1404\n",
      "Training epoch: 750, training rmse: 0.172218, vali rmse:0.424901\n",
      "\n",
      "Start of epoch 750\n",
      "Training loss (for one batch) at step 0: 0.1404\n",
      "Training epoch: 751, training rmse: 0.172109, vali rmse:0.424878\n",
      "\n",
      "Start of epoch 751\n",
      "Training loss (for one batch) at step 0: 0.1403\n",
      "Training epoch: 752, training rmse: 0.172000, vali rmse:0.424856\n",
      "\n",
      "Start of epoch 752\n",
      "Training loss (for one batch) at step 0: 0.1403\n",
      "Training epoch: 753, training rmse: 0.171891, vali rmse:0.424834\n",
      "\n",
      "Start of epoch 753\n",
      "Training loss (for one batch) at step 0: 0.1403\n",
      "Training epoch: 754, training rmse: 0.171782, vali rmse:0.424813\n",
      "\n",
      "Start of epoch 754\n",
      "Training loss (for one batch) at step 0: 0.1402\n",
      "Training epoch: 755, training rmse: 0.171673, vali rmse:0.424791\n",
      "\n",
      "Start of epoch 755\n",
      "Training loss (for one batch) at step 0: 0.1402\n",
      "Training epoch: 756, training rmse: 0.171564, vali rmse:0.424769\n",
      "\n",
      "Start of epoch 756\n",
      "Training loss (for one batch) at step 0: 0.1401\n",
      "Training epoch: 757, training rmse: 0.171455, vali rmse:0.424748\n",
      "\n",
      "Start of epoch 757\n",
      "Training loss (for one batch) at step 0: 0.1401\n",
      "Training epoch: 758, training rmse: 0.171346, vali rmse:0.424726\n",
      "\n",
      "Start of epoch 758\n",
      "Training loss (for one batch) at step 0: 0.1400\n",
      "Training epoch: 759, training rmse: 0.171236, vali rmse:0.424705\n",
      "\n",
      "Start of epoch 759\n",
      "Training loss (for one batch) at step 0: 0.1400\n",
      "Training epoch: 760, training rmse: 0.171126, vali rmse:0.424684\n",
      "\n",
      "Start of epoch 760\n",
      "Training loss (for one batch) at step 0: 0.1399\n",
      "Training epoch: 761, training rmse: 0.171017, vali rmse:0.424663\n",
      "\n",
      "Start of epoch 761\n",
      "Training loss (for one batch) at step 0: 0.1399\n",
      "Training epoch: 762, training rmse: 0.170907, vali rmse:0.424642\n",
      "\n",
      "Start of epoch 762\n",
      "Training loss (for one batch) at step 0: 0.1398\n",
      "Training epoch: 763, training rmse: 0.170797, vali rmse:0.424621\n",
      "\n",
      "Start of epoch 763\n",
      "Training loss (for one batch) at step 0: 0.1398\n",
      "Training epoch: 764, training rmse: 0.170687, vali rmse:0.424600\n",
      "\n",
      "Start of epoch 764\n",
      "Training loss (for one batch) at step 0: 0.1397\n",
      "Training epoch: 765, training rmse: 0.170577, vali rmse:0.424579\n",
      "\n",
      "Start of epoch 765\n",
      "Training loss (for one batch) at step 0: 0.1397\n",
      "Training epoch: 766, training rmse: 0.170466, vali rmse:0.424558\n",
      "\n",
      "Start of epoch 766\n",
      "Training loss (for one batch) at step 0: 0.1396\n",
      "Training epoch: 767, training rmse: 0.170356, vali rmse:0.424538\n",
      "\n",
      "Start of epoch 767\n",
      "Training loss (for one batch) at step 0: 0.1396\n",
      "Training epoch: 768, training rmse: 0.170245, vali rmse:0.424517\n",
      "\n",
      "Start of epoch 768\n",
      "Training loss (for one batch) at step 0: 0.1395\n",
      "Training epoch: 769, training rmse: 0.170135, vali rmse:0.424497\n",
      "\n",
      "Start of epoch 769\n",
      "Training loss (for one batch) at step 0: 0.1395\n",
      "Training epoch: 770, training rmse: 0.170024, vali rmse:0.424477\n",
      "\n",
      "Start of epoch 770\n",
      "Training loss (for one batch) at step 0: 0.1394\n",
      "Training epoch: 771, training rmse: 0.169913, vali rmse:0.424457\n",
      "\n",
      "Start of epoch 771\n",
      "Training loss (for one batch) at step 0: 0.1394\n",
      "Training epoch: 772, training rmse: 0.169802, vali rmse:0.424437\n",
      "\n",
      "Start of epoch 772\n",
      "Training loss (for one batch) at step 0: 0.1393\n",
      "Training epoch: 773, training rmse: 0.169691, vali rmse:0.424417\n",
      "\n",
      "Start of epoch 773\n",
      "Training loss (for one batch) at step 0: 0.1393\n",
      "Training epoch: 774, training rmse: 0.169580, vali rmse:0.424397\n",
      "\n",
      "Start of epoch 774\n",
      "Training loss (for one batch) at step 0: 0.1392\n",
      "Training epoch: 775, training rmse: 0.169468, vali rmse:0.424377\n",
      "\n",
      "Start of epoch 775\n",
      "Training loss (for one batch) at step 0: 0.1392\n",
      "Training epoch: 776, training rmse: 0.169357, vali rmse:0.424357\n",
      "\n",
      "Start of epoch 776\n",
      "Training loss (for one batch) at step 0: 0.1391\n",
      "Training epoch: 777, training rmse: 0.169245, vali rmse:0.424338\n",
      "\n",
      "Start of epoch 777\n",
      "Training loss (for one batch) at step 0: 0.1391\n",
      "Training epoch: 778, training rmse: 0.169134, vali rmse:0.424318\n",
      "\n",
      "Start of epoch 778\n",
      "Training loss (for one batch) at step 0: 0.1390\n",
      "Training epoch: 779, training rmse: 0.169022, vali rmse:0.424299\n",
      "\n",
      "Start of epoch 779\n",
      "Training loss (for one batch) at step 0: 0.1390\n",
      "Training epoch: 780, training rmse: 0.168910, vali rmse:0.424279\n",
      "\n",
      "Start of epoch 780\n",
      "Training loss (for one batch) at step 0: 0.1389\n",
      "Training epoch: 781, training rmse: 0.168798, vali rmse:0.424260\n",
      "\n",
      "Start of epoch 781\n",
      "Training loss (for one batch) at step 0: 0.1389\n",
      "Training epoch: 782, training rmse: 0.168686, vali rmse:0.424241\n",
      "\n",
      "Start of epoch 782\n",
      "Training loss (for one batch) at step 0: 0.1388\n",
      "Training epoch: 783, training rmse: 0.168574, vali rmse:0.424222\n",
      "\n",
      "Start of epoch 783\n",
      "Training loss (for one batch) at step 0: 0.1388\n",
      "Training epoch: 784, training rmse: 0.168461, vali rmse:0.424203\n",
      "\n",
      "Start of epoch 784\n",
      "Training loss (for one batch) at step 0: 0.1387\n",
      "Training epoch: 785, training rmse: 0.168349, vali rmse:0.424184\n",
      "\n",
      "Start of epoch 785\n",
      "Training loss (for one batch) at step 0: 0.1387\n",
      "Training epoch: 786, training rmse: 0.168236, vali rmse:0.424165\n",
      "\n",
      "Start of epoch 786\n",
      "Training loss (for one batch) at step 0: 0.1386\n",
      "Training epoch: 787, training rmse: 0.168123, vali rmse:0.424146\n",
      "\n",
      "Start of epoch 787\n",
      "Training loss (for one batch) at step 0: 0.1386\n",
      "Training epoch: 788, training rmse: 0.168010, vali rmse:0.424127\n",
      "\n",
      "Start of epoch 788\n",
      "Training loss (for one batch) at step 0: 0.1385\n",
      "Training epoch: 789, training rmse: 0.167897, vali rmse:0.424109\n",
      "\n",
      "Start of epoch 789\n",
      "Training loss (for one batch) at step 0: 0.1385\n",
      "Training epoch: 790, training rmse: 0.167784, vali rmse:0.424090\n",
      "\n",
      "Start of epoch 790\n",
      "Training loss (for one batch) at step 0: 0.1384\n",
      "Training epoch: 791, training rmse: 0.167671, vali rmse:0.424072\n",
      "\n",
      "Start of epoch 791\n",
      "Training loss (for one batch) at step 0: 0.1384\n",
      "Training epoch: 792, training rmse: 0.167558, vali rmse:0.424053\n",
      "\n",
      "Start of epoch 792\n",
      "Training loss (for one batch) at step 0: 0.1383\n",
      "Training epoch: 793, training rmse: 0.167444, vali rmse:0.424035\n",
      "\n",
      "Start of epoch 793\n",
      "Training loss (for one batch) at step 0: 0.1383\n",
      "Training epoch: 794, training rmse: 0.167331, vali rmse:0.424017\n",
      "\n",
      "Start of epoch 794\n",
      "Training loss (for one batch) at step 0: 0.1382\n",
      "Training epoch: 795, training rmse: 0.167217, vali rmse:0.423999\n",
      "\n",
      "Start of epoch 795\n",
      "Training loss (for one batch) at step 0: 0.1382\n",
      "Training epoch: 796, training rmse: 0.167103, vali rmse:0.423981\n",
      "\n",
      "Start of epoch 796\n",
      "Training loss (for one batch) at step 0: 0.1381\n",
      "Training epoch: 797, training rmse: 0.166989, vali rmse:0.423963\n",
      "\n",
      "Start of epoch 797\n",
      "Training loss (for one batch) at step 0: 0.1381\n",
      "Training epoch: 798, training rmse: 0.166875, vali rmse:0.423945\n",
      "\n",
      "Start of epoch 798\n",
      "Training loss (for one batch) at step 0: 0.1380\n",
      "Training epoch: 799, training rmse: 0.166761, vali rmse:0.423927\n",
      "\n",
      "Start of epoch 799\n",
      "Training loss (for one batch) at step 0: 0.1380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 800, training rmse: 0.166647, vali rmse:0.423910\n",
      "\n",
      "Start of epoch 800\n",
      "Training loss (for one batch) at step 0: 0.1379\n",
      "Training epoch: 801, training rmse: 0.166533, vali rmse:0.423892\n",
      "\n",
      "Start of epoch 801\n",
      "Training loss (for one batch) at step 0: 0.1379\n",
      "Training epoch: 802, training rmse: 0.166418, vali rmse:0.423874\n",
      "\n",
      "Start of epoch 802\n",
      "Training loss (for one batch) at step 0: 0.1378\n",
      "Training epoch: 803, training rmse: 0.166304, vali rmse:0.423857\n",
      "\n",
      "Start of epoch 803\n",
      "Training loss (for one batch) at step 0: 0.1378\n",
      "Training epoch: 804, training rmse: 0.166189, vali rmse:0.423839\n",
      "\n",
      "Start of epoch 804\n",
      "Training loss (for one batch) at step 0: 0.1377\n",
      "Training epoch: 805, training rmse: 0.166074, vali rmse:0.423822\n",
      "\n",
      "Start of epoch 805\n",
      "Training loss (for one batch) at step 0: 0.1377\n",
      "Training epoch: 806, training rmse: 0.165959, vali rmse:0.423805\n",
      "\n",
      "Start of epoch 806\n",
      "Training loss (for one batch) at step 0: 0.1376\n",
      "Training epoch: 807, training rmse: 0.165844, vali rmse:0.423788\n",
      "\n",
      "Start of epoch 807\n",
      "Training loss (for one batch) at step 0: 0.1376\n",
      "Training epoch: 808, training rmse: 0.165729, vali rmse:0.423770\n",
      "\n",
      "Start of epoch 808\n",
      "Training loss (for one batch) at step 0: 0.1375\n",
      "Training epoch: 809, training rmse: 0.165613, vali rmse:0.423753\n",
      "\n",
      "Start of epoch 809\n",
      "Training loss (for one batch) at step 0: 0.1375\n",
      "Training epoch: 810, training rmse: 0.165498, vali rmse:0.423736\n",
      "\n",
      "Start of epoch 810\n",
      "Training loss (for one batch) at step 0: 0.1374\n",
      "Training epoch: 811, training rmse: 0.165382, vali rmse:0.423719\n",
      "\n",
      "Start of epoch 811\n",
      "Training loss (for one batch) at step 0: 0.1374\n",
      "Training epoch: 812, training rmse: 0.165267, vali rmse:0.423703\n",
      "\n",
      "Start of epoch 812\n",
      "Training loss (for one batch) at step 0: 0.1373\n",
      "Training epoch: 813, training rmse: 0.165151, vali rmse:0.423686\n",
      "\n",
      "Start of epoch 813\n",
      "Training loss (for one batch) at step 0: 0.1372\n",
      "Training epoch: 814, training rmse: 0.165035, vali rmse:0.423669\n",
      "\n",
      "Start of epoch 814\n",
      "Training loss (for one batch) at step 0: 0.1372\n",
      "Training epoch: 815, training rmse: 0.164919, vali rmse:0.423652\n",
      "\n",
      "Start of epoch 815\n",
      "Training loss (for one batch) at step 0: 0.1371\n",
      "Training epoch: 816, training rmse: 0.164803, vali rmse:0.423636\n",
      "\n",
      "Start of epoch 816\n",
      "Training loss (for one batch) at step 0: 0.1371\n",
      "Training epoch: 817, training rmse: 0.164687, vali rmse:0.423619\n",
      "\n",
      "Start of epoch 817\n",
      "Training loss (for one batch) at step 0: 0.1370\n",
      "Training epoch: 818, training rmse: 0.164570, vali rmse:0.423603\n",
      "\n",
      "Start of epoch 818\n",
      "Training loss (for one batch) at step 0: 0.1370\n",
      "Training epoch: 819, training rmse: 0.164454, vali rmse:0.423587\n",
      "\n",
      "Start of epoch 819\n",
      "Training loss (for one batch) at step 0: 0.1369\n",
      "Training epoch: 820, training rmse: 0.164337, vali rmse:0.423570\n",
      "\n",
      "Start of epoch 820\n",
      "Training loss (for one batch) at step 0: 0.1369\n",
      "Training epoch: 821, training rmse: 0.164220, vali rmse:0.423554\n",
      "\n",
      "Start of epoch 821\n",
      "Training loss (for one batch) at step 0: 0.1368\n",
      "Training epoch: 822, training rmse: 0.164104, vali rmse:0.423538\n",
      "\n",
      "Start of epoch 822\n",
      "Training loss (for one batch) at step 0: 0.1368\n",
      "Training epoch: 823, training rmse: 0.163987, vali rmse:0.423522\n",
      "\n",
      "Start of epoch 823\n",
      "Training loss (for one batch) at step 0: 0.1367\n",
      "Training epoch: 824, training rmse: 0.163870, vali rmse:0.423506\n",
      "\n",
      "Start of epoch 824\n",
      "Training loss (for one batch) at step 0: 0.1367\n",
      "Training epoch: 825, training rmse: 0.163752, vali rmse:0.423490\n",
      "\n",
      "Start of epoch 825\n",
      "Training loss (for one batch) at step 0: 0.1366\n",
      "Training epoch: 826, training rmse: 0.163635, vali rmse:0.423474\n",
      "\n",
      "Start of epoch 826\n",
      "Training loss (for one batch) at step 0: 0.1366\n",
      "Training epoch: 827, training rmse: 0.163518, vali rmse:0.423458\n",
      "\n",
      "Start of epoch 827\n",
      "Training loss (for one batch) at step 0: 0.1365\n",
      "Training epoch: 828, training rmse: 0.163400, vali rmse:0.423442\n",
      "\n",
      "Start of epoch 828\n",
      "Training loss (for one batch) at step 0: 0.1365\n",
      "Training epoch: 829, training rmse: 0.163282, vali rmse:0.423427\n",
      "\n",
      "Start of epoch 829\n",
      "Training loss (for one batch) at step 0: 0.1364\n",
      "Training epoch: 830, training rmse: 0.163165, vali rmse:0.423411\n",
      "\n",
      "Start of epoch 830\n",
      "Training loss (for one batch) at step 0: 0.1364\n",
      "Training epoch: 831, training rmse: 0.163047, vali rmse:0.423395\n",
      "\n",
      "Start of epoch 831\n",
      "Training loss (for one batch) at step 0: 0.1363\n",
      "Training epoch: 832, training rmse: 0.162929, vali rmse:0.423380\n",
      "\n",
      "Start of epoch 832\n",
      "Training loss (for one batch) at step 0: 0.1363\n",
      "Training epoch: 833, training rmse: 0.162811, vali rmse:0.423364\n",
      "\n",
      "Start of epoch 833\n",
      "Training loss (for one batch) at step 0: 0.1362\n",
      "Training epoch: 834, training rmse: 0.162692, vali rmse:0.423349\n",
      "\n",
      "Start of epoch 834\n",
      "Training loss (for one batch) at step 0: 0.1362\n",
      "Training epoch: 835, training rmse: 0.162574, vali rmse:0.423334\n",
      "\n",
      "Start of epoch 835\n",
      "Training loss (for one batch) at step 0: 0.1361\n",
      "Training epoch: 836, training rmse: 0.162456, vali rmse:0.423318\n",
      "\n",
      "Start of epoch 836\n",
      "Training loss (for one batch) at step 0: 0.1361\n",
      "Training epoch: 837, training rmse: 0.162337, vali rmse:0.423303\n",
      "\n",
      "Start of epoch 837\n",
      "Training loss (for one batch) at step 0: 0.1360\n",
      "Training epoch: 838, training rmse: 0.162218, vali rmse:0.423288\n",
      "\n",
      "Start of epoch 838\n",
      "Training loss (for one batch) at step 0: 0.1359\n",
      "Training epoch: 839, training rmse: 0.162099, vali rmse:0.423273\n",
      "\n",
      "Start of epoch 839\n",
      "Training loss (for one batch) at step 0: 0.1359\n",
      "Training epoch: 840, training rmse: 0.161980, vali rmse:0.423258\n",
      "\n",
      "Start of epoch 840\n",
      "Training loss (for one batch) at step 0: 0.1358\n",
      "Training epoch: 841, training rmse: 0.161861, vali rmse:0.423243\n",
      "\n",
      "Start of epoch 841\n",
      "Training loss (for one batch) at step 0: 0.1358\n",
      "Training epoch: 842, training rmse: 0.161742, vali rmse:0.423228\n",
      "\n",
      "Start of epoch 842\n",
      "Training loss (for one batch) at step 0: 0.1357\n",
      "Training epoch: 843, training rmse: 0.161623, vali rmse:0.423213\n",
      "\n",
      "Start of epoch 843\n",
      "Training loss (for one batch) at step 0: 0.1357\n",
      "Training epoch: 844, training rmse: 0.161503, vali rmse:0.423198\n",
      "\n",
      "Start of epoch 844\n",
      "Training loss (for one batch) at step 0: 0.1356\n",
      "Training epoch: 845, training rmse: 0.161384, vali rmse:0.423183\n",
      "\n",
      "Start of epoch 845\n",
      "Training loss (for one batch) at step 0: 0.1356\n",
      "Training epoch: 846, training rmse: 0.161264, vali rmse:0.423169\n",
      "\n",
      "Start of epoch 846\n",
      "Training loss (for one batch) at step 0: 0.1355\n",
      "Training epoch: 847, training rmse: 0.161144, vali rmse:0.423154\n",
      "\n",
      "Start of epoch 847\n",
      "Training loss (for one batch) at step 0: 0.1355\n",
      "Training epoch: 848, training rmse: 0.161025, vali rmse:0.423139\n",
      "\n",
      "Start of epoch 848\n",
      "Training loss (for one batch) at step 0: 0.1354\n",
      "Training epoch: 849, training rmse: 0.160905, vali rmse:0.423125\n",
      "\n",
      "Start of epoch 849\n",
      "Training loss (for one batch) at step 0: 0.1354\n",
      "Training epoch: 850, training rmse: 0.160784, vali rmse:0.423110\n",
      "\n",
      "Start of epoch 850\n",
      "Training loss (for one batch) at step 0: 0.1353\n",
      "Training epoch: 851, training rmse: 0.160664, vali rmse:0.423096\n",
      "\n",
      "Start of epoch 851\n",
      "Training loss (for one batch) at step 0: 0.1353\n",
      "Training epoch: 852, training rmse: 0.160544, vali rmse:0.423081\n",
      "\n",
      "Start of epoch 852\n",
      "Training loss (for one batch) at step 0: 0.1352\n",
      "Training epoch: 853, training rmse: 0.160423, vali rmse:0.423067\n",
      "\n",
      "Start of epoch 853\n",
      "Training loss (for one batch) at step 0: 0.1351\n",
      "Training epoch: 854, training rmse: 0.160303, vali rmse:0.423053\n",
      "\n",
      "Start of epoch 854\n",
      "Training loss (for one batch) at step 0: 0.1351\n",
      "Training epoch: 855, training rmse: 0.160182, vali rmse:0.423039\n",
      "\n",
      "Start of epoch 855\n",
      "Training loss (for one batch) at step 0: 0.1350\n",
      "Training epoch: 856, training rmse: 0.160061, vali rmse:0.423024\n",
      "\n",
      "Start of epoch 856\n",
      "Training loss (for one batch) at step 0: 0.1350\n",
      "Training epoch: 857, training rmse: 0.159940, vali rmse:0.423010\n",
      "\n",
      "Start of epoch 857\n",
      "Training loss (for one batch) at step 0: 0.1349\n",
      "Training epoch: 858, training rmse: 0.159819, vali rmse:0.422996\n",
      "\n",
      "Start of epoch 858\n",
      "Training loss (for one batch) at step 0: 0.1349\n",
      "Training epoch: 859, training rmse: 0.159698, vali rmse:0.422982\n",
      "\n",
      "Start of epoch 859\n",
      "Training loss (for one batch) at step 0: 0.1348\n",
      "Training epoch: 860, training rmse: 0.159577, vali rmse:0.422968\n",
      "\n",
      "Start of epoch 860\n",
      "Training loss (for one batch) at step 0: 0.1348\n",
      "Training epoch: 861, training rmse: 0.159455, vali rmse:0.422954\n",
      "\n",
      "Start of epoch 861\n",
      "Training loss (for one batch) at step 0: 0.1347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 862, training rmse: 0.159334, vali rmse:0.422940\n",
      "\n",
      "Start of epoch 862\n",
      "Training loss (for one batch) at step 0: 0.1347\n",
      "Training epoch: 863, training rmse: 0.159212, vali rmse:0.422927\n",
      "\n",
      "Start of epoch 863\n",
      "Training loss (for one batch) at step 0: 0.1346\n",
      "Training epoch: 864, training rmse: 0.159090, vali rmse:0.422913\n",
      "\n",
      "Start of epoch 864\n",
      "Training loss (for one batch) at step 0: 0.1345\n",
      "Training epoch: 865, training rmse: 0.158969, vali rmse:0.422899\n",
      "\n",
      "Start of epoch 865\n",
      "Training loss (for one batch) at step 0: 0.1345\n",
      "Training epoch: 866, training rmse: 0.158847, vali rmse:0.422885\n",
      "\n",
      "Start of epoch 866\n",
      "Training loss (for one batch) at step 0: 0.1344\n",
      "Training epoch: 867, training rmse: 0.158725, vali rmse:0.422872\n",
      "\n",
      "Start of epoch 867\n",
      "Training loss (for one batch) at step 0: 0.1344\n",
      "Training epoch: 868, training rmse: 0.158602, vali rmse:0.422858\n",
      "\n",
      "Start of epoch 868\n",
      "Training loss (for one batch) at step 0: 0.1343\n",
      "Training epoch: 869, training rmse: 0.158480, vali rmse:0.422845\n",
      "\n",
      "Start of epoch 869\n",
      "Training loss (for one batch) at step 0: 0.1343\n",
      "Training epoch: 870, training rmse: 0.158358, vali rmse:0.422831\n",
      "\n",
      "Start of epoch 870\n",
      "Training loss (for one batch) at step 0: 0.1342\n",
      "Training epoch: 871, training rmse: 0.158235, vali rmse:0.422818\n",
      "\n",
      "Start of epoch 871\n",
      "Training loss (for one batch) at step 0: 0.1342\n",
      "Training epoch: 872, training rmse: 0.158112, vali rmse:0.422804\n",
      "\n",
      "Start of epoch 872\n",
      "Training loss (for one batch) at step 0: 0.1341\n",
      "Training epoch: 873, training rmse: 0.157990, vali rmse:0.422791\n",
      "\n",
      "Start of epoch 873\n",
      "Training loss (for one batch) at step 0: 0.1341\n",
      "Training epoch: 874, training rmse: 0.157867, vali rmse:0.422778\n",
      "\n",
      "Start of epoch 874\n",
      "Training loss (for one batch) at step 0: 0.1340\n",
      "Training epoch: 875, training rmse: 0.157744, vali rmse:0.422764\n",
      "\n",
      "Start of epoch 875\n",
      "Training loss (for one batch) at step 0: 0.1339\n",
      "Training epoch: 876, training rmse: 0.157621, vali rmse:0.422751\n",
      "\n",
      "Start of epoch 876\n",
      "Training loss (for one batch) at step 0: 0.1339\n",
      "Training epoch: 877, training rmse: 0.157497, vali rmse:0.422738\n",
      "\n",
      "Start of epoch 877\n",
      "Training loss (for one batch) at step 0: 0.1338\n",
      "Training epoch: 878, training rmse: 0.157374, vali rmse:0.422725\n",
      "\n",
      "Start of epoch 878\n",
      "Training loss (for one batch) at step 0: 0.1338\n",
      "Training epoch: 879, training rmse: 0.157251, vali rmse:0.422712\n",
      "\n",
      "Start of epoch 879\n",
      "Training loss (for one batch) at step 0: 0.1337\n",
      "Training epoch: 880, training rmse: 0.157127, vali rmse:0.422699\n",
      "\n",
      "Start of epoch 880\n",
      "Training loss (for one batch) at step 0: 0.1337\n",
      "Training epoch: 881, training rmse: 0.157003, vali rmse:0.422686\n",
      "\n",
      "Start of epoch 881\n",
      "Training loss (for one batch) at step 0: 0.1336\n",
      "Training epoch: 882, training rmse: 0.156880, vali rmse:0.422673\n",
      "\n",
      "Start of epoch 882\n",
      "Training loss (for one batch) at step 0: 0.1336\n",
      "Training epoch: 883, training rmse: 0.156756, vali rmse:0.422660\n",
      "\n",
      "Start of epoch 883\n",
      "Training loss (for one batch) at step 0: 0.1335\n",
      "Training epoch: 884, training rmse: 0.156632, vali rmse:0.422647\n",
      "\n",
      "Start of epoch 884\n",
      "Training loss (for one batch) at step 0: 0.1334\n",
      "Training epoch: 885, training rmse: 0.156507, vali rmse:0.422634\n",
      "\n",
      "Start of epoch 885\n",
      "Training loss (for one batch) at step 0: 0.1334\n",
      "Training epoch: 886, training rmse: 0.156383, vali rmse:0.422622\n",
      "\n",
      "Start of epoch 886\n",
      "Training loss (for one batch) at step 0: 0.1333\n",
      "Training epoch: 887, training rmse: 0.156259, vali rmse:0.422609\n",
      "\n",
      "Start of epoch 887\n",
      "Training loss (for one batch) at step 0: 0.1333\n",
      "Training epoch: 888, training rmse: 0.156134, vali rmse:0.422596\n",
      "\n",
      "Start of epoch 888\n",
      "Training loss (for one batch) at step 0: 0.1332\n",
      "Training epoch: 889, training rmse: 0.156010, vali rmse:0.422583\n",
      "\n",
      "Start of epoch 889\n",
      "Training loss (for one batch) at step 0: 0.1332\n",
      "Training epoch: 890, training rmse: 0.155885, vali rmse:0.422571\n",
      "\n",
      "Start of epoch 890\n",
      "Training loss (for one batch) at step 0: 0.1331\n",
      "Training epoch: 891, training rmse: 0.155760, vali rmse:0.422558\n",
      "\n",
      "Start of epoch 891\n",
      "Training loss (for one batch) at step 0: 0.1331\n",
      "Training epoch: 892, training rmse: 0.155635, vali rmse:0.422546\n",
      "\n",
      "Start of epoch 892\n",
      "Training loss (for one batch) at step 0: 0.1330\n",
      "Training epoch: 893, training rmse: 0.155510, vali rmse:0.422533\n",
      "\n",
      "Start of epoch 893\n",
      "Training loss (for one batch) at step 0: 0.1329\n",
      "Training epoch: 894, training rmse: 0.155385, vali rmse:0.422521\n",
      "\n",
      "Start of epoch 894\n",
      "Training loss (for one batch) at step 0: 0.1329\n",
      "Training epoch: 895, training rmse: 0.155260, vali rmse:0.422508\n",
      "\n",
      "Start of epoch 895\n",
      "Training loss (for one batch) at step 0: 0.1328\n",
      "Training epoch: 896, training rmse: 0.155135, vali rmse:0.422496\n",
      "\n",
      "Start of epoch 896\n",
      "Training loss (for one batch) at step 0: 0.1328\n",
      "Training epoch: 897, training rmse: 0.155009, vali rmse:0.422484\n",
      "\n",
      "Start of epoch 897\n",
      "Training loss (for one batch) at step 0: 0.1327\n",
      "Training epoch: 898, training rmse: 0.154883, vali rmse:0.422472\n",
      "\n",
      "Start of epoch 898\n",
      "Training loss (for one batch) at step 0: 0.1327\n",
      "Training epoch: 899, training rmse: 0.154758, vali rmse:0.422459\n",
      "\n",
      "Start of epoch 899\n",
      "Training loss (for one batch) at step 0: 0.1326\n",
      "Training epoch: 900, training rmse: 0.154632, vali rmse:0.422447\n",
      "\n",
      "Start of epoch 900\n",
      "Training loss (for one batch) at step 0: 0.1325\n",
      "Training epoch: 901, training rmse: 0.154506, vali rmse:0.422435\n",
      "\n",
      "Start of epoch 901\n",
      "Training loss (for one batch) at step 0: 0.1325\n",
      "Training epoch: 902, training rmse: 0.154380, vali rmse:0.422423\n",
      "\n",
      "Start of epoch 902\n",
      "Training loss (for one batch) at step 0: 0.1324\n",
      "Training epoch: 903, training rmse: 0.154254, vali rmse:0.422411\n",
      "\n",
      "Start of epoch 903\n",
      "Training loss (for one batch) at step 0: 0.1324\n",
      "Training epoch: 904, training rmse: 0.154128, vali rmse:0.422399\n",
      "\n",
      "Start of epoch 904\n",
      "Training loss (for one batch) at step 0: 0.1323\n",
      "Training epoch: 905, training rmse: 0.154001, vali rmse:0.422387\n",
      "\n",
      "Start of epoch 905\n",
      "Training loss (for one batch) at step 0: 0.1323\n",
      "Training epoch: 906, training rmse: 0.153875, vali rmse:0.422375\n",
      "\n",
      "Start of epoch 906\n",
      "Training loss (for one batch) at step 0: 0.1322\n",
      "Training epoch: 907, training rmse: 0.153748, vali rmse:0.422363\n",
      "\n",
      "Start of epoch 907\n",
      "Training loss (for one batch) at step 0: 0.1321\n",
      "Training epoch: 908, training rmse: 0.153621, vali rmse:0.422351\n",
      "\n",
      "Start of epoch 908\n",
      "Training loss (for one batch) at step 0: 0.1321\n",
      "Training epoch: 909, training rmse: 0.153495, vali rmse:0.422339\n",
      "\n",
      "Start of epoch 909\n",
      "Training loss (for one batch) at step 0: 0.1320\n",
      "Training epoch: 910, training rmse: 0.153368, vali rmse:0.422327\n",
      "\n",
      "Start of epoch 910\n",
      "Training loss (for one batch) at step 0: 0.1320\n",
      "Training epoch: 911, training rmse: 0.153241, vali rmse:0.422315\n",
      "\n",
      "Start of epoch 911\n",
      "Training loss (for one batch) at step 0: 0.1319\n",
      "Training epoch: 912, training rmse: 0.153114, vali rmse:0.422303\n",
      "\n",
      "Start of epoch 912\n",
      "Training loss (for one batch) at step 0: 0.1318\n",
      "Training epoch: 913, training rmse: 0.152986, vali rmse:0.422292\n",
      "\n",
      "Start of epoch 913\n",
      "Training loss (for one batch) at step 0: 0.1318\n",
      "Training epoch: 914, training rmse: 0.152859, vali rmse:0.422280\n",
      "\n",
      "Start of epoch 914\n",
      "Training loss (for one batch) at step 0: 0.1317\n",
      "Training epoch: 915, training rmse: 0.152732, vali rmse:0.422268\n",
      "\n",
      "Start of epoch 915\n",
      "Training loss (for one batch) at step 0: 0.1317\n",
      "Training epoch: 916, training rmse: 0.152604, vali rmse:0.422257\n",
      "\n",
      "Start of epoch 916\n",
      "Training loss (for one batch) at step 0: 0.1316\n",
      "Training epoch: 917, training rmse: 0.152476, vali rmse:0.422245\n",
      "\n",
      "Start of epoch 917\n",
      "Training loss (for one batch) at step 0: 0.1316\n",
      "Training epoch: 918, training rmse: 0.152349, vali rmse:0.422234\n",
      "\n",
      "Start of epoch 918\n",
      "Training loss (for one batch) at step 0: 0.1315\n",
      "Training epoch: 919, training rmse: 0.152221, vali rmse:0.422222\n",
      "\n",
      "Start of epoch 919\n",
      "Training loss (for one batch) at step 0: 0.1314\n",
      "Training epoch: 920, training rmse: 0.152093, vali rmse:0.422211\n",
      "\n",
      "Start of epoch 920\n",
      "Training loss (for one batch) at step 0: 0.1314\n",
      "Training epoch: 921, training rmse: 0.151965, vali rmse:0.422199\n",
      "\n",
      "Start of epoch 921\n",
      "Training loss (for one batch) at step 0: 0.1313\n",
      "Training epoch: 922, training rmse: 0.151837, vali rmse:0.422188\n",
      "\n",
      "Start of epoch 922\n",
      "Training loss (for one batch) at step 0: 0.1313\n",
      "Training epoch: 923, training rmse: 0.151708, vali rmse:0.422176\n",
      "\n",
      "Start of epoch 923\n",
      "Training loss (for one batch) at step 0: 0.1312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 924, training rmse: 0.151580, vali rmse:0.422165\n",
      "\n",
      "Start of epoch 924\n",
      "Training loss (for one batch) at step 0: 0.1311\n",
      "Training epoch: 925, training rmse: 0.151451, vali rmse:0.422154\n",
      "\n",
      "Start of epoch 925\n",
      "Training loss (for one batch) at step 0: 0.1311\n",
      "Training epoch: 926, training rmse: 0.151323, vali rmse:0.422142\n",
      "\n",
      "Start of epoch 926\n",
      "Training loss (for one batch) at step 0: 0.1310\n",
      "Training epoch: 927, training rmse: 0.151194, vali rmse:0.422131\n",
      "\n",
      "Start of epoch 927\n",
      "Training loss (for one batch) at step 0: 0.1310\n",
      "Training epoch: 928, training rmse: 0.151065, vali rmse:0.422120\n",
      "\n",
      "Start of epoch 928\n",
      "Training loss (for one batch) at step 0: 0.1309\n",
      "Training epoch: 929, training rmse: 0.150936, vali rmse:0.422109\n",
      "\n",
      "Start of epoch 929\n",
      "Training loss (for one batch) at step 0: 0.1309\n",
      "Training epoch: 930, training rmse: 0.150807, vali rmse:0.422097\n",
      "\n",
      "Start of epoch 930\n",
      "Training loss (for one batch) at step 0: 0.1308\n",
      "Training epoch: 931, training rmse: 0.150678, vali rmse:0.422086\n",
      "\n",
      "Start of epoch 931\n",
      "Training loss (for one batch) at step 0: 0.1307\n",
      "Training epoch: 932, training rmse: 0.150549, vali rmse:0.422075\n",
      "\n",
      "Start of epoch 932\n",
      "Training loss (for one batch) at step 0: 0.1307\n",
      "Training epoch: 933, training rmse: 0.150419, vali rmse:0.422064\n",
      "\n",
      "Start of epoch 933\n",
      "Training loss (for one batch) at step 0: 0.1306\n",
      "Training epoch: 934, training rmse: 0.150290, vali rmse:0.422053\n",
      "\n",
      "Start of epoch 934\n",
      "Training loss (for one batch) at step 0: 0.1306\n",
      "Training epoch: 935, training rmse: 0.150160, vali rmse:0.422042\n",
      "\n",
      "Start of epoch 935\n",
      "Training loss (for one batch) at step 0: 0.1305\n",
      "Training epoch: 936, training rmse: 0.150031, vali rmse:0.422031\n",
      "\n",
      "Start of epoch 936\n",
      "Training loss (for one batch) at step 0: 0.1304\n",
      "Training epoch: 937, training rmse: 0.149901, vali rmse:0.422020\n",
      "\n",
      "Start of epoch 937\n",
      "Training loss (for one batch) at step 0: 0.1304\n",
      "Training epoch: 938, training rmse: 0.149771, vali rmse:0.422009\n",
      "\n",
      "Start of epoch 938\n",
      "Training loss (for one batch) at step 0: 0.1303\n",
      "Training epoch: 939, training rmse: 0.149641, vali rmse:0.421998\n",
      "\n",
      "Start of epoch 939\n",
      "Training loss (for one batch) at step 0: 0.1303\n",
      "Training epoch: 940, training rmse: 0.149511, vali rmse:0.421987\n",
      "\n",
      "Start of epoch 940\n",
      "Training loss (for one batch) at step 0: 0.1302\n",
      "Training epoch: 941, training rmse: 0.149381, vali rmse:0.421976\n",
      "\n",
      "Start of epoch 941\n",
      "Training loss (for one batch) at step 0: 0.1301\n",
      "Training epoch: 942, training rmse: 0.149251, vali rmse:0.421966\n",
      "\n",
      "Start of epoch 942\n",
      "Training loss (for one batch) at step 0: 0.1301\n",
      "Training epoch: 943, training rmse: 0.149120, vali rmse:0.421955\n",
      "\n",
      "Start of epoch 943\n",
      "Training loss (for one batch) at step 0: 0.1300\n",
      "Training epoch: 944, training rmse: 0.148990, vali rmse:0.421944\n",
      "\n",
      "Start of epoch 944\n",
      "Training loss (for one batch) at step 0: 0.1300\n",
      "Training epoch: 945, training rmse: 0.148859, vali rmse:0.421933\n",
      "\n",
      "Start of epoch 945\n",
      "Training loss (for one batch) at step 0: 0.1299\n",
      "Training epoch: 946, training rmse: 0.148729, vali rmse:0.421923\n",
      "\n",
      "Start of epoch 946\n",
      "Training loss (for one batch) at step 0: 0.1298\n",
      "Training epoch: 947, training rmse: 0.148598, vali rmse:0.421912\n",
      "\n",
      "Start of epoch 947\n",
      "Training loss (for one batch) at step 0: 0.1298\n",
      "Training epoch: 948, training rmse: 0.148467, vali rmse:0.421901\n",
      "\n",
      "Start of epoch 948\n",
      "Training loss (for one batch) at step 0: 0.1297\n",
      "Training epoch: 949, training rmse: 0.148336, vali rmse:0.421891\n",
      "\n",
      "Start of epoch 949\n",
      "Training loss (for one batch) at step 0: 0.1297\n",
      "Training epoch: 950, training rmse: 0.148205, vali rmse:0.421880\n",
      "\n",
      "Start of epoch 950\n",
      "Training loss (for one batch) at step 0: 0.1296\n",
      "Training epoch: 951, training rmse: 0.148074, vali rmse:0.421870\n",
      "\n",
      "Start of epoch 951\n",
      "Training loss (for one batch) at step 0: 0.1295\n",
      "Training epoch: 952, training rmse: 0.147943, vali rmse:0.421859\n",
      "\n",
      "Start of epoch 952\n",
      "Training loss (for one batch) at step 0: 0.1295\n",
      "Training epoch: 953, training rmse: 0.147811, vali rmse:0.421849\n",
      "\n",
      "Start of epoch 953\n",
      "Training loss (for one batch) at step 0: 0.1294\n",
      "Training epoch: 954, training rmse: 0.147680, vali rmse:0.421838\n",
      "\n",
      "Start of epoch 954\n",
      "Training loss (for one batch) at step 0: 0.1293\n",
      "Training epoch: 955, training rmse: 0.147548, vali rmse:0.421828\n",
      "\n",
      "Start of epoch 955\n",
      "Training loss (for one batch) at step 0: 0.1293\n",
      "Training epoch: 956, training rmse: 0.147417, vali rmse:0.421817\n",
      "\n",
      "Start of epoch 956\n",
      "Training loss (for one batch) at step 0: 0.1292\n",
      "Training epoch: 957, training rmse: 0.147285, vali rmse:0.421807\n",
      "\n",
      "Start of epoch 957\n",
      "Training loss (for one batch) at step 0: 0.1292\n",
      "Training epoch: 958, training rmse: 0.147153, vali rmse:0.421796\n",
      "\n",
      "Start of epoch 958\n",
      "Training loss (for one batch) at step 0: 0.1291\n",
      "Training epoch: 959, training rmse: 0.147021, vali rmse:0.421786\n",
      "\n",
      "Start of epoch 959\n",
      "Training loss (for one batch) at step 0: 0.1290\n",
      "Training epoch: 960, training rmse: 0.146889, vali rmse:0.421776\n",
      "\n",
      "Start of epoch 960\n",
      "Training loss (for one batch) at step 0: 0.1290\n",
      "Training epoch: 961, training rmse: 0.146757, vali rmse:0.421765\n",
      "\n",
      "Start of epoch 961\n",
      "Training loss (for one batch) at step 0: 0.1289\n",
      "Training epoch: 962, training rmse: 0.146625, vali rmse:0.421755\n",
      "\n",
      "Start of epoch 962\n",
      "Training loss (for one batch) at step 0: 0.1289\n",
      "Training epoch: 963, training rmse: 0.146492, vali rmse:0.421745\n",
      "\n",
      "Start of epoch 963\n",
      "Training loss (for one batch) at step 0: 0.1288\n",
      "Training epoch: 964, training rmse: 0.146360, vali rmse:0.421735\n",
      "\n",
      "Start of epoch 964\n",
      "Training loss (for one batch) at step 0: 0.1287\n",
      "Training epoch: 965, training rmse: 0.146227, vali rmse:0.421724\n",
      "\n",
      "Start of epoch 965\n",
      "Training loss (for one batch) at step 0: 0.1287\n",
      "Training epoch: 966, training rmse: 0.146095, vali rmse:0.421714\n",
      "\n",
      "Start of epoch 966\n",
      "Training loss (for one batch) at step 0: 0.1286\n",
      "Training epoch: 967, training rmse: 0.145962, vali rmse:0.421704\n",
      "\n",
      "Start of epoch 967\n",
      "Training loss (for one batch) at step 0: 0.1285\n",
      "Training epoch: 968, training rmse: 0.145829, vali rmse:0.421694\n",
      "\n",
      "Start of epoch 968\n",
      "Training loss (for one batch) at step 0: 0.1285\n",
      "Training epoch: 969, training rmse: 0.145696, vali rmse:0.421684\n",
      "\n",
      "Start of epoch 969\n",
      "Training loss (for one batch) at step 0: 0.1284\n",
      "Training epoch: 970, training rmse: 0.145563, vali rmse:0.421674\n",
      "\n",
      "Start of epoch 970\n",
      "Training loss (for one batch) at step 0: 0.1284\n",
      "Training epoch: 971, training rmse: 0.145430, vali rmse:0.421664\n",
      "\n",
      "Start of epoch 971\n",
      "Training loss (for one batch) at step 0: 0.1283\n",
      "Training epoch: 972, training rmse: 0.145297, vali rmse:0.421654\n",
      "\n",
      "Start of epoch 972\n",
      "Training loss (for one batch) at step 0: 0.1282\n",
      "Training epoch: 973, training rmse: 0.145164, vali rmse:0.421644\n",
      "\n",
      "Start of epoch 973\n",
      "Training loss (for one batch) at step 0: 0.1282\n",
      "Training epoch: 974, training rmse: 0.145030, vali rmse:0.421634\n",
      "\n",
      "Start of epoch 974\n",
      "Training loss (for one batch) at step 0: 0.1281\n",
      "Training epoch: 975, training rmse: 0.144897, vali rmse:0.421624\n",
      "\n",
      "Start of epoch 975\n",
      "Training loss (for one batch) at step 0: 0.1280\n",
      "Training epoch: 976, training rmse: 0.144763, vali rmse:0.421614\n",
      "\n",
      "Start of epoch 976\n",
      "Training loss (for one batch) at step 0: 0.1280\n",
      "Training epoch: 977, training rmse: 0.144630, vali rmse:0.421604\n",
      "\n",
      "Start of epoch 977\n",
      "Training loss (for one batch) at step 0: 0.1279\n",
      "Training epoch: 978, training rmse: 0.144496, vali rmse:0.421594\n",
      "\n",
      "Start of epoch 978\n",
      "Training loss (for one batch) at step 0: 0.1279\n",
      "Training epoch: 979, training rmse: 0.144362, vali rmse:0.421584\n",
      "\n",
      "Start of epoch 979\n",
      "Training loss (for one batch) at step 0: 0.1278\n",
      "Training epoch: 980, training rmse: 0.144228, vali rmse:0.421574\n",
      "\n",
      "Start of epoch 980\n",
      "Training loss (for one batch) at step 0: 0.1277\n",
      "Training epoch: 981, training rmse: 0.144094, vali rmse:0.421564\n",
      "\n",
      "Start of epoch 981\n",
      "Training loss (for one batch) at step 0: 0.1277\n",
      "Training epoch: 982, training rmse: 0.143960, vali rmse:0.421554\n",
      "\n",
      "Start of epoch 982\n",
      "Training loss (for one batch) at step 0: 0.1276\n",
      "Training epoch: 983, training rmse: 0.143826, vali rmse:0.421545\n",
      "\n",
      "Start of epoch 983\n",
      "Training loss (for one batch) at step 0: 0.1275\n",
      "Training epoch: 984, training rmse: 0.143692, vali rmse:0.421535\n",
      "\n",
      "Start of epoch 984\n",
      "Training loss (for one batch) at step 0: 0.1275\n",
      "Training epoch: 985, training rmse: 0.143557, vali rmse:0.421525\n",
      "\n",
      "Start of epoch 985\n",
      "Training loss (for one batch) at step 0: 0.1274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 986, training rmse: 0.143423, vali rmse:0.421515\n",
      "\n",
      "Start of epoch 986\n",
      "Training loss (for one batch) at step 0: 0.1274\n",
      "Training epoch: 987, training rmse: 0.143289, vali rmse:0.421506\n",
      "\n",
      "Start of epoch 987\n",
      "Training loss (for one batch) at step 0: 0.1273\n",
      "Training epoch: 988, training rmse: 0.143154, vali rmse:0.421496\n",
      "\n",
      "Start of epoch 988\n",
      "Training loss (for one batch) at step 0: 0.1272\n",
      "Training epoch: 989, training rmse: 0.143019, vali rmse:0.421486\n",
      "\n",
      "Start of epoch 989\n",
      "Training loss (for one batch) at step 0: 0.1272\n",
      "Training epoch: 990, training rmse: 0.142884, vali rmse:0.421477\n",
      "\n",
      "Start of epoch 990\n",
      "Training loss (for one batch) at step 0: 0.1271\n",
      "Training epoch: 991, training rmse: 0.142750, vali rmse:0.421467\n",
      "\n",
      "Start of epoch 991\n",
      "Training loss (for one batch) at step 0: 0.1270\n",
      "Training epoch: 992, training rmse: 0.142615, vali rmse:0.421457\n",
      "\n",
      "Start of epoch 992\n",
      "Training loss (for one batch) at step 0: 0.1270\n",
      "Training epoch: 993, training rmse: 0.142480, vali rmse:0.421448\n",
      "\n",
      "Start of epoch 993\n",
      "Training loss (for one batch) at step 0: 0.1269\n",
      "Training epoch: 994, training rmse: 0.142344, vali rmse:0.421438\n",
      "\n",
      "Start of epoch 994\n",
      "Training loss (for one batch) at step 0: 0.1268\n",
      "Training epoch: 995, training rmse: 0.142209, vali rmse:0.421429\n",
      "\n",
      "Start of epoch 995\n",
      "Training loss (for one batch) at step 0: 0.1268\n",
      "Training epoch: 996, training rmse: 0.142074, vali rmse:0.421419\n",
      "\n",
      "Start of epoch 996\n",
      "Training loss (for one batch) at step 0: 0.1267\n",
      "Training epoch: 997, training rmse: 0.141939, vali rmse:0.421410\n",
      "\n",
      "Start of epoch 997\n",
      "Training loss (for one batch) at step 0: 0.1267\n",
      "Training epoch: 998, training rmse: 0.141803, vali rmse:0.421400\n",
      "\n",
      "Start of epoch 998\n",
      "Training loss (for one batch) at step 0: 0.1266\n",
      "Training epoch: 999, training rmse: 0.141668, vali rmse:0.421391\n",
      "\n",
      "Start of epoch 999\n",
      "Training loss (for one batch) at step 0: 0.1265\n",
      "Training epoch: 1000, training rmse: 0.141532, vali rmse:0.421381\n",
      "===== Testing Model =====\n",
      "Test rmse: 0.531764\n"
     ]
    }
   ],
   "source": [
    "print('===== TRAINING PMF =====')\n",
    "## initializing hyperparameters\n",
    "batch_size = 64\n",
    "epoches = 1000\n",
    "seed = 1\n",
    "weight_decay = 0.1\n",
    "emb_dim = 100\n",
    "ratio = 0.8\n",
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "\n",
    "## loading dataset\n",
    "users = pickle.load(open('dataset/user_id_to_num.pkl', 'rb'))\n",
    "items = pickle.load(open('dataset/item_id_to_num.pkl', 'rb'))\n",
    "data = np.load('dataset/data_RL_8000.npy')\n",
    "\n",
    "print(\"===== Dataset has been loaded =====\")\n",
    "\n",
    "## As the paper implemented, normalizing the rating\n",
    "## that will act as reward\n",
    "data[:, 2] = 0.5 * (data[:, 2] - 3)\n",
    "\n",
    "## Shuffle data\n",
    "np.random.shuffle(data)\n",
    " \n",
    "## Splitting data data\n",
    "train_data = data[:int(ratio * data.shape[0])]\n",
    "vali_data = data[int(ratio * data.shape[0]):int((ratio + (1 - ratio) / 2) * data.shape[0])]\n",
    "test_data = data[int((ratio + ( 1 - ratio ) / 2) * data.shape[0]):]\n",
    " \n",
    "## Extract number of users and items\n",
    "NUM_USERS = len(users)\n",
    "NUM_ITEMS = len(items)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(batch_size)\n",
    "print(\"===== Preprocess the data has been finished =====\")\n",
    "\n",
    "model = PMF(NUM_USERS, NUM_ITEMS, emb_dim)\n",
    "model(1, 1)\n",
    "\n",
    "print(\"===== Model Instantiated =====\")\n",
    "model.summary()\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def train(train_dataset, len_dataset):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for step, elem in enumerate(train_dataset):\n",
    "        row = elem[:, 0] ## users as a row\n",
    "        col = elem[:, 1] ## items as a column\n",
    "        val = elem[:, 2] ## normalized ratings as value\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            ## Perlu diperiksa kembali\n",
    "            row = tf.Variable(row, trainable=False)\n",
    "            col = tf.Variable(col, trainable=False)\n",
    "            val = tf.Variable(val, trainable=False)\n",
    "            \n",
    "            ## run the forward pass\n",
    "            logits = model(row, col, training=True)\n",
    "            \n",
    "            ## compute the loss value\n",
    "            loss_value = loss_fn(val, logits)\n",
    "            \n",
    "        ## using gradien tape to automatically retrieves\n",
    "        ## the gradients of the trainable variables with respect to loss\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        \n",
    "        ## run one step of gradient descent by updating\n",
    "        ## the value of variable loss\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        epoch_loss += loss_value\n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "    \n",
    "    return float(loss_value)\n",
    "    \n",
    "print(\"===== Training Model =====\")\n",
    "## this list is used for visualization\n",
    "train_loss_list = []\n",
    "train_rmse_list = []\n",
    "vali_rmse_list = []\n",
    "len_train_data = len(train_data)\n",
    "\n",
    "last_vali_rmse = None\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    ## train epoch losses\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    \n",
    "    train_epoch_loss = train(train_dataset, len_train_data)\n",
    "    \n",
    "    ## test \n",
    "    train_loss_list.append(train_epoch_loss)\n",
    "    \n",
    "    ## creating index for predicting\n",
    "    vali_row = tf.Variable(tf.convert_to_tensor(vali_data[:, 0]), trainable=False)\n",
    "    vali_col = tf.Variable(tf.convert_to_tensor(vali_data[:, 1]), trainable=False)\n",
    "    \n",
    "    ## predicting the value\n",
    "    vali_preds = model(vali_row, vali_col, training=False)\n",
    "    \n",
    "    ## calculating rmse\n",
    "    train_rmse = np.sqrt(train_epoch_loss)\n",
    "    vali_rmse = np.sqrt(np.square(np.subtract(vali_data[:, 2], vali_preds)).mean())\n",
    "    \n",
    "    train_rmse_list.append(train_rmse)\n",
    "    vali_rmse_list.append(vali_rmse)\n",
    "    \n",
    "    print('Training epoch:{: d}, training rmse:{: .6f}, vali rmse:{:.6f}'.format(epoch+1, train_rmse, vali_rmse))\n",
    "    \n",
    "    if last_vali_rmse and last_vali_rmse < 0.4:\n",
    "        break\n",
    "    else:\n",
    "        last_vali_rmse = vali_rmse\n",
    "\n",
    "print(\"===== Testing Model =====\")\n",
    "test_row = tf.Variable(tf.convert_to_tensor(test_data[:, 0]), trainable=False)\n",
    "test_col = tf.Variable(tf.convert_to_tensor(test_data[:, 1]), trainable=False)\n",
    "\n",
    "preds = model(test_row, test_col, training=False)\n",
    "\n",
    "test_rmse = np.sqrt(np.square(np.subtract(test_data[:, 2], vali_preds)).mean())\n",
    "print('Test rmse: {:f}'.format(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "923cfa66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.2738414>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PMF(943, 1682, 100)\n",
    "model(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "133f7d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('trained/pmf_weights/pmf_150')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7552b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x231f13ff070>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('trained/adam/pmf_150_adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb59942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = model.user_embedding.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90b2e0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 943, 100)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(user_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed6bad21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([324., 113.,   1.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d30d19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = train_data[0, 0]\n",
    "x2 = train_data[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45bbc262",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17040/433269627.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4fda2697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51334107"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(324., 113., training=False).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43c88825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create plots\n",
    "plt.figure(1)\n",
    "plt.plot(range(1, len(train_rmse_list)+1), train_rmse_list, color='r', label='train rmse')\n",
    "plt.plot(range(1, len(vali_rmse_list)+1), vali_rmse_list, color='b', label='test rmse')\n",
    "plt.legend()\n",
    "plt.annotate(r'train=%f' % (train_rmse_list[-1]), xy=(len(train_rmse_list), train_rmse_list[-1]),\n",
    "             xycoords='data', xytext=(-30, 30), textcoords='offset points', fontsize=10,\n",
    "             arrowprops=dict(arrowstyle='->', connectionstyle='arc3, rad=.2'))\n",
    "plt.annotate(r'vali=%f' % (vali_rmse_list[-1]), xy=(len(vali_rmse_list), vali_rmse_list[-1]),\n",
    "             xycoords='data', xytext=(-30, 30), textcoords='offset points', fontsize=10,\n",
    "             arrowprops=dict(arrowstyle='->', connectionstyle='arc3, rad=.2'))\n",
    "plt.xlim([1, len(train_rmse_list)+10])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE Curve in Training Process')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
